{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25aff06-ced7-4d60-8e25-323c105b7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# import pickle\n",
    "# import pickletools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4055044c-3567-4d18-a584-f761ea52a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_extract = '2023-06-11 16:25'\n",
    "df = pd.read_csv(f'data/reddit_posts_raw_{most_recent_extract}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb092a5-94f3-42c0-b8ad-7c84a81271b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1971, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ebf688-8ea0-4043-88eb-90c52d8becb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>top_comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dating</td>\n",
       "      <td>1471ube</td>\n",
       "      <td>2023-06-11 18:49:33</td>\n",
       "      <td>Am I Clueless?</td>\n",
       "      <td>So there is this girl I’ve known my whole life...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit       id          created_utc           title  \\\n",
       "0    dating  1471ube  2023-06-11 18:49:33  Am I Clueless?   \n",
       "\n",
       "                                            selftext top_comment_text  \n",
       "0  So there is this girl I’ve known my whole life...              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021088-c511-400b-ae95-5ea30d513cb5",
   "metadata": {},
   "source": [
    "### Self-text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b87f74-a99a-4930-a060-41a47fe7e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series(df['selftext'])\n",
    "y = df['subreddit'].map({'dating': 0,\n",
    "                    'datingoverthirty':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792701d4-b20f-48ca-ace2-e34628960c6a",
   "metadata": {},
   "source": [
    "### Self Text and Top Comment - Alternative Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81dca47-8a62-4dc1-835b-67b92a603b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['self_text_and_comment'] = df['self_text'].astype(str) + df['top_comment_text'].astype(str)\n",
    "# X = pd.Series(df['self_text_and_comment'])\n",
    "# y = df['subreddit'].map({'dating': 0,\n",
    "#                    'datingoverthirty':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c0234-f3f8-4a5b-b6e5-bd6245d5e1de",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf8e268-e628-47f6-937f-8bde2e2ab325",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "# X_train.to_pickle('./pickled_models/X_train.pkl')\n",
    "# X_test.to_pickle('./pickled_models/X_test.pkl')\n",
    "# y_train.to_pickle('./pickled_models/y_train.pkl')\n",
    "# y_test.to_pickle('./pickled_models/y_test.pkl')\n",
    "\n",
    "# X_train = pd.read_pickle('./pickled_models/X_train.pkl')\n",
    "# X_test = pd.read_pickle('./pickled_models/X_test.pkl')\n",
    "# y_train = pd.read_pickle('./pickled_models/y_train.pkl')\n",
    "# y_test = pd.read_pickle('./pickled_models/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674e456-0407-4f36-897a-52dfdde06822",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c35f5f-2dd3-4ad8-934e-ec3bb2783183",
   "metadata": {},
   "source": [
    "> The majority class holds 50.63% of responses.  This is the baseline score to beat.\n",
    "\n",
    "> Even class distribution makes a 75/25 train test split possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a934cc43-a7d1-499a-89c2-751b8e072a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5091277890466531"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy_preds = dummy.predict(y_test)\n",
    "dummy_accuracy = accuracy_score(y_test, dummy_preds)\n",
    "dummy_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439cc05-bbd5-459b-8b93-93766af8f2f0",
   "metadata": {},
   "source": [
    "## Baseline Investigation with Standard Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c4714-6c21-4912-bf6d-c11448013d92",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8937a30d-1edb-4714-9028-2699ef3ce2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec0 = CountVectorizer() #standard CountVectorizer\n",
    "cvec0.fit(X_train)\n",
    "# pickle.dump(cvec0, open('./pickled_models/cvec0_baseline', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4df9b-d837-493c-95bb-03c7de07a16c",
   "metadata": {},
   "source": [
    "> See Model Investigaion for Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004fc85f-1d24-4fc9-a9a4-be2d0c2eea52",
   "metadata": {},
   "source": [
    "#### Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf17ad6-346c-4dca-88eb-079fc3281c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec0 = TfidfVectorizer()\n",
    "tvec0.fit(X_train)\n",
    "# pickle.dump(tvec0, open('./pickled_models/tvec0_baseline', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc3109-57f4-461d-84df-9e014ced471b",
   "metadata": {},
   "source": [
    "> See Model Investigation for Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f253c1-1a47-4c5b-817d-a78af324a13e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea6fa8-f6a7-4399-9347-7804c8430ebd",
   "metadata": {},
   "source": [
    "#### Stemming and Lematizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c12595-e429-4d6f-85ef-4bdb583b4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "def stem_post(post):\n",
    "    split_post = post.split(' ')\n",
    "    return ' '.join([p_stemmer.stem(word) for word in split_post])\n",
    "#cite 6/9 Breakfast Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b46e54f-84c4-4c2d-a709-7a2767ad4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(stem_post, open('./pickled_models/function_stem_post.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57cac3e0-718b-46a7-af1f-4217a6013053",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# cite: Lesson 504 NLP 1 - Modified to handle complete words.\n",
    "def lemmatize_post(post):\n",
    "    mapper = { \n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    post_split = post.split(' ')\n",
    "    post_tokens = [(token, tag) for token, tag in nltk.pos_tag(post_split)]\n",
    "    post_lem = []\n",
    "    for token in post_tokens:\n",
    "        pos = mapper.get(token[1][0])\n",
    "        # post_lem.append((token[0],pos) if pos != None else (token[0]))\n",
    "        post_lem.append(lemmatizer.lemmatize(token[0], pos) if pos != None else token[0])\n",
    "    return ' '.join(post_lem).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f085bfe4-8bca-4ee6-b77a-d8650845ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(lemmatize_post, open('./pickled_models/function_lemmatize_post', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9b446-9390-4b88-bb83-c473c9609104",
   "metadata": {},
   "source": [
    "## RS-0\n",
    "RandomSearchCV over Multiple Model Types with Tfidf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05cd6f3a-0294-496e-931e-58c096f06156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating multiple classifiers in the same RandomSearchCV, trying different combinations of Tfidf / CountVectorizer and LogisticRegression() / MultinomialNB\n",
    "# Inspiration: Wrapper Class (https://stackoverflow.com/questions/50285973/pipeline-multiple-classifiers).  Content: DSI Lesson 507 on OOP (https://git.generalassemb.ly/bobadams1/507-lesson-object-oriented-programming)\n",
    "'''\n",
    "Notes from Inspiration above (no copy-paste):\n",
    "1. Need BaseEstimator() as the base class for all sklearn estimators - as a stand in for the estimator being selected\n",
    "2. The class only really needs to to have self and the estimator as objects in the class.\n",
    "3. The methods you would normally call for the estimator should be defined as functions within the model (don't forget to pass self every time!)\n",
    "'''\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Multi_Classifier(BaseEstimator):\n",
    "    def __init__(self, estimator = MultinomialNB()): #LogisticRegression as default\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def fit(self, X, y): # interested in LogisticRegression, NB... both take primarily X,y\n",
    "        return self.estimator.fit(X,y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "    \n",
    "    def score(self, X,y):\n",
    "        return self.estimator.score(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddd8c8-5ab3-4535-b072-dab124957860",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b41831c-f116-4628-b840-075752cf6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe0 = Pipeline([\n",
    "    ('tvec' , TfidfVectorizer()),\n",
    "    ('cls' , Multi_Classifier())\n",
    "])\n",
    "\n",
    "params0 = [{ # list of params... one for each estimator (order matters here). Cite: Tim Office Hours\n",
    "## Logistic Regression\n",
    "         'tvec__preprocessor': [None, stem_post, lemmatize_post],     \n",
    "         'tvec__max_df': [1.0, 0.9],\n",
    "         # 'tvec__max_features': None,\n",
    "         # 'tvec__min_df': 1,\n",
    "         'tvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'tvec__stop_words': [None, 'english'],\n",
    "        \n",
    "        'cls__estimator': [LogisticRegression()],\n",
    "        'cls__estimator__C': np.linspace(0.00001, 1, 10),\n",
    "        # 'cls__estimator__max_iter': 100,\n",
    "        # 'cls__estimator__penalty': 'l2'\n",
    "},\n",
    "## Multinomial Naive Bayes\n",
    "{        'tvec__preprocessor': [None, stem_post, lemmatize_post],     \n",
    "         'tvec__max_df': [1.0, 0.9],\n",
    "         # 'tvec__max_features': None,\n",
    "         # 'tvec__min_df': 1,\n",
    "         'tvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'tvec__stop_words': [None, 'english'],\n",
    "         'cls__estimator': [MultinomialNB()]\n",
    "}]\n",
    "# pipe0.get_params() ## For LogisticRegression\n",
    "# pipe0.get_params() ## for Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3634f8bd-9429-481d-9f12-aad28a337c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs0 = RandomizedSearchCV(estimator=pipe0,\n",
    "                        param_distributions=params0,\n",
    "                        cv = 5,\n",
    "                        n_iter = 50\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aff007-a2ae-4a50-8a6b-7bc71fffe696",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "rs0.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c53088-9ddd-41e6-9f83-bc671ba2f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(rs0, open('./pickled_models/rs0.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f72bd5-d50c-4e2e-aea7-738746fa4438",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs0.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1ac03-b2f3-4185-a286-22b16a399946",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs0.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a9df6-8671-43c9-a0f1-fa4cb7f8fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accy0 = rs0.score(X_train, y_train)\n",
    "test_accy0 = rs0.score(X_test, y_test)\n",
    "preds0 = rs0.predict(X_test)\n",
    "preds_proba0 = rs0.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c08eb-77f5-4806-8c5c-73310477aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train-Test Accuracy Scores:\\n  Train: {round(train_accy0,5)} \\n  Test: {round(test_accy0,5)}\\n  Baseline: {round(dummy_accuracy,5)}\\n---\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, rs0.predict(X_test), digits = 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1c0f6-243d-4d14-a46d-b5f937e69c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, preds0, cmap = 'YlOrBr', display_labels=['r/dating','r/datingoverthirty'])\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351f11b-115a-467c-a4c6-4d1c8f882ba5",
   "metadata": {},
   "source": [
    "## RS1\n",
    "RandomSearchCV over Multiple Model Types with CountVectorization - Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4b70e-317c-4068-bea6-7b27cf3af673",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('cvec' , CountVectorizer()),\n",
    "    ('cls' , Multi_Classifier())\n",
    "])\n",
    "\n",
    "params0 = [{ # list of params... one for each estimator (order matters here). Cite: Tim Office Hours\n",
    "## Logistic Regression\n",
    "         'cvec__max_df': [1.0, 0.9],\n",
    "         # 'cvec__max_features': None,\n",
    "         # 'cvec__min_df': 1,\n",
    "         'cvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'cvec__preprocessor': [None, stem_post, lemmatize_post],\n",
    "         'cvec__stop_words': [None, 'english'],\n",
    "        \n",
    "        'cls__estimator': [LogisticRegression()],\n",
    "        'cls__estimator__C': np.linspace(0.00001, 1, 10),\n",
    "        # 'cls__estimator__max_iter': 100,\n",
    "        # 'cls__estimator__penalty': 'l2'\n",
    "},\n",
    "## Multinomial Naive Bayes\n",
    "{        'cvec__max_df': [1.0, 0.9],\n",
    "         # 'cvec__max_features': None,\n",
    "         # 'cvec__min_df': 1,\n",
    "         'cvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'cvec__preprocessor': [None, stem_post, lemmatize_post],\n",
    "         'cvec__stop_words': [None, 'english'],\n",
    "         \n",
    "         'cls__estimator': [MultinomialNB()]\n",
    "}]\n",
    "# pipe0.get_params() ## For LogisticRegression\n",
    "# pipe0.get_params() ## for Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab060a0-f6f2-4755-8ad3-ad3a7c63f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1 = RandomizedSearchCV(estimator=pipe0,\n",
    "                        param_distributions=params0,\n",
    "                        cv = 5,\n",
    "                        n_iter = 50\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633b13e-6e5d-44d6-8164-d53ff90c3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "rs1.fig(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794e333-536b-4db6-827f-1f044c5b0279",
   "metadata": {},
   "source": [
    "## VC 2\n",
    "VotingClassifier Ensemble of KNN Classification with Tfidf Vectorization\n",
    "> Using TFIDF Output, will an Bootstrap Aggregated Ensemble of K-Nearest Neighbors result in predictable performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfde834-de36-4017-895e-469491f153c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "# pipe2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b4a27-7f94-4ecc-9449-d0f364a2af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = {\n",
    "     # 'tvec': TfidfVectorizer(),\n",
    "     # 'knn': KNeighborsClassifier(),\n",
    "     'tvec__max_df': np.linspace(0.8,1,5),\n",
    "     # 'tvec__max_features': None,\n",
    "     'tvec__min_df': np.linspace(0.01, 0.05, 3),\n",
    "     'tvec__ngram_range': [(1, 1),(1,2),(1,3)],\n",
    "     'tvec__preprocessor': [stem_post, lemmatize_post],\n",
    "     'tvec__stop_words': [None, 'english'],\n",
    "     'knn__n_neighbors': [5],\n",
    "} #100 knn runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d253cf0-d5e1-4c9b-88e9-2abb878b33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc2 = VotingClassifier(pipe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2b62f-4d87-4b94-86fd-e90884513aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vc2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc885b8-e126-4732-99a2-fb8c8878359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs2 = RandomizedSearchCV(estimator=pipe2, \n",
    "                         param_distributions=params2,\n",
    "                         cv = 5, \n",
    "                         # n_jobs=2,# n_jobs can call pickle when preparing models to run in parallel\n",
    "                        n_iter=20) # 180 total combinations.  5 fold cross-validation.  running 20 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee90f2b-ea70-4d78-bdc4-2be8149c7400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rs2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "256a7a03-57b0-4198-998f-c5b616cfed3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__stop_words': None,\n",
       " 'tvec__preprocessor': <function __main__.lemmatize_post(post)>,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__min_df': 0.02,\n",
       " 'tvec__max_df': 0.6842105263157895,\n",
       " 'knn__n_neighbors': 5}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a92709-3f06-4928-8457-b5db559fe741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
