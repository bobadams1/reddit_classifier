{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0371cdb1-ed20-4d08-8745-e19493d44d7c",
   "metadata": {},
   "source": [
    "# Model Fitting and Evalutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975f39b-5b89-4986-98b9-c8ad52908caf",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b25aff06-ced7-4d60-8e25-323c105b7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# import pickle\n",
    "# import pickletools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4055044c-3567-4d18-a584-f761ea52a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_extract = '2023-06-11 16:25'\n",
    "df = pd.read_csv(f'data/reddit_posts_raw_{most_recent_extract}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb092a5-94f3-42c0-b8ad-7c84a81271b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1971, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ebf688-8ea0-4043-88eb-90c52d8becb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>top_comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dating</td>\n",
       "      <td>1471ube</td>\n",
       "      <td>2023-06-11 18:49:33</td>\n",
       "      <td>Am I Clueless?</td>\n",
       "      <td>So there is this girl I’ve known my whole life...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit       id          created_utc           title  \\\n",
       "0    dating  1471ube  2023-06-11 18:49:33  Am I Clueless?   \n",
       "\n",
       "                                            selftext top_comment_text  \n",
       "0  So there is this girl I’ve known my whole life...              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53578f0-9e88-4023-92ad-e103fa24003a",
   "metadata": {},
   "source": [
    "### Data Leveraged\n",
    "For this project, reddit posts were pulled from two subreddits: r/dating, and r/datingoverthirty.  Due to limitations with community access (Summer 2023 Reddit Blackout) and APIs (the removal of some of Reddit's APIs), sourcing data was impeded.  Approximately 1000 posts were sourced the day before protests began, constituting the working dataset used in EDA and Modeling.  To augment the information available, the text of the top-voted comment was pulled for each post.  This enables the investigation to cover broader community interaction.\n",
    "\n",
    "Comment text was interesting in EDA to understand the post-community response in each subreddit.  For modeling, I am using selftext only (the bulk of the content submitted by the original poster on reddit).  The primary area of focus is which community a poster should visit if they are in need of advice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021088-c511-400b-ae95-5ea30d513cb5",
   "metadata": {},
   "source": [
    "#### Self-text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b87f74-a99a-4930-a060-41a47fe7e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series(df['selftext'])\n",
    "y = df['subreddit'].map({'dating': 0,\n",
    "                    'datingoverthirty':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792701d4-b20f-48ca-ace2-e34628960c6a",
   "metadata": {},
   "source": [
    "#### Self Text and Top Comment - Alternative Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81dca47-8a62-4dc1-835b-67b92a603b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['self_text_and_comment'] = df['self_text'].astype(str) + df['top_comment_text'].astype(str)\n",
    "# X = pd.Series(df['self_text_and_comment'])\n",
    "# y = df['subreddit'].map({'dating': 0,\n",
    "#                    'datingoverthirty':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c0234-f3f8-4a5b-b6e5-bd6245d5e1de",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf8e268-e628-47f6-937f-8bde2e2ab325",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "# X_train.to_pickle('./pickled_models/X_train.pkl')\n",
    "# X_test.to_pickle('./pickled_models/X_test.pkl')\n",
    "# y_train.to_pickle('./pickled_models/y_train.pkl')\n",
    "# y_test.to_pickle('./pickled_models/y_test.pkl')\n",
    "\n",
    "# X_train = pd.read_pickle('./pickled_models/X_train.pkl')\n",
    "# X_test = pd.read_pickle('./pickled_models/X_test.pkl')\n",
    "# y_train = pd.read_pickle('./pickled_models/y_train.pkl')\n",
    "# y_test = pd.read_pickle('./pickled_models/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674e456-0407-4f36-897a-52dfdde06822",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c35f5f-2dd3-4ad8-934e-ec3bb2783183",
   "metadata": {},
   "source": [
    "> The majority class holds 50.63% of responses.  This is the baseline score to beat.\n",
    "\n",
    "> Even class distribution makes a 75/25 train test split possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a934cc43-a7d1-499a-89c2-751b8e072a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5091277890466531"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy_preds = dummy.predict(y_test)\n",
    "dummy_accuracy = accuracy_score(y_test, dummy_preds)\n",
    "dummy_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439cc05-bbd5-459b-8b93-93766af8f2f0",
   "metadata": {},
   "source": [
    "## Baseline Investigation with Standard Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e359a09-fe26-4bac-8415-e06dbdd1cdb9",
   "metadata": {},
   "source": [
    "Vectorizers perform differently on varying corpora.  This simple look in Model Investigations helps shed the light on how these vectorizers perform out of the box with selftext from these subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c4714-6c21-4912-bf6d-c11448013d92",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8937a30d-1edb-4714-9028-2699ef3ce2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec0 = CountVectorizer() #standard CountVectorizer\n",
    "cvec0.fit(X_train)\n",
    "# pickle.dump(cvec0, open('./pickled_models/cvec0_baseline', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4df9b-d837-493c-95bb-03c7de07a16c",
   "metadata": {},
   "source": [
    "> See Model Investigaion for Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004fc85f-1d24-4fc9-a9a4-be2d0c2eea52",
   "metadata": {},
   "source": [
    "#### Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf17ad6-346c-4dca-88eb-079fc3281c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec0 = TfidfVectorizer()\n",
    "tvec0.fit(X_train)\n",
    "# pickle.dump(tvec0, open('./pickled_models/tvec0_baseline', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc3109-57f4-461d-84df-9e014ced471b",
   "metadata": {},
   "source": [
    "> See Model Investigation for Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f253c1-1a47-4c5b-817d-a78af324a13e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea6fa8-f6a7-4399-9347-7804c8430ebd",
   "metadata": {},
   "source": [
    "#### Stemming and Lematizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c12595-e429-4d6f-85ef-4bdb583b4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "def stem_post(post):\n",
    "    split_post = post.split(' ')\n",
    "    return ' '.join([p_stemmer.stem(word) for word in split_post])\n",
    "#cite 6/9 Breakfast Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b46e54f-84c4-4c2d-a709-7a2767ad4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(stem_post, open('./pickled_models/function_stem_post.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57cac3e0-718b-46a7-af1f-4217a6013053",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# cite: Lesson 504 NLP 1 - Modified to handle complete words.\n",
    "def lemmatize_post(post):\n",
    "    mapper = { \n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    post_split = post.split(' ')\n",
    "    post_tokens = [(token, tag) for token, tag in nltk.pos_tag(post_split)]\n",
    "    post_lem = []\n",
    "    for token in post_tokens:\n",
    "        pos = mapper.get(token[1][0])\n",
    "        # post_lem.append((token[0],pos) if pos != None else (token[0]))\n",
    "        post_lem.append(lemmatizer.lemmatize(token[0], pos) if pos != None else token[0])\n",
    "    return ' '.join(post_lem).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f085bfe4-8bca-4ee6-b77a-d8650845ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(lemmatize_post, open('./pickled_models/function_lemmatize_post', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dffd3a-0416-479d-9e91-d7dbd139907e",
   "metadata": {},
   "source": [
    "#### Multiple Estimator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05cd6f3a-0294-496e-931e-58c096f06156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating multiple classifiers in the same RandomSearchCV, trying different combinations of Tfidf / CountVectorizer and LogisticRegression() / MultinomialNB\n",
    "# Inspiration: Wrapper Class (https://stackoverflow.com/questions/50285973/pipeline-multiple-classifiers).  Content: DSI Lesson 507 on OOP (https://git.generalassemb.ly/bobadams1/507-lesson-object-oriented-programming)\n",
    "'''\n",
    "Notes from Inspiration above (no copy-paste):\n",
    "1. Need BaseEstimator() as the base class for all sklearn estimators - as a stand in for the estimator being selected\n",
    "2. The class only really needs to to have self and the estimator as objects in the class.\n",
    "3. The methods you would normally call for the estimator should be defined as functions within the model (don't forget to pass self every time!)\n",
    "'''\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Multi_Classifier(BaseEstimator):\n",
    "    def __init__(self, estimator = MultinomialNB()): #Multinomial NB as default\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def fit(self, X, y): # interested in LogisticRegression, NB... both take primarily X,y\n",
    "        return self.estimator.fit(X,y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "    \n",
    "    def score(self, X,y):\n",
    "        return self.estimator.score(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f7dab-4fd6-404d-93ff-a3a0b2a4d2e1",
   "metadata": {},
   "source": [
    "#### Multiple Vectorizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e62a7e3f-5335-4b3f-bd8e-227275bb72fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1145354163.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[256], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    def__init__(self, vectorizer):\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Multi_Vectorizer():\n",
    "    def__init__(self, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self.vectorizer.fit(X)\n",
    "    \n",
    "    def transform(self, X);\n",
    "        return self.vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b40938-5477-4c81-8db1-3501c4aacd66",
   "metadata": {},
   "source": [
    "#### Model Performance Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8c5f8db7-2fd4-42b1-a21e-503244720f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Data Frame to capture output from each model fit.\n",
    "model_performance_capture = pd.DataFrame(columns = ['model_name', 'model', 'best_score','model_params', 'train_acuracy', 'test_accuracy', 'baseline_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5c1a9df6-8671-43c9-a0f1-fa4cb7f8fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "def model_evaluation(model, model_name):\n",
    "    \n",
    "    # print(model_performance)\n",
    "    \n",
    "    #Print Model Evaluations to the screen\n",
    "    print(f\"Train-Test Accuracy Scores:\\n  Train: {round(train_accy0,5)} \\n  Test: {round(test_accy0,5)}\\n  Baseline: {round(dummy_accuracy,5)}\\n---\")\n",
    "    print(f\"\\n Classification Report:\\n{classification_report(y_test, rs0.predict(X_test), digits = 4)}\")\n",
    "    print(f\"\\n---\\nBest Parameters: \\n{model.best_params_}\")\n",
    "    \n",
    "    # Plot and Save the Confusion Matrix\n",
    "    plt.figure(figsize = (8,5))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, preds0, cmap = 'YlOrBr', display_labels=['r/dating','r/datingoverthirty'])\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "    # plt.suptitle('Stop Words: English | Unigrams and Bigrams | Max Documents:90% | No Stem/Lem | LogisticRegression', y=0, fontsize = 9)\n",
    "    plt.savefig(fname= f'./images/{model_name}_Confusion Matrix.png', bbox_inches = 'tight', dpi = 200)\n",
    "    plt.show()\n",
    "    \n",
    "    #Append results of key metrics to \n",
    "    # pd.concat(model_performance_capture,\n",
    "    model_performance = pd.DataFrame({\n",
    "        'model_name' : model_name,\n",
    "        'model' : model,\n",
    "        'best_score_CV' : model.best_score_,\n",
    "        'model_params' : [model.best_params_],\n",
    "        'train_acuracy' : model.score(X_train, y_train),\n",
    "        'test_accuracy' : model.score(X_test, y_test),\n",
    "        'baseline_accuracy' : dummy_accuracy\n",
    "        })\n",
    "\n",
    "    return model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e18c48f-c771-4f92-9574-a3f4d34b9d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>model_params</th>\n",
       "      <th>train_acuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>best_score</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_name, model, model_params, train_acuracy, test_accuracy, best_score, baseline_accuracy]\n",
       "Index: []"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance_capture.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9b446-9390-4b88-bb83-c473c9609104",
   "metadata": {},
   "source": [
    "## 01 - RandomSearch over Multiple Estimators with Tfidf Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddd8c8-5ab3-4535-b072-dab124957860",
   "metadata": {},
   "source": [
    "#### Pipeline & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8b41831c-f116-4628-b840-075752cf6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('tvec' , TfidfVectorizer()),\n",
    "    # ('sc', StandardScaler()),\n",
    "    ('cls' , Multi_Classifier())\n",
    "])\n",
    "\n",
    "params1 = [# list of params... one for each estimator (order matters here). Cite: Tim Office Hours\n",
    "    { \n",
    "        ## Logistic Regression\n",
    "        'cls__estimator': [LogisticRegression()],\n",
    "        'cls__estimator__C': np.linspace(0.9, 2, 10), \n",
    "        \n",
    "        'tvec__preprocessor': [lemmatize_post],     # Lemmatizing showed the best results in initial testing\n",
    "        'tvec__max_df': [1.0, 0.9],\n",
    "        'tvec__max_features': [None, 5000],\n",
    "        'tvec__min_df': [1],\n",
    "        'tvec__ngram_range': [(1,2)],               #words and bigrams showed best results early\n",
    "        'tvec__stop_words': ['english'],            #English stop words showed best results early.\n",
    "        \n",
    "    }\n",
    "        ,# Multinomial Naive Bayes\n",
    "        {\n",
    "        'cls__estimator': [MultinomialNB()],\n",
    "            \n",
    "         'tvec__preprocessor': [lemmatize_post],     # Lemmatizing showed the best results in initial testing\n",
    "        'tvec__max_df': [1.0, 0.9],\n",
    "        'tvec__max_features': [None, 5000],\n",
    "        'tvec__min_df': [1],\n",
    "        'tvec__ngram_range': [(1,2)],               #words and bigrams showed best results early\n",
    "        'tvec__stop_words': ['english'],            #English stop words showed best results early.\n",
    "            \n",
    "    }\n",
    "        ,#Kernelized SVM\n",
    "        {\n",
    "        'cls__estimator': [SVC()],\n",
    "        'cls__estimator__C': np.linspace(0.05, 2, 10),\n",
    "        'cls__estimator__degree': [2,3],\n",
    "        'cls__estimator__kernel': ['poly','rbf'],\n",
    "            \n",
    "        'tvec__preprocessor': [lemmatize_post],     # Lemmatizing showed the best results in initial testing\n",
    "        'tvec__max_df': [1.0, 0.9],\n",
    "        'tvec__max_features': [None, 5000],\n",
    "        'tvec__min_df': [1],\n",
    "        'tvec__ngram_range': [(1,2)],               #words and bigrams showed best results early\n",
    "        'tvec__stop_words': ['english'],            #English stop words showed best results early.\n",
    "         \n",
    "    }\n",
    "]\n",
    "# pipe1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3634f8bd-9429-481d-9f12-aad28a337c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1 = RandomizedSearchCV(estimator=pipe0,\n",
    "                        param_distributions=params0,\n",
    "                        cv = 5,\n",
    "                        n_iter = 100\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795b264-fb94-45af-a4d3-17fddebf17ba",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d9aff007-a2ae-4a50-8a6b-7bc71fffe696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 51 is smaller than n_iter=100. Running 51 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;cls&#x27;, Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{&#x27;cls__estimator&#x27;: [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)]...\n",
       "                                        {&#x27;cls__estimator&#x27;: [SVC(C=1.1333333333333335,\n",
       "                                                                degree=2)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         &#x27;cls__estimator__degree&#x27;: [2, 3],\n",
       "                                         &#x27;cls__estimator__kernel&#x27;: [&#x27;poly&#x27;,\n",
       "                                                                    &#x27;rbf&#x27;],\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)],\n",
       "                                         &#x27;tvec__preprocessor&#x27;: [&lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                         &#x27;tvec__stop_words&#x27;: [&#x27;english&#x27;]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-58\" type=\"checkbox\" ><label for=\"sk-estimator-id-58\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;cls&#x27;, Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{&#x27;cls__estimator&#x27;: [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)]...\n",
       "                                        {&#x27;cls__estimator&#x27;: [SVC(C=1.1333333333333335,\n",
       "                                                                degree=2)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         &#x27;cls__estimator__degree&#x27;: [2, 3],\n",
       "                                         &#x27;cls__estimator__kernel&#x27;: [&#x27;poly&#x27;,\n",
       "                                                                    &#x27;rbf&#x27;],\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)],\n",
       "                                         &#x27;tvec__preprocessor&#x27;: [&lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                         &#x27;tvec__stop_words&#x27;: [&#x27;english&#x27;]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-59\" type=\"checkbox\" ><label for=\"sk-estimator-id-59\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()), (&#x27;cls&#x27;, Multi_Classifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-60\" type=\"checkbox\" ><label for=\"sk-estimator-id-60\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-61\" type=\"checkbox\" ><label for=\"sk-estimator-id-61\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cls: Multi_Classifier</label><div class=\"sk-toggleable__content\"><pre>Multi_Classifier()</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-62\" type=\"checkbox\" ><label for=\"sk-estimator-id-62\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-63\" type=\"checkbox\" ><label for=\"sk-estimator-id-63\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                             ('cls', Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{'cls__estimator': [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         'cls__estimator__C': array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         'tvec__ngram_range': [(1, 2)]...\n",
       "                                        {'cls__estimator': [SVC(C=1.1333333333333335,\n",
       "                                                                degree=2)],\n",
       "                                         'cls__estimator__C': array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         'cls__estimator__degree': [2, 3],\n",
       "                                         'cls__estimator__kernel': ['poly',\n",
       "                                                                    'rbf'],\n",
       "                                         'tvec__ngram_range': [(1, 2)],\n",
       "                                         'tvec__preprocessor': [<function lemmatize_post at 0x7fbf48805510>],\n",
       "                                         'tvec__stop_words': ['english']}])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "rs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c53088-9ddd-41e6-9f83-bc671ba2f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(rs1, open('./pickled_models/rs0.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3fb95-12d5-4b3d-be2c-959df629c0e5",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ff55cf62-d44e-41ab-b098-e4127cbe1f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy Scores:\n",
      "  Train: 0.98985 \n",
      "  Test: 0.79716\n",
      "  Baseline: 0.50913\n",
      "---\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8319    0.7769    0.8034       242\n",
      "           1     0.7978    0.8486    0.8224       251\n",
      "\n",
      "    accuracy                         0.8134       493\n",
      "   macro avg     0.8148    0.8127    0.8129       493\n",
      "weighted avg     0.8145    0.8134    0.8131       493\n",
      "\n",
      "\n",
      "---\n",
      "Best Parameters: \n",
      "{'tvec__stop_words': 'english', 'tvec__preprocessor': <function lemmatize_post at 0x7fbf48805510>, 'tvec__ngram_range': (1, 2), 'cls__estimator__C': 1.7555555555555555, 'cls__estimator': LogisticRegression(C=1.7555555555555555)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/4sqt0lt90t72lgqr2zq3p6l80000gn/T/ipykernel_33031/3652503576.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_performance_capture.append(model_evaluation(rs1, 'Model_1_RSCV_Multi_Tfidf'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>model_params</th>\n",
       "      <th>train_acuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "      <th>best_score_CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_1_RSCV_Multi_Tfidf</td>\n",
       "      <td>RandomizedSearchCV(cv=5,\\n                   e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.99797</td>\n",
       "      <td>0.805274</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.798388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model_name  \\\n",
       "0  Model_1_RSCV_Multi_Tfidf   \n",
       "\n",
       "                                               model best_score  \\\n",
       "0  RandomizedSearchCV(cv=5,\\n                   e...        NaN   \n",
       "\n",
       "                                        model_params  train_acuracy  \\\n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...        0.99797   \n",
       "\n",
       "   test_accuracy  baseline_accuracy  best_score_CV  \n",
       "0       0.805274           0.509128       0.798388  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance_capture.append(model_evaluation(rs1, 'Model_1_RSCV_Multi_Tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351f11b-115a-467c-a4c6-4d1c8f882ba5",
   "metadata": {},
   "source": [
    "## 02 - RandomSearchCV over Multiple Estimators with CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3eb4b70e-317c-4068-bea6-7b27cf3af673",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('cvec' , CountVectorizer()),\n",
    "    ('cls' , Multi_Classifier())\n",
    "])\n",
    "\n",
    "params2 = [{ # list of params... one for each estimator (order matters here). Cite: Tim Office Hours\n",
    "        ## Logistic Regression\n",
    "        'cls__estimator': [LogisticRegression()],\n",
    "        'cls__estimator__C': np.linspace(0.00001, 1, 10), \n",
    "        \n",
    "        'cvec__max_df': [1.0, 0.9],\n",
    "         'cvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'cvec__preprocessor': [None, stem_post, lemmatize_post],\n",
    "         'cvec__stop_words': [None, 'english']          \n",
    "\n",
    "        # 'cls__estimator__max_iter': 100,\n",
    "        # 'cls__estimator__penalty': 'l2'\n",
    "        },\n",
    "        ## Multinomial Naive Bayes\n",
    "        {\n",
    "        'cls__estimator': [MultinomialNB()],\n",
    "            \n",
    "        'cvec__max_df': [1.0, 0.9],\n",
    "         'cvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'cvec__preprocessor': [None, stem_post, lemmatize_post],\n",
    "         'cvec__stop_words': [None, 'english']\n",
    "            \n",
    "},\n",
    "        #Kernelized SVM\n",
    "        {\n",
    "         'cls__estimator': [SVC()],\n",
    "         'cls__estimator__C': np.linspace(0.05, 2, 10),\n",
    "         'cls__estimator__degree': [2,3],\n",
    "         'cls__estimator__kernel': ['poly','rbf'],\n",
    "            \n",
    "         'cvec__max_df': [1.0, 0.9],\n",
    "         'cvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'cvec__preprocessor': [None, stem_post, lemmatize_post],\n",
    "         'cvec__stop_words': [None, 'english']\n",
    "         \n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2ab060a0-f6f2-4755-8ad3-ad3a7c63f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs2 = RandomizedSearchCV(estimator=pipe1,\n",
    "                        param_distributions=params1,\n",
    "                        cv = 5,\n",
    "                        n_iter = 100\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4633b13e-6e5d-44d6-8164-d53ff90c3151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;cls&#x27;, Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{&#x27;cls__estimator&#x27;: [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                         &#x27;t...\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         &#x27;cls__estimator__degree&#x27;: [2, 3],\n",
       "                                         &#x27;cls__estimator__kernel&#x27;: [&#x27;poly&#x27;,\n",
       "                                                                    &#x27;rbf&#x27;],\n",
       "                                         &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                         &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                         &#x27;tvec__min_df&#x27;: [1],\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)],\n",
       "                                         &#x27;tvec__preprocessor&#x27;: [&lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                         &#x27;tvec__stop_words&#x27;: [&#x27;english&#x27;]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-64\" type=\"checkbox\" ><label for=\"sk-estimator-id-64\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;cls&#x27;, Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{&#x27;cls__estimator&#x27;: [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                         &#x27;t...\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         &#x27;cls__estimator__degree&#x27;: [2, 3],\n",
       "                                         &#x27;cls__estimator__kernel&#x27;: [&#x27;poly&#x27;,\n",
       "                                                                    &#x27;rbf&#x27;],\n",
       "                                         &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                         &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                         &#x27;tvec__min_df&#x27;: [1],\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)],\n",
       "                                         &#x27;tvec__preprocessor&#x27;: [&lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                         &#x27;tvec__stop_words&#x27;: [&#x27;english&#x27;]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-65\" type=\"checkbox\" ><label for=\"sk-estimator-id-65\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()), (&#x27;cls&#x27;, Multi_Classifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-66\" type=\"checkbox\" ><label for=\"sk-estimator-id-66\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-67\" type=\"checkbox\" ><label for=\"sk-estimator-id-67\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cls: Multi_Classifier</label><div class=\"sk-toggleable__content\"><pre>Multi_Classifier()</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-68\" type=\"checkbox\" ><label for=\"sk-estimator-id-68\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-69\" type=\"checkbox\" ><label for=\"sk-estimator-id-69\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                             ('cls', Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{'cls__estimator': [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         'cls__estimator__C': array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         'tvec__max_df': [1.0, 0.9],\n",
       "                                         't...\n",
       "                                         'cls__estimator__C': array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         'cls__estimator__degree': [2, 3],\n",
       "                                         'cls__estimator__kernel': ['poly',\n",
       "                                                                    'rbf'],\n",
       "                                         'tvec__max_df': [1.0, 0.9],\n",
       "                                         'tvec__max_features': [None, 5000],\n",
       "                                         'tvec__min_df': [1],\n",
       "                                         'tvec__ngram_range': [(1, 2)],\n",
       "                                         'tvec__preprocessor': [<function lemmatize_post at 0x7fbf48805510>],\n",
       "                                         'tvec__stop_words': ['english']}])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "rs2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8555740-ff8d-48a1-b823-27bde51202d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_capture.append(model_evaluation(rs2, 'Model_2_RsCV_Multi_CVEC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ee78a-6b5a-42f2-aeda-975e77901061",
   "metadata": {},
   "source": [
    "## Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c3a21-ef1f-470d-b8be-fb46e766c058",
   "metadata": {},
   "source": [
    "### 03 - Bagged Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "becd9a4c-8836-48ff-a312-35e957226203",
   "metadata": {},
   "outputs": [],
   "source": [
    "params3 = {\n",
    "     'tvec__preprocessor': [None, stem_post, lemmatize_post],\n",
    "     'tvec__max_df': [1.0, 0.9],\n",
    "     'tvec__max_features': [None, 5000],\n",
    "     'tvec__min_df': [1],\n",
    "     'tvec__ngram_range': [(1, 2), (1, 2)],\n",
    "     'tvec__stop_words': [None, 'english'],\n",
    "     'bag__estimator__max_depth': np.arange(1, 12),\n",
    "     'bag__estimator__min_samples_leaf': np.arange(1, 12, 2),\n",
    "     'bag__n_estimators': [100]\n",
    "}\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "pipe3 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('bag', BaggingClassifier(tree))\n",
    "])\n",
    "rs3 = RandomizedSearchCV(estimator=pipe3, param_distributions=params3, cv = 5, n_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4dc73be2-492c-4b22-ae7a-93290f6137f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {color: black;background-color: white;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;bag&#x27;,\n",
       "                                              BaggingClassifier(estimator=DecisionTreeClassifier()))]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={&#x27;bag__estimator__max_depth&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       "                                        &#x27;bag__estimator__min_samples_leaf&#x27;: array([ 1,  3,  5,  7,  9, 11]),\n",
       "                                        &#x27;bag__n_estimators&#x27;: [100],\n",
       "                                        &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                        &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                        &#x27;tvec__min_df&#x27;: [1],\n",
       "                                        &#x27;tvec__ngram_range&#x27;: [(1, 2), (1, 2)],\n",
       "                                        &#x27;tvec__preprocessor&#x27;: [None,\n",
       "                                                               &lt;function stem_post at 0x7fbf4dfebb50&gt;,\n",
       "                                                               &lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                        &#x27;tvec__stop_words&#x27;: [None, &#x27;english&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-70\" type=\"checkbox\" ><label for=\"sk-estimator-id-70\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;bag&#x27;,\n",
       "                                              BaggingClassifier(estimator=DecisionTreeClassifier()))]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={&#x27;bag__estimator__max_depth&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       "                                        &#x27;bag__estimator__min_samples_leaf&#x27;: array([ 1,  3,  5,  7,  9, 11]),\n",
       "                                        &#x27;bag__n_estimators&#x27;: [100],\n",
       "                                        &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                        &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                        &#x27;tvec__min_df&#x27;: [1],\n",
       "                                        &#x27;tvec__ngram_range&#x27;: [(1, 2), (1, 2)],\n",
       "                                        &#x27;tvec__preprocessor&#x27;: [None,\n",
       "                                                               &lt;function stem_post at 0x7fbf4dfebb50&gt;,\n",
       "                                                               &lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                        &#x27;tvec__stop_words&#x27;: [None, &#x27;english&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-71\" type=\"checkbox\" ><label for=\"sk-estimator-id-71\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                (&#x27;bag&#x27;, BaggingClassifier(estimator=DecisionTreeClassifier()))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-72\" type=\"checkbox\" ><label for=\"sk-estimator-id-72\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">bag: BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(estimator=DecisionTreeClassifier())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                             ('bag',\n",
       "                                              BaggingClassifier(estimator=DecisionTreeClassifier()))]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'bag__estimator__max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       "                                        'bag__estimator__min_samples_leaf': array([ 1,  3,  5,  7,  9, 11]),\n",
       "                                        'bag__n_estimators': [100],\n",
       "                                        'tvec__max_df': [1.0, 0.9],\n",
       "                                        'tvec__max_features': [None, 5000],\n",
       "                                        'tvec__min_df': [1],\n",
       "                                        'tvec__ngram_range': [(1, 2), (1, 2)],\n",
       "                                        'tvec__preprocessor': [None,\n",
       "                                                               <function stem_post at 0x7fbf4dfebb50>,\n",
       "                                                               <function lemmatize_post at 0x7fbf48805510>],\n",
       "                                        'tvec__stop_words': [None, 'english']})"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1aa572cb-6338-4f09-a200-093d3c7da77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy Scores:\n",
      "  Train: 0.98985 \n",
      "  Test: 0.79716\n",
      "  Baseline: 0.50913\n",
      "---\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8319    0.7769    0.8034       242\n",
      "           1     0.7978    0.8486    0.8224       251\n",
      "\n",
      "    accuracy                         0.8134       493\n",
      "   macro avg     0.8148    0.8127    0.8129       493\n",
      "weighted avg     0.8145    0.8134    0.8131       493\n",
      "\n",
      "\n",
      "---\n",
      "Best Parameters: \n",
      "{'tvec__stop_words': 'english', 'tvec__preprocessor': None, 'tvec__ngram_range': (1, 2), 'tvec__min_df': 1, 'tvec__max_features': 5000, 'tvec__max_df': 0.9, 'bag__n_estimators': 100, 'bag__estimator__min_samples_leaf': 1, 'bag__estimator__max_depth': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/4sqt0lt90t72lgqr2zq3p6l80000gn/T/ipykernel_33031/812298378.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_performance_capture.append(model_evaluation(rs3, 'Model_3_Bagged_Trees'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>model_params</th>\n",
       "      <th>train_acuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "      <th>best_score_CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_3_Bagged_Trees</td>\n",
       "      <td>RandomizedSearchCV(cv=5,\\n                   e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.939107</td>\n",
       "      <td>0.740365</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.766555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_name                                              model  \\\n",
       "0  Model_3_Bagged_Trees  RandomizedSearchCV(cv=5,\\n                   e...   \n",
       "\n",
       "  best_score                                       model_params  \\\n",
       "0        NaN  {'tvec__stop_words': 'english', 'tvec__preproc...   \n",
       "\n",
       "   train_acuracy  test_accuracy  baseline_accuracy  best_score_CV  \n",
       "0       0.939107       0.740365           0.509128       0.766555  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance_capture.append(model_evaluation(rs3, 'Model_3_Bagged_Trees'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ef965-d9b3-44cb-a21b-7cfad5bda13b",
   "metadata": {},
   "source": [
    "### 04 - RandomForrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5aecea7f-6b25-4c38-9c5e-d63ee8386855",
   "metadata": {},
   "outputs": [],
   "source": [
    "params4 = {\n",
    "    'tvec__preprocessor': [None, stem_post, lemmatize_post],\n",
    "     'tvec__max_df': [1.0, 0.9],\n",
    "     'tvec__max_features': [None, 5000],\n",
    "     'tvec__min_df': [1],\n",
    "     'tvec__ngram_range': [(1, 2), (1, 2)],\n",
    "     'tvec__stop_words': [None, 'english'],\n",
    "    \n",
    "     'rfc__max_depth': [10,20,30],\n",
    "     'rfc__min_samples_split': [2,4,8],\n",
    "     'rfc__n_estimators': [100,200],\n",
    "     'rfc__random_state': [2187]\n",
    "}\n",
    "\n",
    "pipe4 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rfc', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "rs4 = RandomizedSearchCV(estimator=pipe4, param_distributions=params4, n_iter = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "60098ba6-6b02-48f1-a3f1-963270a076a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;background-color: white;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;rfc&#x27;,\n",
       "                                              RandomForestClassifier())]),\n",
       "                   n_iter=1,\n",
       "                   param_distributions={&#x27;rfc__max_depth&#x27;: [10, 20, 30],\n",
       "                                        &#x27;rfc__min_samples_split&#x27;: [2, 4, 8],\n",
       "                                        &#x27;rfc__n_estimators&#x27;: [100, 200],\n",
       "                                        &#x27;rfc__random_state&#x27;: [2187],\n",
       "                                        &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                        &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                        &#x27;tvec__min_df&#x27;: [1],\n",
       "                                        &#x27;tvec__ngram_range&#x27;: [(1, 2), (1, 2)],\n",
       "                                        &#x27;tvec__preprocessor&#x27;: [None,\n",
       "                                                               &lt;function stem_post at 0x7fbf4dfebb50&gt;,\n",
       "                                                               &lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                        &#x27;tvec__stop_words&#x27;: [None, &#x27;english&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;rfc&#x27;,\n",
       "                                              RandomForestClassifier())]),\n",
       "                   n_iter=1,\n",
       "                   param_distributions={&#x27;rfc__max_depth&#x27;: [10, 20, 30],\n",
       "                                        &#x27;rfc__min_samples_split&#x27;: [2, 4, 8],\n",
       "                                        &#x27;rfc__n_estimators&#x27;: [100, 200],\n",
       "                                        &#x27;rfc__random_state&#x27;: [2187],\n",
       "                                        &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                        &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                        &#x27;tvec__min_df&#x27;: [1],\n",
       "                                        &#x27;tvec__ngram_range&#x27;: [(1, 2), (1, 2)],\n",
       "                                        &#x27;tvec__preprocessor&#x27;: [None,\n",
       "                                                               &lt;function stem_post at 0x7fbf4dfebb50&gt;,\n",
       "                                                               &lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                        &#x27;tvec__stop_words&#x27;: [None, &#x27;english&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-77\" type=\"checkbox\" ><label for=\"sk-estimator-id-77\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()), (&#x27;rfc&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-78\" type=\"checkbox\" ><label for=\"sk-estimator-id-78\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\" ><label for=\"sk-estimator-id-79\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                             ('rfc',\n",
       "                                              RandomForestClassifier())]),\n",
       "                   n_iter=1,\n",
       "                   param_distributions={'rfc__max_depth': [10, 20, 30],\n",
       "                                        'rfc__min_samples_split': [2, 4, 8],\n",
       "                                        'rfc__n_estimators': [100, 200],\n",
       "                                        'rfc__random_state': [2187],\n",
       "                                        'tvec__max_df': [1.0, 0.9],\n",
       "                                        'tvec__max_features': [None, 5000],\n",
       "                                        'tvec__min_df': [1],\n",
       "                                        'tvec__ngram_range': [(1, 2), (1, 2)],\n",
       "                                        'tvec__preprocessor': [None,\n",
       "                                                               <function stem_post at 0x7fbf4dfebb50>,\n",
       "                                                               <function lemmatize_post at 0x7fbf48805510>],\n",
       "                                        'tvec__stop_words': [None, 'english']})"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "00bc69f3-0adf-4299-855a-eab116adb882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy Scores:\n",
      "  Train: 0.98985 \n",
      "  Test: 0.79716\n",
      "  Baseline: 0.50913\n",
      "---\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8319    0.7769    0.8034       242\n",
      "           1     0.7978    0.8486    0.8224       251\n",
      "\n",
      "    accuracy                         0.8134       493\n",
      "   macro avg     0.8148    0.8127    0.8129       493\n",
      "weighted avg     0.8145    0.8134    0.8131       493\n",
      "\n",
      "\n",
      "---\n",
      "Best Parameters: \n",
      "{'tvec__stop_words': 'english', 'tvec__preprocessor': <function lemmatize_post at 0x7fbf48805510>, 'tvec__ngram_range': (1, 2), 'tvec__min_df': 1, 'tvec__max_features': 5000, 'tvec__max_df': 1.0, 'rfc__random_state': 2187, 'rfc__n_estimators': 100, 'rfc__min_samples_split': 8, 'rfc__max_depth': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAHFCAYAAABLm3WjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABkvUlEQVR4nO3dd1gU1/oH8O/Sl7ZSBEQpioigBHtPkCtKrKg/Y42BSIzGqMGeXI0aE+uNLRhLogKx5yYWookKFuwFFY2KMSrWQDAEQUCp5/cHl4krRRYWYeT7eZ55HnfmzJl3dhd4fc+ZGYUQQoCIiIiIqj2dqg6AiIiIiMqGiRsRERGRTDBxIyIiIpIJJm5EREREMsHEjYiIiEgmmLgRERERyQQTNyIiIiKZYOJGREREJBNM3IiIiIhkgokbVXuXLl3Cu+++i/r168PIyAimpqZo0aIFFi1ahL///rtSj33hwgV4e3tDpVJBoVBg2bJlWj+GQqHA7Nmztd7vi4SFhUGhUEChUODw4cNFtgsh0LBhQygUCnTu3Llcx1i5ciXCwsI02ufw4cMlxqRNgYGBUCgUMDMzQ3p6epHtd+7cgY6OjtY/n4qcX+Fndvv27XIfPyoqSvrc//rrL432LYy9cNHV1UXt2rXRu3dvxMTElDumitLG+1KZnn3Pnl2sra2rOjSNzZs3Dzt37qzqMGo0vaoOgKg03377LcaMGQM3NzdMmTIFHh4eyMnJQUxMDFavXo2TJ09ix44dlXb8ESNGICMjA1u3boWFhQWcnZ21foyTJ0+iXr16Wu+3rMzMzLBu3boiyVl0dDRu3rwJMzOzcve9cuVKWFtbIzAwsMz7tGjRAidPnoSHh0e5j1tW+vr6yM3NxbZt2xAUFKS2LTQ0FGZmZkhLS6v0OF6W9PR0jBw5Evb29vjjjz/K3c+8efPg4+ODnJwcXLhwAZ999hm8vb0RGxsLV1dXLUb86hgwYAAmTZqktk5fX7+Koim/efPmYcCAAejbt29Vh1JjMXGjauvkyZP44IMP0LVrV+zcuROGhobStq5du2LSpEnYu3dvpcZw+fJljBw5Et27d6+0Y7Rr167S+i6LQYMGYdOmTfj6669hbm4urV+3bh3at2//0hKXnJwcKBQKmJubv7T3xMDAAL1798b69evVEjchBMLCwjBo0CB8++23LyWWl+Hjjz+GhYUFevbsiS+++KLc/bi6ukqf0euvv45atWohICAAGzduxGeffaatcF8ptra2lfK9zsvLQ25urtrvx0KZmZkwNjbW+jGpanGolKqtefPmQaFQ4Jtvvin2l5KBgQH69Okjvc7Pz8eiRYvQuHFjGBoawsbGBu+88w7u37+vtl/nzp3RtGlTnD17Fq+//jqMjY3RoEEDLFiwAPn5+QD+GXrJzc3FqlWrpKENAJg9e7b072cVN1xz8OBBdO7cGVZWVlAqlXB0dMT//d//ITMzU2pT3FDc5cuX4e/vDwsLCxgZGaFZs2YIDw9Xa1M4bLVlyxZMnz4d9vb2MDc3h6+vL3777beyvckAhgwZAgDYsmWLtC41NRU//vgjRowYUew+n332Gdq2bQtLS0uYm5ujRYsWWLduHYQQUhtnZ2dcuXIF0dHR0vtXWLEsjH3Dhg2YNGkS6tatC0NDQ9y4caPIUOJff/0FBwcHdOjQATk5OVL/V69ehYmJCYYPH17mcy3OiBEjcOLECbX3LCoqCnfu3MG7775b7D5l+XwA4Nq1a3jzzTdhbGwMa2trjB49Go8fPy62z6ioKHTp0gXm5uYwNjZGx44dceDAgQqd27OOHj2Kb775BmvXroWurq7W+gWAVq1aAQD+/PNPtfVl+Z4ABd+VXr16Ye/evWjRogWUSiUaN26M9evXFznWqVOn0LFjRxgZGcHe3h6ffPKJ2veikKa/D06ePIkOHTpAqVTC2dkZoaGhAIA9e/agRYsWMDY2hqenZ6X9Z/Hu3bt4++23YWNjA0NDQ7i7u2Px4sXS7yQAuH37NhQKBRYtWoQvvvgC9evXh6GhIQ4dOiT9Xjp//jwGDBgACwsLuLi4ACj4j8jKlSvRrFkzKJVKWFhYYMCAAbh165ZaDBcuXECvXr2kGOzt7dGzZ0/pPVMoFMjIyEB4eLj0M13eaRRUAYKoGsrNzRXGxsaibdu2Zd7n/fffFwDE2LFjxd69e8Xq1atF7dq1hYODg3j48KHUztvbW1hZWQlXV1exevVqERkZKcaMGSMAiPDwcCGEEElJSeLkyZMCgBgwYIA4efKkOHnypBBCiFmzZonifnRCQ0MFABEfHy+EECI+Pl4YGRmJrl27ip07d4rDhw+LTZs2ieHDh4uUlBRpPwBi1qxZ0utr164JMzMz4eLiIr777juxZ88eMWTIEAFALFy4UGp36NAhAUA4OzuLYcOGiT179ogtW7YIR0dH4erqKnJzc0t9vwrjPXv2rBg+fLho06aNtG3VqlXCxMREpKWliSZNmghvb2+1fQMDA8W6detEZGSkiIyMFJ9//rlQKpXis88+k9qcP39eNGjQQDRv3lx6/86fP68We926dcWAAQNERESE2L17t0hOTpa2HTp0SOrr2LFjQk9PT0yYMEEIIURGRobw8PAQjRs3Funp6UXek2ffz5IEBAQIExMTkZ+fL5ycnMTUqVOlbYMGDRJvvPGGePjwYbk/n8TERGFjYyPq1q0rQkNDxc8//yyGDRsmHB0di5zfhg0bhEKhEH379hXbt28XP/30k+jVq5fQ1dUVUVFRRT6zwu9YWWVmZgpXV1cxZcoUIcQ/3+Fnfy7KovD9/e9//6u2fvfu3QKAWLx4sdr6snxPhBDCyclJ1KtXT3h4eIjvvvtO7Nu3T7z11lsCgIiOjpbaXblyRRgbGwsPDw+xZcsWsWvXLuHn5ye9p8++L5r+PnBzcxPr1q0T+/btE7169RIAxGeffSY8PT3Fli1bxM8//yzatWsnDA0NxYMHDzR63wCIMWPGiJycHLUlPz9fCFHw+6Zu3bqidu3aYvXq1WLv3r1i7NixAoD44IMPpH7i4+OlnxsfHx/xww8/iP3794v4+HjpM3VychLTpk0TkZGRYufOnUIIIUaOHCn09fXFpEmTxN69e8XmzZtF48aNha2trUhMTBRCCJGeni6srKxEq1atxPfffy+io6PFtm3bxOjRo8XVq1eFEEKcPHlSKJVK0aNHD+ln+sqVKxq9F1RxTNyoWkpMTBQAxODBg8vUPi4uTvrl+KzTp08LAOLf//63tM7b21sAEKdPn1Zr6+HhIfz8/NTWARAffvih2rqyJm4//PCDACBiY2NLjf35xGDw4MHC0NBQ3L17V61d9+7dhbGxsXj06JEQ4p8/oj169FBr9/333wsAUqJZkmcTt8K+Ll++LIQQonXr1iIwMFAIIYpN3J6Vl5cncnJyxJw5c4SVlZX0x6i0fQuP98Ybb5S47dnERgghFi5cKACIHTt2iICAAKFUKsWlS5fU2hw+fFjo6uoWSQyKU5i4CVHwmdrZ2YmcnByRnJwsDA0NRVhYWLGJW1k/n2nTpgmFQlHk8+/atava+WVkZAhLS0vRu3dvtXZ5eXnCy8tLLaEub+I2adIk0aBBA5GZmSmdb0USt23btomcnByRmZkpjh8/Ltzc3ISHh4faf0ieV9r3xMnJSRgZGYk7d+5I6548eSIsLS3FqFGjpHWDBg0SSqVSSjaEKPhPXuPGjdXel/L8PoiJiZHWJScnC11dXaFUKtWStNjYWAFAfPXVV2V/00TBz3hxy7fffiuEEOLjjz8u9nfSBx98IBQKhfjtt9+EEP8kbi4uLiI7O1utbeFnOnPmTLX1hf8BfT6pvnfvnlAqldJ/WGJiYgQAKdkriYmJiQgICNDo/Em7OFRKr4RDhw4BQJFJ8G3atIG7u3uRISc7Ozu0adNGbd1rr72GO3fuaC2mZs2awcDAAO+//z7Cw8OLDEuU5ODBg+jSpQscHBzU1gcGBiIzMxMnT55UW//scDFQcB4ANDoXb29vuLi4YP369fj1119x9uzZEodJC2P09fWFSqWCrq4u9PX1MXPmTCQnJyMpKanMx/2///u/MredMmUKevbsiSFDhiA8PBwhISHw9PQsch65ubmYOXNmmfsFgHfffRd//vknfvnlF2zatAkGBgZ46623im1b1s/n0KFDaNKkCby8vNTaDR06VO31iRMn8PfffyMgIAC5ubnSkp+fjzfffBNnz55FRkaGRufzrDNnzmDZsmVYs2YNlEpluft51qBBg6Cvry8N6aalpWHPnj2oVauWWjtNvifNmjWDo6Oj9NrIyAiNGjVS+x4fOnQIXbp0ga2trbROV1cXgwYNUutL098HderUQcuWLaXXlpaWsLGxQbNmzWBvby+td3d3B6DZz1ahgQMH4uzZs2pL4QT/gwcPwsPDo8jvpMDAQAghcPDgQbX1ffr0KfHChud/pnbv3g2FQoG3335b7ftlZ2cHLy8vaUpCw4YNYWFhgWnTpmH16tW4evWqxudILwcTN6qWrK2tYWxsjPj4+DK1T05OBlDwC/h59vb20vZCVlZWRdoZGhriyZMn5Yi2eC4uLoiKioKNjQ0+/PBDuLi4wMXFBcuXLy91v+Tk5BLPo3D7s54/l8L5gJqci0KhwLvvvouNGzdi9erVaNSoEV5//fVi2545cwbdunUDUHDV7/Hjx3H27FlMnz5d4+MWd56lxRgYGIinT5/Czs6uwnPbnuXk5IQuXbpg/fr1WL9+PQYPHlzipO6yfj7Jycmws7Mr0u75dYXzwgYMGAB9fX21ZeHChRBCVOi2NyNGjED//v3RqlUrPHr0CI8ePcLTp08BAGlpaSXOuSvNwoULcfbsWURHR2P69On4888/0bdvX2RlZUltNP2elOVnsqzvqaa/DywtLYu0MzAwKLLewMAAAKT3TxO1a9dGq1at1JbC24Fo+jNf2s/N89v+/PNPCCFga2tb5Pt16tQp6ZYwKpUK0dHRaNasGf7973+jSZMmsLe3x6xZs4qdQ0hVh1eVUrWkq6uLLl264JdffsH9+/dfeLuMwl/6CQkJRdr+8ccfWr1fkpGREQAgKytL7aKJ4u6J9frrr+P1119HXl4eYmJiEBISguDgYNja2mLw4MHF9m9lZYWEhIQi6wtv31BZ934KDAzEzJkzsXr1asydO7fEdlu3boW+vj52794tvRcAynVvp+Iu8ihJQkICPvzwQzRr1gxXrlzB5MmT8dVXX2l8zJKMGDECb7/9NvLz87Fq1aoS25X187GyskJiYmKRds+vK2wfEhJS4lWHz1aYNHXlyhVcuXIF//3vf4tsc3FxgZeXF2JjYzXqs0GDBtIFCW+88QaUSiVmzJiBkJAQTJ48GYB2vyeFyvqevszfB9qg6c98aT83z2+ztraGQqHA0aNHi73I69l1np6e2Lp1K4QQuHTpEsLCwjBnzhwolUp8/PHHGp0TVR5W3Kja+uSTTyCEwMiRI5GdnV1ke05ODn766ScAwL/+9S8AwMaNG9XanD17FnFxcejSpYvW4iq8MvLSpUtq6wtjKY6uri7atm2Lr7/+GgBw/vz5Ett26dIFBw8eLHKfre+++w7GxsaVdquMunXrYsqUKejduzcCAgJKbKdQKKCnp6d2ZeKTJ0+wYcOGIm21VcXMy8vDkCFDoFAo8Msvv2D+/PkICQnB9u3bK9x3oX79+qFfv34YMWJEqe9xWT8fHx8fXLlyBRcvXlRrt3nzZrXXHTt2RK1atXD16tUiFZnCpbDSUx6HDh0qshR+vjt37sTatWvL3XehqVOnomHDhliwYIFUwdPke1JWPj4+OHDggNrVq3l5edi2bZtau5f5+0AbunTpgqtXrxb5vfDdd99BoVDAx8en3H336tULQgg8ePCg2O/W89MNgILPzsvLC0uXLkWtWrXU4tL2yARpjhU3qrbat2+PVatWYcyYMWjZsiU++OADNGnSRLrp5zfffIOmTZuid+/ecHNzw/vvv4+QkBDo6Oige/fuuH37Nj799FM4ODhgwoQJWourR48esLS0RFBQEObMmQM9PT2EhYXh3r17au1Wr16NgwcPomfPnnB0dMTTp0+l2xv4+vqW2P+sWbOwe/du+Pj4YObMmbC0tMSmTZuwZ88eLFq0CCqVSmvn8rwFCxa8sE3Pnj2xZMkSDB06FO+//z6Sk5Px5ZdfFvu/+cL/wW/btg0NGjSAkZFRsX8oXmTWrFk4evQo9u/fDzs7O0yaNAnR0dEICgpC8+bNUb9+fQAFNw3u0qULZs6cqfE8NyMjI/zwww9liqUsn09wcDDWr18v3TPN1tYWmzZtwrVr19T6MzU1RUhICAICAvD3339jwIABsLGxwcOHD3Hx4kU8fPiw1ArgixR3u4bCeU0dO3bUSvVJX18f8+bNw8CBA7F8+XLMmDFDo+9JWc2YMQMRERH417/+hZkzZ8LY2Bhff/11kTmAL/P3gTZMmDAB3333HXr27Ik5c+bAyckJe/bswcqVK/HBBx+gUaNG5e67Y8eOeP/99/Huu+8iJiYGb7zxBkxMTJCQkIBjx47B09MTH3zwAXbv3o2VK1eib9++aNCgAYQQ2L59Ox49eoSuXbtK/Xl6euLw4cP46aefUKdOHZiZmcHNzU0bbwOVVRVeGEFUJrGxsSIgIEA4OjoKAwMDYWJiIpo3by5mzpwpkpKSpHZ5eXli4cKFolGjRkJfX19YW1uLt99+W9y7d0+tP29vb9GkSZMixwkICBBOTk5q61DMVaVCCHHmzBnRoUMHYWJiIurWrStmzZol1q5dq3Zl28mTJ0W/fv2Ek5OTMDQ0FFZWVsLb21tEREQUOcbzt6/49ddfRe/evYVKpRIGBgbCy8tLhIaGqrUp6dYMhVeePd/+ec9eVVqa4q4MXb9+vXBzcxOGhoaiQYMGYv78+WLdunVFrni8ffu26NatmzAzM5NuVVBa7M9uK7zqcv/+/UJHR6fIe5ScnCwcHR1F69atRVZWltq+mtwOpDTFXVUqRNk+HyGEuHr1qujataswMjISlpaWIigoSOzatavYq2ajo6NFz549haWlpdDX1xd169YVPXv2VHuPyntV6fO0fTuQQm3bthUWFhbSlbVl/Z44OTmJnj17FunP29u7yHfv+PHj0m057OzsxJQpU8Q333xTpM+K/j4oKaaSfieUpiz73LlzRwwdOlRYWVkJfX194ebmJv7zn/+IvLw8qU3hz/Z//vOfIvu/6DNdv369aNu2rTAxMRFKpVK4uLiId955R7qa9tq1a2LIkCHCxcVFKJVKoVKpRJs2bURYWJhaP7GxsaJjx47C2NhYACj1inOqHAohnrsTIhERERFVS5zjRkRERCQTnONGRCRDQgjk5eWV2kZXV7dMV+5qs6+aJDc3t9TtOjo60NFhfYS0i98oIiIZCg8PL3JfrueX6Ojol95XTXH79u0Xvmdz5syp6jDpFcQ5bkREMpScnPzCG1S7ubnBzMzspfZVU2RnZxe5JdDz7O3t1Z68QKQNTNyIiIiIZIJDpUREREQywYsTqFrIz8/HH3/8ATMzM06AJiKSGSEEHj9+DHt7+0q9IOPp06fFPkmnPAwMDNQexyYXTNyoWvjjjz/g4OBQ1WEQEVEF3Lt374XPli6vp0+fwtJUiSelXwBdZnZ2doiPj5dd8sbEjaqFwknPMzx0YKTLihu9msZFn6nqEIgqRVpaOhycvSv1Apbs7Gw8yQOGOuvDoIJFvex8YPPtRGRnZzNxIyqPwuFRI10FEzd6ZZmbm1Z1CESV6mVMdTHSAQwq+HdCB/K9LpOJGxEREcmGQlGwVLQPuWLiRkRERLKhg4rfEkPOt9SQc+xERERElWr+/Plo3bo1zMzMYGNjg759++K3335TayOEwOzZs2Fvbw+lUonOnTvjypUram2ysrIwbtw4WFtbw8TEBH369MH9+/c1joeJGxEREclG4VBpRZeyio6OxocffohTp04hMjISubm56NatGzIyMqQ2ixYtwpIlS7BixQqcPXsWdnZ26Nq1Kx4/fiy1CQ4Oxo4dO7B161YcO3YM6enp6NWr1wufE/w8DpUSERGRbChQ8aqTJlPc9u7dq/Y6NDQUNjY2OHfuHN544w0IIbBs2TJMnz4d/fv3B1Dw/F9bW1ts3rwZo0aNQmpqKtatW4cNGzbA19cXALBx40Y4ODggKioKfn5+ZY6HFTciIiKiMkpNTQUAWFpaAgDi4+ORmJiIbt26SW0MDQ3h7e2NEydOAADOnTuHnJwctTb29vZo2rSp1KasWHEjIiIi2dBRFCwV7QMA0tLS1NYbGhrC0NCwxP2EEJg4cSI6deqEpk2bAgASExMBALa2tmptbW1tcefOHamNgYEBLCwsirQp3L/MsWvUmoiIiKgKKbS0AICDgwNUKpW0zJ8/v9Rjjx07FpcuXcKWLVuKxvXcxDkhxAvva1eWNs9jxY2IiIhqpHv37sHc3Fx6XVq1bdy4cYiIiMCRI0fUHutlZ2cHoKCqVqdOHWl9UlKSVIWzs7NDdnY2UlJS1KpuSUlJ6NChg0Yxs+JGREREsqGjEFpZAMDc3FxtKS5xE0Jg7Nix2L59Ow4ePIj69eurba9fvz7s7OwQGRkprcvOzkZ0dLSUlLVs2RL6+vpqbRISEnD58mWNEzdW3IiIiEg2nh3qrEgfZfXhhx9i8+bN2LVrF8zMzKQ5aSqVCkqlEgqFAsHBwZg3bx5cXV3h6uqKefPmwdjYGEOHDpXaBgUFYdKkSbCysoKlpSUmT54MT09P6SrTsmLiRkRERFSCVatWAQA6d+6stj40NBSBgYEAgKlTp+LJkycYM2YMUlJS0LZtW+zfvx9mZmZS+6VLl0JPTw8DBw7EkydP0KVLF4SFhUFXV1ejeBRCCPk+aZVeGWlpaVCpVPjCU5cPmadX1qSYKy9uRCRDaWnpUFm2RGpqqtqcMe0eo+DvxEduejCs4N+JrDyB5b/lVmq8lYUVNyIiIpKNmv6sUiZuREREJBuaPrKqpD7kSs5JJxEREVGNwoobERERyQaHSomIiIhkgkOlRERERCQLrLgRERGRbHColIiIiEgmFApAh0OlRERERFTdseJGREREsvGyn1Va3TBxIyIiItmo6XPc5Bw7ERERUY3CihsRERHJRk2/jxsTNyIiIpKNmj5UysSNiIiIZENHC7cDqej+VUnOSScRERFRjcKKGxEREckGbwdCREREJBMcKiUiIiIiWWDFjYiIiGRDAaGFoVKhlViqAhM3IiIikg0OlRIRERGRLLDiRkRERLLBG/ASERERyURNf+SVnJNOIiIiohqFFTciIiKSDQUqXnWSccGNiRsRERHJR00fKmXiRkRERLJR0y9OkHPsRERERDUKK25EREQkGzX9BrxM3IiIiEg2FKj4xQUyzts4VEpEREQkF6y4ERERkWxwqJSIiIhIJmr67UA4VEpEREQkE6y4ERERkWzwPm5EREREMqGDf+a5lXvR8JhHjhxB7969YW9vD4VCgZ07d6ptT09Px9ixY1GvXj0olUq4u7tj1apVam2ysrIwbtw4WFtbw8TEBH369MH9+/fLdf5EREREVIKMjAx4eXlhxYoVxW6fMGEC9u7di40bNyIuLg4TJkzAuHHjsGvXLqlNcHAwduzYga1bt+LYsWNIT09Hr169kJeXp1EsHColIiIi2aiKixO6d++O7t27l7j95MmTCAgIQOfOnQEA77//PtasWYOYmBj4+/sjNTUV69atw4YNG+Dr6wsA2LhxIxwcHBAVFQU/P78yx8KKGxEREclGhYdJtXA7ked16tQJERERePDgAYQQOHToEK5fvy4lZOfOnUNOTg66desm7WNvb4+mTZvixIkTGh2LFTciIiKSFW3lXWlpaWqvDQ0NYWhoqHE/X331FUaOHIl69epBT08POjo6WLt2LTp16gQASExMhIGBASwsLNT2s7W1RWJiokbHYsWNiIiIaiQHBweoVCppmT9/frn6+eqrr3Dq1ClERETg3LlzWLx4McaMGYOoqKhS9xNCQKHhuC0rbkRERCQbOgqhhScnCADAvXv3YG5uLq0vT7XtyZMn+Pe//40dO3agZ8+eAIDXXnsNsbGx+PLLL+Hr6ws7OztkZ2cjJSVFreqWlJSEDh06aBa7xhESERERVRFtznEzNzdXW8qTuOXk5CAnJwc6Ouopla6uLvLz8wEALVu2hL6+PiIjI6XtCQkJuHz5ssaJGytuRERERKVIT0/HjRs3pNfx8fGIjY2FpaUlHB0d4e3tjSlTpkCpVMLJyQnR0dH47rvvsGTJEgCASqVCUFAQJk2aBCsrK1haWmLy5Mnw9PSUrjItKyZuREREJBtVcTuQmJgY+Pj4SK8nTpwIAAgICEBYWBi2bt2KTz75BMOGDcPff/8NJycnzJ07F6NHj5b2Wbp0KfT09DBw4EA8efIEXbp0QVhYGHR1dTWLXQghNAufSPvS0tKgUqnwhacujHRl/PRfolJMirlS1SEQVYq0tHSoLFsiNTVVbc6Ydo9R8HcivJ0OjPUq9nciM1cg4FR+pcZbWTjHjYiIiEgmOFRKREREslEVQ6XVCRM3IiIikg1tPPlA209OeJk4VEpEREQkE6y4ERERkWzU9IobEzciIiKSDQUq/qxSGedtTNyIiIhIPmp6xY1z3IiIiIhkghU3IiIikg3eDoSIiIhIJjhUSkRERESywIobERERyYYCFa86ybjgxsSNiIiI5KOmz3HjUCkRERGRTLDiRkRERLJR0y9OYOJGREREssGhUiIiIiKSBVbciIiISDZ0UPGqk5yrVkzciIiISDZ0FEILc9yEdoKpAkzciIiISDY4x42IiIiIZIEVNyIiIpIN3g6EiIiISCYUqPgjq2Sct3GolIiIiEguWHF7BYSFhSE4OBiPHj2qFv1Q1ajbog1avzMKth6eMK1ti10TRuLG4f3Sdn2lMV4f/zEa+nSDkcoCaX/cx4Wtobj4341SG1U9R3hPmI66zVtDV98At09E4+DCWcj8+6+qOCWiUv20YiV2f71KbZ25tRX+c/SwtP3sz78gJfFP6OnrwdHDA32Dx6O+12svP1jSGh1oYahUK5FUDTnHXqPUr18fe/fu1Vp/zs7OWLZsmdq6QYMG4fr161o7Br1c+kpjPLwehwMLZha7vfPkmXDu4I2fpwcjrH8XnNu0Fv+a+hlcOncFAOgZKTFg5UZAAP99fwi2vvt/0NXXR9/l6+R9CRa90uwbNsSiI4ekZeau7dI2W2cnDJnxb8zc9SOmbPwOVnXrYtl7o/D477+rMGKqqMI5bhVd5IoVt2osOzsbBgYGuHTpEpKTk+Hj41Opx1MqlVAqlZV6DKo8t48fxu3jh0vcbv9aC1zd/SPunzsFAPh1+xZ4/d8w2Hq8hpuHI1G3WSuY29fDhiE9kJ2RDgDYO2syxh75FY5tOuDu6eMv4zSINKKjpwtVbetit7Xp1VPt9VsfT8HxH7fj/m/X4d6+3csIj0jrWHGrRjp37oyxY8di4sSJsLa2RteuBZWQXbt2wc/PD4aGhgAKhjQdHR1hbGyMfv36ITk5Wa2fmzdvwt/fH7a2tjA1NUXr1q0RFRWldpw7d+5gwoQJUCgUUPyvmhIWFoZatWpJ7WbPno1mzZphw4YNcHZ2hkqlwuDBg/H48WOpzePHjzFs2DCYmJigTp06WLp0KTp37ozg4OBKepeovB7EnoWLty9Ma9sCABxatYeFU33cORENANA1MACEQF52trRPXnYW8vPyULdZ6yqJmehFku7cxdQ3/oV/+76JbydOwcN794ptl5udg6Pf/wClmRkcGru95ChJqxT/3MutvIucr05g4lbNhIeHQ09PD8ePH8eaNWsAABEREfD39wcAnD59GiNGjMCYMWMQGxsLHx8ffPHFF2p9pKeno0ePHoiKisKFCxfg5+eH3r174+7duwCA7du3o169epgzZw4SEhKQkJBQYjw3b97Ezp07sXv3buzevRvR0dFYsGCBtH3ixIk4fvw4IiIiEBkZiaNHj+L8+fPafltICw4unI3kW79j1P4zCD5zA/2/DkfU/Bl4EBsDAEj49QJynmTi9Y8+hp6REfSMlHgjeDp0dHVhYm1TxdETFVX/NU+8u2AuPlq7GsPnzELaX39h0dDhSE95JLW5dCga41u2wdhmLXEgfAOC130DUwuLqguaKkxHS4tccai0mmnYsCEWLVokvX7w4AEuXryIHj16AACWL18OPz8/fPzxxwCARo0a4cSJE2rz37y8vODl5SW9/uKLL7Bjxw5ERERg7NixsLS0hK6uLszMzGBnZ1dqPPn5+QgLC4OZmRkAYPjw4Thw4ADmzp2Lx48fIzw8HJs3b0aXLl0AAKGhobC3t3/heWZlZSErK0t6nZaW9sJ9qGJaDHkXdTybY8dHI5CW8AD1WrSF7ydfIOOvJNw9fRxPUv7GT1PHwPffc9FiyLsQ+fm4tjcCf179FSI/v6rDJyqi6RuvS/+u2who0MwLM/x64OSuXegaGAAAcGvbGjO2/4D0lBQc+++P+GbCZHy8bRPMrayqKmyiCpFz0vlKatWqldrriIgIdOzYEZaWlgCAuLg4tG/fXq3N868zMjIwdepUeHh4oFatWjA1NcW1a9ekipsmnJ2dpaQNAOrUqYOkpCQAwK1bt5CTk4M2bdpI21UqFdzcXjwMMX/+fKhUKmlxcHDQODYqOz1DQ3QaNwWHF3+BW0cO4K/fryF2Wzh+278brYa/L7W7c+oo1vV5A6u6tMBKn+b45dMJMLWxReqD4oefiKoTQ2Nj1HV1RdLtu2rrbJwc0aCZF96ZOwe6uro4/uOOKoySKqqiw6TaeGRWVWLiVs2YmJiovX52mBQAhHjxg3GnTJmCH3/8EXPnzsXRo0cRGxsLT09PZD8zd6ms9PX11V4rFArk/6/6UhiL4rmfgLLE+MknnyA1NVVa7pUwL4W0Q0dPH7r6BhBCvXKWn5cHhU7RXwNPHqUgKz0NDq07wNjSGjejI19WqETllpOdjYRbt0q8WAEABARyy/G7kKqPwrnZFV3kikOl1Vh6ejoOHTqEr7/+Wlrn4eGBU6dOqbV7/vXRo0cRGBiIfv36Sf3cvn1brY2BgQHy8vIqFJ+Liwv09fVx5swZqWKWlpaG33//Hd7e3qXua2hoKF1sQdqhrzRGLQdn6bV5XQfUbuSBp2mP8DjxD9yLOQnv4H8j9+lTpCU8gEPLtvDo9X+IXvK5tE+TPm/h7/gbyExJhv1rLeEzZRbObVqHlDu3quCMiEr3w6Iv8Vpnb1ja18Hj5L+xZ/U3eJqegfZ9/ZGVmYmf13wLL5/OUNWujYxHj3B4yzakJP6Jln7dqjp0qgCFTsFS0T7kiolbNbZ37164urqiQYMG0rrx48ejQ4cOWLRoEfr27Yv9+/cXub9bw4YNsX37dvTu3RsKhQKffvqpVCUr5OzsjCNHjmDw4MEwNDSEtXXJ/0MtiZmZGQICAjBlyhRYWlrCxsYGs2bNgo6Ojqz/NyNXth6vYdDabdJrn8kF93O7HPFf7Js1Gbs/HofXx01Fj3nLYWReC48T7uP41/9RuwGvpXMDvD5uKoxUtZD6x32cXrcC5zaufennQlQWKYl/Yu3kaUh/lAIzC0vU93oN07ZuglVde+RkZSHxVjxO7YxAekoKTGrVgrNnE0zZGA5714ZVHTpRuTFxq8Z27dqlNkwKAO3atcPatWsxa9YszJ49G76+vpgxYwY+//yfqsnSpUsxYsQIdOjQAdbW1pg2bVqRyf9z5szBqFGj4OLigqysrDINbxZnyZIlGD16NHr16gVzc3NMnToV9+7dg5GRUbn6o/K7f+4UFjd3KnF7ZvJD7Js9pdQ+jn61EEe/Wqjt0Igqxcgl/ylxm76hIT4IWfbygqGXRhtDnXKuLShEef9iU6XKy8uDjY0NfvnlF7XJ/9VdRkYG6tati8WLFyMoKKjM+6WlpUGlUuELT10Y6cr4J4qoFJNirlR1CESVIi0tHSrLlkhNTYW5uXklHaPg78SxXrow1a/Y34n0HIFOu/MqNd7KIuNR3ldbcnIyJkyYgNatq/eNTy9cuIAtW7bg5s2bOH/+PIYNGwYARSqFREREcnXkyBH07t0b9vb2UCgU2LlzZ5E2cXFx6NOnD1QqFczMzNCuXTu1uzlkZWVh3LhxsLa2homJCfr06YP79+9rHAsTt2rKxsYGM2bMkMVcsS+//BJeXl7w9fVFRkYGjh49Wq45c0RERC9SFVeVZmRkwMvLCytWrCh2+82bN9GpUyc0btwYhw8fxsWLF/Hpp5+qTRsKDg7Gjh07sHXrVhw7dgzp6eno1auXxhcKco4bVUjz5s1x7ty5qg6DiIhqiKqY49a9e3d07969xO3Tp09Hjx491G6g/+yFhampqVi3bh02bNgAX19fAMDGjRvh4OCAqKgo+Pn5lTkWVtyIiIiIyik/Px979uxBo0aN4OfnBxsbG7Rt21ZtOPXcuXPIyclBt27/3IrG3t4eTZs2xYkTJzQ6HhM3IiIikg1tPjkhLS1NbXn2UYxllZSUhPT0dCxYsABvvvkm9u/fj379+qF///6Ijo4GACQmJsLAwAAWzz0n19bWFomJiRodj0OlREREJBvaHCp9/nGLhbfa0kThfVL9/f0xYcIEAECzZs1w4sQJrF69utQb0gshND4XJm5ERERUI927d0/tdiDleaKPtbU19PT04OHhobbe3d0dx44dAwDY2dkhOzsbKSkpalW3pKQkdOjQQaPjcaiUiIiIZEObQ6Xm5uZqS3kSNwMDA7Ru3Rq//fab2vrr16/DyangpugtW7aEvr4+IiP/ee5zQkICLl++rHHixoobERERyYZCRwGFTgWHSjUsW6Wnp+PGjRvS6/j4eMTGxsLS0hKOjo6YMmUKBg0ahDfeeAM+Pj7Yu3cvfvrpJxw+fBgAoFKpEBQUhEmTJsHKygqWlpaYPHkyPD09patMy4qJGxEREcnGsxWzivShiZiYGPj4+EivJ06cCAAICAhAWFgY+vXrh9WrV2P+/PkYP3483Nzc8OOPP6JTp07SPkuXLoWenh4GDhyIJ0+eoEuXLggLC4Ourq5msfORV1Qd8JFXVBPwkVf0qnqZj7w6O8BQK4+8av1DliwfecWKGxEREclGTX/IPBM3IiIiko2qGCqtTnhVKREREZFMsOJGREREsqGAFoZKtRRLVWDiRkRERPKhhTlucs7cOFRKREREJBOsuBEREZFs1PSLE5i4ERERkWzU9NuBcKiUiIiISCZYcSMiIiLZUOho/qzR4vqQKyZuREREJBs1faiUiRsRERHJRk2/OEHGxUIiIiKimoUVNyIiIpINDpUSERERyURNT9w4VEpEREQkE6y4ERERkWzU9IsTmLgRERGRbHColIiIiIhkgRU3IiIikg0+OYGIiIhIJjhUSkRERESywIobERERyQavKiUiIiKSiZo+VMrEjYiIiGSjoOJW0cRNaCmal49z3IiIiIhkghU3IiIikg0FtDDHTSuRVA0mbkRERCQb2pnjJt/UjUOlRERERDLBihsRERHJBm8HQkRERCQXOgoodCqYeVV0/yrEoVIiIiIimWDFjYiIiOSjho+VMnEjIiIi2ajheRsTNyIiIpIRHUXF56hxjhsRERERVTYmbkRERCQbhTfgreiiiSNHjqB3796wt7eHQqHAzp07S2w7atQoKBQKLFu2TG19VlYWxo0bB2tra5iYmKBPnz64f/++xufPxI2IiIhko3COW0UXTWRkZMDLywsrVqwotd3OnTtx+vRp2NvbF9kWHByMHTt2YOvWrTh27BjS09PRq1cv5OXlaRQL57gRERERlaJ79+7o3r17qW0ePHiAsWPHYt++fejZs6fattTUVKxbtw4bNmyAr68vAGDjxo1wcHBAVFQU/Pz8yhwLK25EREQkH1VRcnuB/Px8DB8+HFOmTEGTJk2KbD937hxycnLQrVs3aZ29vT2aNm2KEydOaHQsVtyIiIhINhRaeHJC4f5paWlq6w0NDWFoaKhxfwsXLoSenh7Gjx9f7PbExEQYGBjAwsJCbb2trS0SExM1OhYrbkRERFQjOTg4QKVSScv8+fM17uPcuXNYvnw5wsLCNL7oQQih8T6suBEREZF8KP63VLQPAPfu3YO5ubm0ujzVtqNHjyIpKQmOjo7Sury8PEyaNAnLli3D7du3YWdnh+zsbKSkpKhV3ZKSktChQweNjlemxO2rr74qc4cllQmJiIiIKqo8t/Morg8AMDc3V0vcymP48OHSBQeF/Pz8MHz4cLz77rsAgJYtW0JfXx+RkZEYOHAgACAhIQGXL1/GokWLNDpemRK3pUuXlqkzhULBxI2IiIheKenp6bhx44b0Oj4+HrGxsbC0tISjoyOsrKzU2uvr68POzg5ubm4AAJVKhaCgIEyaNAlWVlawtLTE5MmT4enpWSTpe5EyJW7x8fEadUpERERUKXRQ8Rn6Gu4fExMDHx8f6fXEiRMBAAEBAQgLCytTH0uXLoWenh4GDhyIJ0+eoEuXLggLC4Ourq5GsZR7jlt2djbi4+Ph4uICPT1OlSMiIqLKp4AWhko1nCTXuXNnCCHK3P727dtF1hkZGSEkJAQhISEaHft5GuesmZmZCAoKgrGxMZo0aYK7d+8CKJjbtmDBggoFQ0RERFSaqnjkVXWiceL2ySef4OLFizh8+DCMjIyk9b6+vti2bZtWgyMiIiKif2g8xrlz505s27YN7dq1U8tYPTw8cPPmTa0GR0RERKRGi7cDkSONE7eHDx/CxsamyPqMjAxZlx6JiIio+tPmkxPkSOOh0tatW2PPnj3S68Jk7dtvv0X79u21FxkRERERqdG44jZ//ny8+eabuHr1KnJzc7F8+XJcuXIFJ0+eRHR0dGXESERERFRAGw+Jl/EIocYVtw4dOuD48ePIzMyEi4sL9u/fD1tbW5w8eRItW7asjBiJiIiIAPyTt1V0katy3YDN09MT4eHh2o6FiIiIiEpRrsQtLy8PO3bsQFxcHBQKBdzd3eHv788b8RIREVHl0lEULBXtQ6Y0zrQuX74Mf39/JCYmSs/gun79OmrXro2IiAh4enpqPUgiIiIiQLsPmZcjjee4vffee2jSpAnu37+P8+fP4/z587h37x5ee+01vP/++5URIxERERGhHBW3ixcvIiYmBhYWFtI6CwsLzJ07F61bt9ZqcERERETPquEXlWpecXNzc8Off/5ZZH1SUhIaNmyolaCIiIiIilXDLystU8UtLS1N+ve8efMwfvx4zJ49G+3atQMAnDp1CnPmzMHChQsrJ0oiIiIi8MkJZUrcatWqpTaRTwiBgQMHSuuEEACA3r17Iy8vrxLCJCIiIqIyJW6HDh2q7DiIiIiIXowPmX8xb2/vyo6DiIiI6IVq+u1Ayn3H3MzMTNy9exfZ2dlq61977bUKB0VERERERWmcuD18+BDvvvsufvnll2K3c44bERERVRodaOHJCVqJpEpoHHpwcDBSUlJw6tQpKJVK7N27F+Hh4XB1dUVERERlxEhEREQE4H9T3Cp6N5CqPokK0LjidvDgQezatQutW7eGjo4OnJyc0LVrV5ibm2P+/Pno2bNnZcRJREREVONpXHHLyMiAjY0NAMDS0hIPHz4EAHh6euL8+fPajY6IiIjoWTX8BrzlenLCb7/9BgBo1qwZ1qxZgwcPHmD16tWoU6eO1gMkIiIiKlR4VWlFF7nSeKg0ODgYCQkJAIBZs2bBz88PmzZtgoGBAcLCwrQdHxERERH9j8aJ27Bhw6R/N2/eHLdv38a1a9fg6OgIa2trrQZHRERE9CyFTsFS0T7kqtz3cStkbGyMFi1aaCMWIiIiotJpY47aqz5UOnHixDJ3uGTJknIHQ0RERFQaPjmhDC5cuFCmzuT8RhARERFVd3zIPFUr445dgbm5WVWHQVQpRjVyruoQiCpFdp54eQfTUWjhyQnyLTRVeI4bERER0UtTw+e4yfi6CiIiIqKahRU3IiIiko8aXnFj4kZERETyUcPnuHGolIiIiEgmypW4bdiwAR07doS9vT3u3LkDAFi2bBl27dql1eCIiIiI1PAh85pZtWoVJk6ciB49euDRo0fIy8sDANSqVQvLli3TdnxERERE/yh85lVFF5nSOPKQkBB8++23mD59OnR1daX1rVq1wq+//qrV4IiIiIjoHxonbvHx8WjevHmR9YaGhsjIyNBKUERERETFKrw4oaKLBo4cOYLevXvD3t4eCoUCO3fulLbl5ORg2rRp8PT0hImJCezt7fHOO+/gjz/+UOsjKysL48aNg7W1NUxMTNCnTx/cv39f89PXdIf69esjNja2yPpffvkFHh4eGgdAREREVGZVMMctIyMDXl5eWLFiRZFtmZmZOH/+PD799FOcP38e27dvx/Xr19GnTx+1dsHBwdixYwe2bt2KY8eOIT09Hb169ZKmnJWVxrcDmTJlCj788EM8ffoUQgicOXMGW7Zswfz587F27VpNuyMiIiLSgDYuLtBs/+7du6N79+7FblOpVIiMjFRbFxISgjZt2uDu3btwdHREamoq1q1bhw0bNsDX1xcAsHHjRjg4OCAqKgp+fn5ljkXjxO3dd99Fbm4upk6diszMTAwdOhR169bF8uXLMXjwYE27IyIiIqoSaWlpaq8NDQ1haGhY4X5TU1OhUChQq1YtAMC5c+eQk5ODbt26SW3s7e3RtGlTnDhxQqPErVyXVYwcORJ37txBUlISEhMTce/ePQQFBZWnKyIiIqKy0+IcNwcHB6hUKmmZP39+hcN7+vQpPv74YwwdOhTm5uYAgMTERBgYGMDCwkKtra2tLRITEzXqv0JPTrC2tq7I7kRERESa0cbtPBQCAHDv3j0puQJQ4WpbTk4OBg8ejPz8fKxcufKF7YUQUGg47Ktx4la/fv1SD3Lr1i1NuyQiIiJ66czNzdUSt4rIycnBwIEDER8fj4MHD6r1a2dnh+zsbKSkpKhV3ZKSktChQweNjqNx4hYcHFwk0AsXLmDv3r2YMmWKpt0RERERlZ0OtPCsUq1EIilM2n7//XccOnQIVlZWattbtmwJfX19REZGYuDAgQCAhIQEXL58GYsWLdLoWBonbh999FGx67/++mvExMRo2h0RERFR2WnjkVUa7p+eno4bN25Ir+Pj4xEbGwtLS0vY29tjwIABOH/+PHbv3o28vDxp3pqlpSUMDAygUqkQFBSESZMmwcrKCpaWlpg8eTI8PT2lq0zLSms5Z/fu3fHjjz9qqzsiIiKiaiEmJgbNmzeXHkAwceJENG/eHDNnzsT9+/cRERGB+/fvo1mzZqhTp460nDhxQupj6dKl6Nu3LwYOHIiOHTvC2NgYP/30k9pTqMqiQhcnPOuHH36ApaWltrojIiIiKqoKKm6dO3eGEKLE7aVtK2RkZISQkBCEhIRodOznaZy4NW/eXO3iBCEEEhMT8fDhwzJdQUFERERUbuV4ZFWxfciUxolb37591V7r6Oigdu3a6Ny5Mxo3bqytuIiIiIjoORolbrm5uXB2doafnx/s7OwqKyYiIiKi4lXBUGl1otHFCXp6evjggw+QlZVVWfEQERERlazwBrwVXWRK48jbtm2LCxcuVEYsRERERKXT4iOv5EjjOW5jxozBpEmTcP/+fbRs2RImJiZq21977TWtBUdERERE/yhz4jZixAgsW7YMgwYNAgCMHz9e2qZQKKTnbeXl5Wk/SiIiIiKgxs9xK3PiFh4ejgULFiA+Pr4y4yEiIiIqGRO3sim8uZyTk1OlBUNEREREJdNojptCxhkqERERvQJ4A96ya9So0QuTt7///rtCARERERGVSBu385Dx7UA0Stw+++wzqFSqyoqFiIiIiEqhUeI2ePBg2NjYVFYsRERERC+ghYsTUAOGSjm/jYiIiKpcDZ/jVuZB3sKrSomIiIioapS54pafn1+ZcRARERG9GO/jRkRERCQTTNyIiIiIZEJHAehU8HYeNWGOGxERERFVLVbciIiISD44VEpEREQkEzU8ceNQKREREZFMsOJGRERE8lHDb8DLxI2IiIjkg0OlRERERCQHrLgRERGRfCh0CpaK9iFTTNyIiIhIPmr4HDf5ppxERERENQwrbkRERCQfHColIiIikgkmbkREREQyodAtWCrUR752YqkC8k05iYiIiGoYVtyIiIhIRnRQ8bqTfOtWTNyIiIhIRrQwx03GiZt8IyciIiKqYVhxIyIiIvlQKLRwVSlvwEtERERU+QpvB1LRRQNHjhxB7969YW9vD4VCgZ07d6ptF0Jg9uzZsLe3h1KpROfOnXHlyhW1NllZWRg3bhysra1hYmKCPn364P79+xqfPhM3IiIiolJkZGTAy8sLK1asKHb7okWLsGTJEqxYsQJnz56FnZ0dunbtisePH0ttgoODsWPHDmzduhXHjh1Deno6evXqhby8PI1i4VApERERyUcV3IC3e/fu6N69e7HbhBBYtmwZpk+fjv79+wMAwsPDYWtri82bN2PUqFFITU3FunXrsGHDBvj6+gIANm7cCAcHB0RFRcHPz6/MsbDiRkRERPKhxaHStLQ0tSUrK0vjcOLj45GYmIhu3bpJ6wwNDeHt7Y0TJ04AAM6dO4ecnBy1Nvb29mjatKnUpqyYuBEREVGN5ODgAJVKJS3z58/XuI/ExEQAgK2trdp6W1tbaVtiYiIMDAxgYWFRYpuy4lApERERyYcWh0rv3bsHc3NzabWhoWH5u3zuSlUhRJF1zytLm+ex4kZERETyocWhUnNzc7WlPImbnZ0dABSpnCUlJUlVODs7O2RnZyMlJaXENmXFxI2IiIjkowpuB1Ka+vXrw87ODpGRkdK67OxsREdHo0OHDgCAli1bQl9fX61NQkICLl++LLUpKw6VEhEREZUiPT0dN27ckF7Hx8cjNjYWlpaWcHR0RHBwMObNmwdXV1e4urpi3rx5MDY2xtChQwEAKpUKQUFBmDRpEqysrGBpaYnJkyfD09NTusq0rJi4ERERkXxUwe1AYmJi4OPjI72eOHEiACAgIABhYWGYOnUqnjx5gjFjxiAlJQVt27bF/v37YWZmJu2zdOlS6OnpYeDAgXjy5Am6dOmCsLAw6Orqaha6EEJotAdRJUhLS4NKpUJq6jWYm5u9eAciGRrVyLmqQyCqFNl5AmG3cpGamqo22V+bCv9OPDoxCuamBhXrKz0btTqsqdR4KwvnuBERERHJBIdKiYiISD5q+EPmmbgRERGRfFTBHLfqRL6RExEREdUwrLgRERGRfNTwihsTNyIiIpIPhW7BUtE+ZEq+KScRERFRDcOKGxEREckHh0qJiIiIZIKJGxEREZFM1PDETb6RExEREdUwrLgRERGRfNTwihsTNyIiIpIRLTzyCvJ95JV8U04iIiKiGoYVNyIiIpIPDpUSERERyUQNT9zkGzkRERFRDcOKGxEREclHDa+4MXEjIiIi+ajhiZt8IyciIiKqYWSZuIWFhaFWrVrVpp/qriznGRgYiL59+76UeKhq/LJ6JUY1qo9tc+dI60Y1ql/ssm/tmiqMlKh4b44ag09+3IXl5y/jPydj8MHKb2Bbv0GRdr3GBWPh0dMIuXQNEzdsRZ2GriX2OW5tGNZcvw0v326VGTppU2HFraKLTFXbyOvXr4+9e/dqrT9nZ2csW7ZMbd2gQYNw/fp1rR2jOijuPMti+fLlCAsLe2E7hUKBnTt3atw/Va3bly7i6PdbUM+tsdr6RcfPqC3vzF8EhUKBFt26V1GkRCVr1LotDm/cgAUD+2H5u8Oho6uLj9Z/BwOlUmrjN3I0fN8NwtbPZ2L+//VB2l8PERy6EYYmJkX66xIYBCHEyzwF0gYmbtVHdnY2AODSpUtITk6Gj49PpR5PqVTCxsamUo/xshS+d+WlUqlKrcpVtH+qOk8zMrBucjCGfz4fxiqV2jZV7dpqy8WoSDRq2x61HR2rKFqikn31XgBO7vgBCTd+x/1rcQj/eAqs6taDUxNPqU2XgBH4ZdXXuLB/H/74/TrCpk6CgVKJNr381fqq19gdvu8G4btPpr7s06CKYuJWdTp37oyxY8di4sSJsLa2RteuXQEAu3btgp+fHwwNDQEUDPU5OjrC2NgY/fr1Q3Jyslo/N2/ehL+/P2xtbWFqaorWrVsjKipK7Th37tzBhAkToFAooFAopH6fTVZmz56NZs2aYcOGDXB2doZKpcLgwYPx+PFjqc3jx48xbNgwmJiYoE6dOli6dCk6d+6M4OBgqU1KSgreeecdWFhYwNjYGN27d8fvv/8OAEhNTYVSqSxSTdy+fTtMTEyQnp4OAHjw4AEGDRoECwsLWFlZwd/fH7dv35baFw5tzp8/H/b29mjUqFGJ51lo3759cHd3h6mpKd58800kJCQU6a+0z8bZ2RkA0K9fPygUCjg7O+P27dvQ0dFBTEyM2rFCQkLg5OTE/81WA1s+mwnPzv+Ce8dOpbZL++shfo0+hE5vDXxJkRFVjNLMDACQkfoIAGDt4ACVjQ2uHjsqtcnNycb1M6fh0qKltE7fyAhBS77C1jmzkPbXw5caM1FFVXnKGR4eDj09PRw/fhxr1hTMq4mIiIC/f8H/jk6fPo0RI0ZgzJgxiI2NhY+PD7744gu1PtLT09GjRw9ERUXhwoUL8PPzQ+/evXH37l0ABUlRvXr1MGfOHCQkJKglLM+7efMmdu7cid27d2P37t2Ijo7GggULpO0TJ07E8ePHERERgcjISBw9ehTnz59X6yMwMBAxMTGIiIjAyZMnIYRAjx49kJOTA5VKhZ49e2LTpk1q+2zevBn+/v4wNTVFZmYmfHx8YGpqiiNHjuDYsWNSsvVs5evAgQOIi4tDZGQkdu/eXep5ZmZm4ssvv8SGDRtw5MgR3L17F5MnT9boszl79iwAIDQ0FAkJCTh79iycnZ3h6+uL0NBQtX1DQ0MRGBhYJHkslJWVhbS0NLWFtO/s7p9w9+oV9Jv04qrCyR0/wsjEBM27vfkSIiOquLc+mYHfY87gj98LpryYW9cGAKQlqydjj5MfStsAYOC/Z+LWhXO4eCDy5QVL2lPDK25VfjuQhg0bYtGiRdLrBw8e4OLFi+jRoweAgrlXfn5++PjjjwEAjRo1wokTJ9QqVl5eXvDy8pJef/HFF9ixYwciIiIwduxYWFpaQldXF2ZmZrCzsys1nvz8fISFhcHsf/+TGz58OA4cOIC5c+fi8ePHCA8Px+bNm9GlSxcABQmKvb29tP/vv/+OiIgIHD9+HB06dAAAbNq0CQ4ODti5cyfeeustDBs2DO+88w4yMzNhbGyMtLQ07NmzBz/++CMAYOvWrdDR0cHatWulxCc0NBS1atXC4cOH0a1bwSRaExMTrF27FgYGBtLxSzrPnJwcrF69Gi4uLgCAsWPHYs6cOSjN859NoVq1aqn1/95772H06NFYsmQJDA0NcfHiRcTGxmL79u0l9j1//nx89tlnpR6fKubvhD+wbe5n+Gj9d9D/X/W6NMd/+C/a9PYvU1uiqjZk1hzUdXPHf4YMKLKtSKVfoQD+t+61f/nCrV17zO3b82WESZVCgYrXnfiQ+XJr1aqV2uuIiAh07NgRlpaWAIC4uDi0b99erc3zrzMyMjB16lR4eHigVq1aMDU1xbVr16SKmyacnZ2lpA0A6tSpg6SkJADArVu3kJOTgzZt2kjbVSoV3NzcpNdxcXHQ09ND27ZtpXVWVlZwc3NDXFwcAKBnz57Q09NDREQEAODHH3+EmZmZlJCdO3cON27cgJmZGUxNTWFqagpLS0s8ffoUN2/elPr19PRUS9pKY2xsLCVtz59XSZ7/bErSt29f6OnpYceOHQCA9evXw8fHRxpaLc4nn3yC1NRUabl3716ZjkVld/fyZTxOTsa8/n3wgXtDfODeENfPnMah78LwgXtD5OflSW1/P3sGf8bfQqe3BlVhxERlM/jT2XjtX75Y8s5gPPozUVpfOOypslafu2xmaY205L8AAI3bdUBtRycsjbmElVdvYOXVGwCA0SGrMHHD1pd0BkTlV+UVN5PnrvR5dpgUKOZ/TsWYMmUK9u3bhy+//BINGzaEUqnEgAEDyjWhXl9fX+21QqFAfn6+WizPD/89G2NJ8QohpP0MDAwwYMAAbN68GYMHD8bmzZsxaNAg6OkVfBz5+flo2bJlkeFUAKhd+59y//Pvnabn9aL3tqz9GxgYYPjw4QgNDUX//v2xefPmF17ZamhoKM1hpMrRuH0HzNytPpcy/OOpsGvQAH7vj4aOrq60/vgP38OxqScc3D1edphEGhk88zM06+qHJW8PRvL9+2rb/rp3D6lJSXDv2An34q4AAHT19dGoTVts/0/BlJe936zCsf+qJ2iz9uzH9/M+x6VDUSAZUCgKlor2IVNVnrg9Kz09HYcOHcLXX38trfPw8MCpU6fU2j3/+ujRowgMDES/fv2kfp6dyA8UJBd5z1QYysPFxQX6+vo4c+YMHBwcAABpaWn4/fff4e3tLcWbm5uL06dPS0OlycnJuH79Otzd3aW+hg0bhm7duuHKlSs4dOgQPv/8c2lbixYtsG3bNtjY2MDc3FyjGLVxnqXR19cvtv/33nsPTZs2xcqVK5GTk4P+/ftXWgxUNkampqjbyE1tnaGxEiYWFmrrn6Q/xrm9P2PAx9NfdohEGhky63O06e2PlR+MxNOMDGne2pPHacjJygIAHAhfj+6jP0TSndtIuh2P7qM/RPaTJzizexeAgqpccRck/J3wR5FEkKopPjmh+ti7dy9cXV3RoME/N1QcP3489u7di0WLFuH69etYsWJFkSsyGzZsiO3btyM2NhYXL17E0KFDpSpZIWdnZxw5cgQPHjzAX3/9Va74zMzMEBAQgClTpuDQoUO4cuUKRowYAR0dHama5urqCn9/f4wcORLHjh3DxYsX8fbbb6Nu3bpqlURvb2/Y2tpi2LBhcHZ2Rrt27aRtw4YNg7W1Nfz9/XH06FHEx8cjOjoaH330Ee6/4BeLNs7zRf0fOHAAiYmJSElJkda7u7ujXbt2mDZtGoYMGQLlM/dVourt7O6fIIRAm169qzoUolJ1HjYcxubmmLxpG/5z4qy0tOrxz3d337ercSB8PYbO+hz/3v4TatnaYfmI4cjKyKjCyIm0p1olbrt27VJLbgCgXbt2WLt2LUJCQtCsWTPs378fM2bMUGuzdOlSWFhYoEOHDujduzf8/PzQokULtTZz5szB7du34eLiojbcqKklS5agffv26NWrF3x9fdGxY0e4u7vDyMhIahMaGoqWLVuiV69eaN++PYQQ+Pnnn9WGKxUKBYYMGYKLFy9i2LBhascwNjbGkSNH4OjoiP79+8Pd3R0jRozAkydPXliB09Z5lmTx4sWIjIyEg4MDmjdvrrYtKCgI2dnZGDFihNaPS9oxaeNWDJo+U23dG4OHYsWlOCjNNKvuEr1soxo5F7uc3PGDWrvdIcswtVMbjPV0w+K3B0lXnZbW78Wo/ZUZOmmVQkuLPClENbnRVl5eHmxsbPDLL7+oTf6v7jIyMlC3bl0sXrwYQUFBVR1OlZo7dy62bt2KX3/9VeN909LSoFKpkJp6DebmZi/egUiGRjVyruoQiCpFdp5A2K1cpKamajzFp6wK/048+m0JzM0qNqqT9vgJarlNrNR4K0u1meOWnJyMCRMmoHXr1lUdSqkuXLiAa9euoU2bNkhNTZVuqfF8pbAmSU9PR1xcHEJCQtTm6hEREZF2VZuhUhsbG8yYMaPEG7ZWJ19++SW8vLzg6+uLjIwMHD16FNbW1lUdVpUZO3YsOnXqBG9vbw6TEhFR5eINeEkTzZs3x7lz56o6jGolLCysTA+oJyIiqjhtzFGr/kWikjBxIyIiIvmo4fdxk2+tkIiIiKiS5ebmYsaMGahfvz6USiUaNGiAOXPmqN12TAiB2bNnw97eHkqlEp07d8aVK1cqJR4mbkRERCQjOlpaymbhwoVYvXo1VqxYgbi4OCxatAj/+c9/EBISIrVZtGgRlixZghUrVuDs2bOws7ND165d8fjxYy2crzoOlRIREZF8vOSh0pMnT8Lf3x89e/YEUHAj+i1btiAmJgZAQbVt2bJlmD59uvTUoPDwcNja2mLz5s0YNWpUxWJ9DituREREVCOlpaWpLVn/e3Taszp16oQDBw7g+vWCGzlfvHgRx44dQ48ePQAA8fHxSExMRLdu3aR9DA0N4e3tjRMnTmg9ZlbciIiISD60+KzSwueOF5o1axZmz56ttm7atGlITU1F48aNoauri7y8PMydOxdDhgwBACQmJgIAbG1t1faztbXFnTt3KhZnMZi4ERERkYxo73Yg9+7dU3tygqGhYZGW27Ztw8aNG7F582Y0adIEsbGxCA4Ohr29PQICAv7p8bnhVyFEpdyblokbERER1Ujm5uYvfOTVlClT8PHHH2Pw4MEAAE9PT9y5cwfz589HQEAA7OzsABRU3urUqSPtl5SUVKQKpw2c40ZERETyUXhxQkWXMsrMzISOjnq6pKurK90OpH79+rCzs0NkZKS0PTs7G9HR0ejQoYN2zvkZrLgRERGRfCgUWpjjVvbErXfv3pg7dy4cHR3RpEkTXLhwAUuWLJEe8ahQKBAcHIx58+bB1dUVrq6umDdvHoyNjTF06NCKxVkMJm5EREREJQgJCcGnn36KMWPGICkpCfb29hg1ahRmzpwptZk6dSqePHmCMWPGICUlBW3btsX+/fthZmam9XgUQgih9V6JNJSWlgaVSoXU1GswN9f+F52oOhjVyLmqQyCqFNl5AmG3cpGamvrCOWPlVfh34lH8tzA3M65YX48zUav+yEqNt7Kw4kZEREQyooUb8PIh80RERESVT6HQgaKCc9wqun9Vkm/kRERERDUMK25EREQkI9q7Aa8cMXEjIiIi+XjJD5mvbjhUSkRERCQTrLgRERGRjOig4nUn+datmLgRERGRfHColIiIiIjkgBU3IiIiko8aXnFj4kZEREQyUrPnuMk3ciIiIqIahhU3IiIikg8OlRIRERHJBBM3IiIiIrngHDciIiIikgFW3IiIiEg+OFRKREREJBeK/y0V7UOeOFRKREREJBOsuBEREZF8KBSAooJ1Jw6VEhEREb0ENXyOG4dKiYiIiGSCFTciIiKSkZp9cQITNyIiIpIPhY4W5rjJd8BRvpETERER1TCsuBEREZGMcKiUiIiISCaYuBERERHJA+e4EREREZEcsOJGREREMsKhUiIiIiKZqNmJG4dKiYiIiGSCFTciIiKSER1UvO4k37oVEzciIiKSDz5knoiIiIjkgBU3IiIikhFenEBEREQkEwotLWX34MEDvP3227CysoKxsTGaNWuGc+fOSduFEJg9ezbs7e2hVCrRuXNnXLlypYLnWTwmbkREREQlSElJQceOHaGvr49ffvkFV69exeLFi1GrVi2pzaJFi7BkyRKsWLECZ8+ehZ2dHbp27YrHjx9rPR4OlRIREZGMKFDxulPZK24LFy6Eg4MDQkNDpXXOzs7Sv4UQWLZsGaZPn47+/fsDAMLDw2Fra4vNmzdj1KhRFYxVHStuREREJB+FV5VWdAGQlpamtmRlZRU5XEREBFq1aoW33noLNjY2aN68Ob799ltpe3x8PBITE9GtWzdpnaGhIby9vXHixAmtnz4TNyIiIpIR7c1xc3BwgEqlkpb58+cXOdqtW7ewatUquLq6Yt++fRg9ejTGjx+P7777DgCQmJgIALC1tVXbz9bWVtqmTRwqJSIiohrp3r17MDc3l14bGhoWaZOfn49WrVph3rx5AIDmzZvjypUrWLVqFd555x2pneK5e8MJIYqs0wZW3IiIiEhGdLS0AObm5mpLcYlbnTp14OHhobbO3d0dd+/eBQDY2dkBQJHqWlJSUpEqnDYwcSMiIiIZebm3A+nYsSN+++03tXXXr1+Hk5MTAKB+/fqws7NDZGSktD07OxvR0dHo0KFDuc6wNBwqJSIiIirBhAkT0KFDB8ybNw8DBw7EmTNn8M033+Cbb74BUDBEGhwcjHnz5sHV1RWurq6YN28ejI2NMXToUK3Hw8SNiIiI5OMlP6u0devW2LFjBz755BPMmTMH9evXx7JlyzBs2DCpzdSpU/HkyROMGTMGKSkpaNu2Lfbv3w8zM7OKxVlc6EIIofVeiTSUlpYGlUqF1NRrMDfX/hedqDoY1ci5qkMgqhTZeQJht3KRmpqqNtlfm6S/E8nHYG5uWsG+0qGy6lSp8VYWznEjIiIikgkOlRIREZGM/HNVaMX6kCcmbkRERCQjmj8kvvg+5Em+KScRERFRDcOKGxEREcnHS76qtLph4kZEREQywjluRERERDLBOW5EREREJAOsuBEREZGM1OyKGxM3IiIiko8afnECh0qJiIiIZIIVNyIiIpIRBSped5JvxY2JGxEREclIzZ7jxqFSIiIiIplgxY2IiIhkpGZX3Ji4ERERkXwodAqWivYhU/KNnIiIiKiGYcWNiIiIZIRDpUREREQywcSNiIiISCZqduLGOW5EREREMsGKGxEREclHDb+qlIkbERERyUjNHipl4kbVghACAJCWll7FkRBVnuw8UdUhEFWK7PyC73bh7/LKlJb2uFr0UVWYuFG18PhxwQ+Rg0OrKo6EiIjK6/Hjx1CpVJXSt4GBAezs7ODg0For/dnZ2cHAwEArfb1MCvEy0mOiF8jPz8cff/wBMzMzKBTyLWHLRVpaGhwcHHDv3j2Ym5tXdThEWsfv+MslhMDjx49hb28PHZ3Kmz/29OlTZGdna6UvAwMDGBkZaaWvl4kVN6oWdHR0UK9evaoOo8YxNzfnHzV6pfE7/vJUVqXtWUZGRrJMtrRJvpdVEBEREdUwTNyIiIiIZIKJG1ENZGhoiFmzZsHQ0LCqQyGqFPyO06uKFycQERERyQQrbkREREQywcSNiIiISCaYuBERERHJBBM3oldEWFgYatWqVW36oVcDv1eaKct5BgYGom/fvi8lHnr1MHEjkpH69etj7969WuvP2dkZy5YtU1s3aNAgXL9+XWvHoOqP36vyKe48y2L58uUICwt7YTuFQoGdO3dq3D+92vjkBKJqLjs7GwYGBrh06RKSk5Ph4+NTqcdTKpVQKpWVegyqevxelV/he1deL3rCQEX7p1cbK25E1Uznzp0xduxYTJw4EdbW1ujatSsAYNeuXfDz85PuSxUWFgZHR0cYGxujX79+SE5OVuvn5s2b8Pf3h62tLUxNTdG6dWtERUWpHefOnTuYMGECFAqF9IzY54d6Zs+ejWbNmmHDhg1wdnaGSqXC4MGD8fjxY6nN48ePMWzYMJiYmKBOnTpYunQpOnfujODg4Ep6l0hTr+r3KiUlBe+88w4sLCxgbGyM7t274/fffwcApKamQqlUFqkmbt++HSYmJkhPTwcAPHjwAIMGDYKFhQWsrKzg7++P27dvS+0Lhzbnz58Pe3t7NGrUqMTzLLRv3z64u7vD1NQUb775JhISEor0V9pn4+zsDADo168fFAoFnJ2dcfv2bejo6CAmJkbtWCEhIXBycgLv7lUzMHEjqobCw8Ohp6eH48ePY82aNQCAiIgI+Pv7AwBOnz6NESNGYMyYMYiNjYWPjw+++OILtT7S09PRo0cPREVF4cKFC/Dz80Pv3r1x9+5dAAV/vOrVq4c5c+YgISFB7Q/L827evImdO3di9+7d2L17N6Kjo7FgwQJp+8SJE3H8+HFEREQgMjISR48exfnz57X9tlAFvYrfq8DAQMTExCAiIgInT56EEAI9evRATk4OVCoVevbsiU2bNqnts3nzZvj7+8PU1BSZmZnw8fGBqakpjhw5gmPHjknJ1rMPMz9w4ADi4uIQGRmJ3bt3l3qemZmZ+PLLL7FhwwYcOXIEd+/exeTJkzX6bM6ePQsACA0NRUJCAs6ePQtnZ2f4+voiNDRUbd/Q0FAEBgYWSR7pFSWIqFrx9vYWzZo1U1t3//59oa+vL5KTk4UQQgwZMkS8+eabam0GDRokVCpVqX17eHiIkJAQ6bWTk5NYunSpWpvQ0FC1fmbNmiWMjY1FWlqatG7KlCmibdu2Qggh0tLShL6+vvjvf/8rbX/06JEwNjYWH3300YtOl16SV/F7df36dQFAHD9+XGrz119/CaVSKb7//nshhBDbt28XpqamIiMjQwghRGpqqjAyMhJ79uwRQgixbt064ebmJvLz86U+srKyhFKpFPv27RNCCBEQECBsbW1FVlaW2jmVdJ4AxI0bN6R1X3/9tbC1tZVeBwQECH9/f+l1cZ+NEEIAEDt27FBbt23bNmFhYSGePn0qhBAiNjZWKBQKER8fX2R/ejWx4kZUDbVq1UrtdUREBDp27AhLS0sAQFxcHNq3b6/W5vnXGRkZmDp1Kjw8PFCrVi2Ympri2rVrUmVEE87OzjAzM5Ne16lTB0lJSQCAW7duIScnB23atJG2q1QquLm5aXwcqlyv2vcqLi4Oenp6aNu2rbTOysoKbm5uiIuLAwD07NkTenp6iIiIAAD8+OOPMDMzQ7du3QAA586dw40bN2BmZgZTU1OYmprC0tIST58+xc2bN6V+PT09yzzvzNjYGC4uLsWeV0me/2xK0rdvX+jp6WHHjh0AgPXr18PHx0caWqVXHy9OIKqGTExM1F4/O5wFoExzWaZMmYJ9+/bhyy+/RMOGDaFUKjFgwAC14Z+y0tfXV3utUCiQn5+vFsvzwzRliZFerlfte1VSvEIIaT8DAwMMGDAAmzdvxuDBg7F582YMGjQIenoFf/7y8/PRsmXLIsOpAFC7dm3p38+/d5qe14ve27L2b2BggOHDhyM0NBT9+/fH5s2by3VlK8kXK25E1Vx6ejoOHTqEPn36SOs8PDxw6tQptXbPvz569CgCAwPRr18/eHp6ws7OTm3CNVDwRyAvL69C8bm4uEBfXx9nzpyR1qWlpUkTxKl6ehW+Vx4eHsjNzcXp06eldcnJybh+/Trc3d2ldcOGDcPevXtx5coVHDp0CMOGDZO2tWjRAr///jtsbGzQsGFDteVFV39q4zxLo6+vX2z/7733HqKiorBy5Urk5OSgf//+lRYDVT9M3Iiqub1798LV1RUNGjSQ1o0fPx579+7FokWLcP36daxYsaLIlXMNGzbE9u3bERsbi4sXL2Lo0KFSNaOQs7Mzjhw5ggcPHuCvv/4qV3xmZmYICAjAlClTcOjQIVy5cgUjRoyAjo4OJ0tXY6/C98rV1RX+/v4YOXIkjh07hosXL+Ltt99G3bp11SqJ3t7esLW1xbBhw+Ds7Ix27dpJ24YNGwZra2v4+/vj6NGjiI+PR3R0ND766CPcv3+/1Bi1cZ4v6v/AgQNITExESkqKtN7d3R3t2rXDtGnTMGTIkFfmNitUNkzciKq5Xbt2qf0RAoB27dph7dq1CAkJQbNmzbB//37MmDFDrc3SpUthYWGBDh06oHfv3vDz80OLFi3U2syZMwe3b9+Gi4uL2rCQppYsWYL27dujV69e8PX1RceOHeHu7g4jI6Ny90mV61X5XoWGhqJly5bo1asX2rdvDyEEfv75Z7XhSoVCgSFDhuDixYtq1TagYD7akSNH4OjoiP79+8Pd3R0jRozAkydPYG5uXmp82jrPkixevBiRkZFwcHBA8+bN1bYFBQUhOzsbI0aM0PpxqXpTCE5EIaq28vLyYGNjg19++UVtknZ1l5GRgbp162Lx4sUICgqq6nDoOfxeyd/cuXOxdetW/Prrr1UdCr1kvDiBqBpLTk7GhAkT0Lp166oOpVQXLlzAtWvX0KZNG6SmpmLOnDkAUKSiQ9UDv1fylZ6ejri4OISEhODzzz+v6nCoCrDiRkQVduHCBbz33nv47bffYGBggJYtW2LJkiXw9PSs6tBIxvi9KiowMBBbtmxB3759sXnzZujq6lZ1SPSSMXEjIiIikglenEBEREQkE0zciIiIiGSCiRsRERGRTDBxIyIiIpIJJm5ERP8ze/ZsNGvWTHodGBiIvn37vvQ4bt++DYVCgdjY2BLbODs7a/SMyrCwMNSqVavCsSkUCuzcubPC/RBR+TBxI6JqLTAwEAqFAgqFAvr6+mjQoAEmT56MjIyMSj/28uXLERYWVqa2ZUm2iIgqijfgJaJq780330RoaChycnJw9OhRvPfee8jIyMCqVauKtM3JyVF73FFFvOgh40RELxsrbkRU7RkaGsLOzg4ODg4YOnQohg0bJg3XFQ5vrl+/Hg0aNIChoSGEEEhNTcX7778PGxsbmJub41//+hcuXryo1u+CBQtga2sLMzMzBAUF4enTp2rbnx8qzc/Px8KFC9GwYUMYGhrC0dERc+fOBQDUr18fANC8eXMoFAp07txZ2i80NFR6xmbjxo2xcuVKteOcOXMGzZs3h5GREVq1aoULFy5o/B4V3pjWxMQEDg4OGDNmDNLT04u027lzJxo1agQjIyN07doV9+7dU9v+008/oWXLljAyMkKDBg3w2WefITc3V+N4iKhyMHEjItlRKpXIycmRXt+4cQPff/89fvzxR2mosmfPnkhMTMTPP/+Mc+fOoUWLFujSpQv+/vtvAMD333+PWbNmYe7cuYiJiUGdOnWKJFTP++STT7Bw4UJ8+umnuHr1KjZv3gxbW1sABckXAERFRSEhIQHbt28HAHz77beYPn065s6di7i4OMybNw+ffvopwsPDARQ8f7NXr15wc3PDuXPnMHv2bEyePFnj90RHRwdfffUVLl++jPDwcBw8eBBTp05Va5OZmYm5c+ciPDwcx48fR1paGgYPHixt37dvH95++22MHz8eV69exZo1axAWFiYlp0RUDQgiomosICBA+Pv7S69Pnz4trKysxMCBA4UQQsyaNUvo6+uLpKQkqc2BAweEubm5ePr0qVpfLi4uYs2aNUIIIdq3by9Gjx6ttr1t27bCy8ur2GOnpaUJQ0ND8e233xYbZ3x8vAAgLly4oLbewcFBbN68WW3d559/Ltq3by+EEGLNmjXC0tJSZGRkSNtXrVpVbF/PcnJyEkuXLi1x+/fffy+srKyk16GhoQKAOHXqlLQuLi5OABCnT58WQgjx+uuvi3nz5qn1s2HDBlGnTh3pNQCxY8eOEo9LRJWLc9yIqNrbvXs3TE1NkZubi5ycHPj7+yMkJETa7uTkhNq1a0uvz507h/T0dFhZWan18+TJE9y8eRMAEBcXh9GjR6ttb9++PQ4dOlRsDHFxccjKykKXLl3KHPfDhw9x7949BAUFYeTIkdL63Nxcaf5cXFwcvLy8YGxsrBaHpg4dOoR58+bh6tWrSEtLQ25uLp4+fYqMjAyYmJgAAPT09NCqVStpn8aNG6NWrVqIi4tDmzZtcO7cOZw9e1atwpaXl4enT58iMzNTLUYiqhpM3Iio2vPx8cGqVaugr68Pe3v7IhcfFCYmhfLz81GnTh0cPny4SF/lvSWGUqnUeJ/8/HwABcOlbdu2VdtW+HBwoYXHRd+5cwc9evTA6NGj8fnnn8PS0hLHjh1DUFCQ2pAyUHA7j+cVrsvPz8dnn32G/v37F2ljZGRU4TiJqOKYuBFRtWdiYoKGDRuWuX2LFi2QmJgIPT09ODs7F9vG3d0dp06dwjvvvCOtO3XqVIl9urq6QqlU4sCBA3jvvfeKbDcwMABQUKEqZGtri7p16+LWrVsYNmxYsf16eHhgw4YNePLkiZQclhZHcWJiYpCbm4vFixdDR6dg6vL3339fpF1ubi5iYmLQpk0bAMBvv/2GR48eoXHjxgAK3rfffvtNo/eaiF4uJm5E9Mrx9fVF+/bt0bdvXyxcuBBubm74448/8PPPP6Nv375o1aoVPvroIwQEBKBVq1bo1KkTNm3ahCtXrqBBgwbF9mlkZIRp06Zh6tSpMDAwQMeOHfHw4UNcuXIFQUFBsLGxgVKpxN69e1GvXj0YGRlBpVJh9uzZGD9+PMzNzdG9e3dkZWUhJiYGKSkpmDhxIoYOHYrp06cjKCgIM2bMwO3bt/Hll19qdL4uLi7Izc1FSEgIevfujePHj2P16tVF2unr62PcuHH46quvoK+vj7Fjx6Jdu3ZSIjdz5kz06tULDg4OeOutt6Cjo4NLly7h119/xRdffKH5B0FEWserSonolaNQKPDzzz/jjTfewIgRI9CoUSMMHjwYt2/flq4CHTRoEGbOnIlp06ahZcuWuHPnDj744INS+/30008xadIkzJw5E+7u7hg0aBCSkpIAFMwf++qrr7BmzRrY29vD398fAPDee+9h7dq1CAsLg6enJ7y9vREWFibdPsTU1BQ//fQTrl69iubNm2P69OlYuHChRufbrFkzLFmyBAsXLkTTpk2xadMmzJ8/v0g7Y2NjTJs2DUOHDkX79u2hVCqxdetWabufnx92796NyMhItG7dGu3atcOSJUvg5OSkUTxEVHkUQhsTLIiIiIio0rHiRkRERCQTTNyIiIiIZIKJGxEREZFMMHEjIiIikgkmbkREREQywcSNiIiISCaYuBERERHJBBM3IiIiIplg4kZEREQkE0zciIiIiGSCiRsRERGRTDBxIyIiIpKJ/wcvcqfxLfNz+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/4sqt0lt90t72lgqr2zq3p6l80000gn/T/ipykernel_33031/1624629724.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_performance_capture.append(model_evaluation(rs4, 'Model_4_Random_Forrest'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>model_params</th>\n",
       "      <th>train_acuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "      <th>best_score_CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_4_Random_Forrest</td>\n",
       "      <td>RandomizedSearchCV(estimator=Pipeline(steps=[(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.895129</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.742242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name                                              model  \\\n",
       "0  Model_4_Random_Forrest  RandomizedSearchCV(estimator=Pipeline(steps=[(...   \n",
       "\n",
       "  best_score                                       model_params  \\\n",
       "0        NaN  {'tvec__stop_words': 'english', 'tvec__preproc...   \n",
       "\n",
       "   train_acuracy  test_accuracy  baseline_accuracy  best_score_CV  \n",
       "0       0.895129       0.793103           0.509128       0.742242  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance_capture.append(model_evaluation(rs4, 'Model_4_Random_Forrest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6e51d-71f1-4507-a263-6652ef6331d2",
   "metadata": {},
   "source": [
    "### 05 - Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad56b0-a2fb-4787-bb61-0ed9560b85aa",
   "metadata": {},
   "source": [
    "## Single Estimator Models for Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9603bd-d086-4dfb-91e9-47c23a7fe4a4",
   "metadata": {},
   "source": [
    "### 06 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "98a2444a-e944-4c57-8549-72feb4195b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe6 = Pipeline([\n",
    "    ('mvec' , Multi_Vectorizer()),\n",
    "    ('cls' , Multi_Classifier())\n",
    "])\n",
    "\n",
    "params1 = [# list of params... one for each estimator (order matters here). Cite: Tim Office Hours\n",
    "    { \n",
    "        ## Logistic Regression\n",
    "        'cls__estimator': [LogisticRegression()],\n",
    "        'cls__estimator__C': np.linspace(0.9, 2, 10), \n",
    "        \n",
    "        'tvec__preprocessor': [lemmatize_post],     # Lemmatizing showed the best results in initial testing\n",
    "        'tvec__max_df': [1.0, 0.9],\n",
    "        'tvec__max_features': [None, 5000],\n",
    "        'tvec__min_df': [1],\n",
    "        'tvec__ngram_range': [(1,2)],               #words and bigrams showed best results early\n",
    "        'tvec__stop_words': ['english'],            #English stop words showed best results early.\n",
    "        \n",
    "    }\n",
    "        ,# Multinomial Naive Bayes\n",
    "        {\n",
    "        'cls__estimator': [MultinomialNB()],\n",
    "            \n",
    "         'tvec__preprocessor': [lemmatize_post],     # Lemmatizing showed the best results in initial testing\n",
    "        'tvec__max_df': [1.0, 0.9],\n",
    "        'tvec__max_features': [None, 5000],\n",
    "        'tvec__min_df': [1],\n",
    "        'tvec__ngram_range': [(1,2)],               #words and bigrams showed best results early\n",
    "        'tvec__stop_words': ['english'],            #English stop words showed best results early.\n",
    "            \n",
    "    }\n",
    "        ,#Kernelized SVM\n",
    "        {\n",
    "        'cls__estimator': [SVC()],\n",
    "        'cls__estimator__C': np.linspace(0.05, 2, 10),\n",
    "        'cls__estimator__degree': [2,3],\n",
    "        'cls__estimator__kernel': ['poly','rbf'],\n",
    "            \n",
    "        'tvec__preprocessor': [lemmatize_post],     # Lemmatizing showed the best results in initial testing\n",
    "        'tvec__max_df': [1.0, 0.9],\n",
    "        'tvec__max_features': [None, 5000],\n",
    "        'tvec__min_df': [1],\n",
    "        'tvec__ngram_range': [(1,2)],               #words and bigrams showed best results early\n",
    "        'tvec__stop_words': ['english'],            #English stop words showed best results early.\n",
    "         \n",
    "    }\n",
    "]\n",
    "pipe6.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f6dbfdd4-8eec-43ff-a89f-11b203cdc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs6 = RandomizedSearchCV(estimator=pipe0,\n",
    "                        param_distributions=params0,\n",
    "                        cv = 5,\n",
    "                        n_iter = 100\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b422f-50e4-4d61-8ac9-c728fc8efe85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
