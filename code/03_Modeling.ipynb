{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b25aff06-ced7-4d60-8e25-323c105b7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4055044c-3567-4d18-a584-f761ea52a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_extract = '2023-06-11 16:25'\n",
    "df = pd.read_csv(f'data/reddit_posts_raw_{most_recent_extract}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb092a5-94f3-42c0-b8ad-7c84a81271b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1971, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ebf688-8ea0-4043-88eb-90c52d8becb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>top_comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dating</td>\n",
       "      <td>1471ube</td>\n",
       "      <td>2023-06-11 18:49:33</td>\n",
       "      <td>Am I Clueless?</td>\n",
       "      <td>So there is this girl I’ve known my whole life...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit       id          created_utc           title  \\\n",
       "0    dating  1471ube  2023-06-11 18:49:33  Am I Clueless?   \n",
       "\n",
       "                                            selftext top_comment_text  \n",
       "0  So there is this girl I’ve known my whole life...              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021088-c511-400b-ae95-5ea30d513cb5",
   "metadata": {},
   "source": [
    "### Self-text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b87f74-a99a-4930-a060-41a47fe7e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series(df['selftext'])\n",
    "y = df['subreddit'].map({'dating': 0,\n",
    "                    'datingoverthirty':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792701d4-b20f-48ca-ace2-e34628960c6a",
   "metadata": {},
   "source": [
    "### Self Text and Top Comment - Alternative Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81dca47-8a62-4dc1-835b-67b92a603b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['self_text_and_comment'] = df['self_text'].astype(str) + df['top_comment_text'].astype(str)\n",
    "# X = pd.Series(df['self_text_and_comment'])\n",
    "# y = df['subreddit'].map({'dating': 0,\n",
    "#                    'datingoverthirty':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c0234-f3f8-4a5b-b6e5-bd6245d5e1de",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cf8e268-e628-47f6-937f-8bde2e2ab325",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "X_train.to_pickle('./pickled_models/X_train.pkl')\n",
    "X_test.to_pickle('./pickled_models/X_test.pkl')\n",
    "y_train.to_pickle('./pickled_models/y_train.pkl')\n",
    "y_test.to_pickle('./pickled_models/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674e456-0407-4f36-897a-52dfdde06822",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c35f5f-2dd3-4ad8-934e-ec3bb2783183",
   "metadata": {},
   "source": [
    "> The majority class holds 50.63% of responses.  This is the baseline score to beat.\n",
    "\n",
    "> Even class distribution makes a 75/25 train test split possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a934cc43-a7d1-499a-89c2-751b8e072a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy_preds = dummy.predict(y_test)\n",
    "\n",
    "# mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c5b41e-798c-48bf-8388-bb51f45e18d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datingoverthirty    0.506342\n",
       "dating              0.493658\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439cc05-bbd5-459b-8b93-93766af8f2f0",
   "metadata": {},
   "source": [
    "## Baseline Investigation with Standard Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c4714-6c21-4912-bf6d-c11448013d92",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8937a30d-1edb-4714-9028-2699ef3ce2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec0 = CountVectorizer() #standard CountVectorizer\n",
    "cvec0.fit(X_train)\n",
    "pickle.dump(cvec0, open('./pickled_models/cvec0_baseline', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4df9b-d837-493c-95bb-03c7de07a16c",
   "metadata": {},
   "source": [
    "> See Model Investigaion for Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004fc85f-1d24-4fc9-a9a4-be2d0c2eea52",
   "metadata": {},
   "source": [
    "#### Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baf17ad6-346c-4dca-88eb-079fc3281c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec0 = TfidfVectorizer()\n",
    "pickle.dump(tvec0, open('./pickled_models/tvec0_baseline', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc3109-57f4-461d-84df-9e014ced471b",
   "metadata": {},
   "source": [
    "> See Model Investigation for Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f253c1-1a47-4c5b-817d-a78af324a13e",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea6fa8-f6a7-4399-9347-7804c8430ebd",
   "metadata": {},
   "source": [
    "#### Stemming and Lematizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66c12595-e429-4d6f-85ef-4bdb583b4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "def stem_post(post):\n",
    "    split_post = post.split(' ')\n",
    "    return ' '.join([p_stemmer.stem(word) for word in split_post])\n",
    "#cite 6/9 Breakfast Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "57cac3e0-718b-46a7-af1f-4217a6013053",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# cite: Lesson 504 NLP 1 - Modified to handle complete words.\n",
    "def lemmatize_post(post):\n",
    "    mapper = { \n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    post_split = post.split(' ')\n",
    "    post_tokens = [(token, tag) for token, tag in nltk.pos_tag(post_split)]\n",
    "    post_lem = []\n",
    "    for token in post_tokens:\n",
    "        pos = mapper.get(token[1][0])\n",
    "        # post_lem.append((token[0],pos) if pos != None else (token[0]))\n",
    "        post_lem.append(lemmatizer.lemmatize(token[0], pos) if pos != None else token[0])\n",
    "    return ' '.join(post_lem).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9b446-9390-4b88-bb83-c473c9609104",
   "metadata": {},
   "source": [
    "## RandomSearchCV over Multiple Model Types with Tfidf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05cd6f3a-0294-496e-931e-58c096f06156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to evaluate multiple classifiers in the same RandomSearchCV, trying different combinations of Tfidf / CountVectorizer and LogisticRegression() / MultinomialNB\n",
    "# Inspiration: Wrapper Class (https://stackoverflow.com/questions/50285973/pipeline-multiple-classifiers).  Content: DSI Lesson 507 on OOP (https://git.generalassemb.ly/bobadams1/507-lesson-object-oriented-programming)\n",
    "'''\n",
    "Notes from Inspiration above (no copy-paste):\n",
    "1. Need BaseEstimator() as the base class for all sklearn estimators - as a stand in for the estimator being selected\n",
    "2. The class only really needs to to have self and the estimator as objects in the class.\n",
    "3. The methods you would normally call for the estimator should be defined as functions within the model (don't forget to pass self every time!)\n",
    "'''\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Multi_Classifier(BaseEstimator):\n",
    "    def __init__(self, estimator = MultinomialNB()): #LogisticRegression as default\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def fit(self, X, y): # interested in LogisticRegression, NB... both take primarily X,y\n",
    "        return self.estimator.fit(X,y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(X)\n",
    "    \n",
    "    def score(self, X,y):\n",
    "        return self.estimator.score(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddd8c8-5ab3-4535-b072-dab124957860",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b41831c-f116-4628-b840-075752cf6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe0 = Pipeline([\n",
    "    ('tvec' , TfidfVectorizer()),\n",
    "    ('cls' , Multi_Classifier())\n",
    "])\n",
    "\n",
    "params0 = [{ # list of params... one for each estimator (order matters here). Cite: Tim Office Hours\n",
    "## Logistic Regression\n",
    "         'tvec__preprocessor': [None, stem_post, lemmatize_post],     \n",
    "         'tvec__max_df': [1.0, 0.9],\n",
    "         # 'tvec__max_features': None,\n",
    "         # 'tvec__min_df': 1,\n",
    "         'tvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'tvec__stop_words': [None, 'english'],\n",
    "        \n",
    "        'cls__estimator': [LogisticRegression()],\n",
    "        'cls__estimator__C': np.linspace(0.00001, 1, 10),\n",
    "        # 'cls__estimator__max_iter': 100,\n",
    "        # 'cls__estimator__penalty': 'l2'\n",
    "},\n",
    "## Multinomial Naive Bayes\n",
    "{        'tvec__preprocessor': [None, stem_post, lemmatize_post],     \n",
    "         'tvec__max_df': [1.0, 0.9],\n",
    "         # 'tvec__max_features': None,\n",
    "         # 'tvec__min_df': 1,\n",
    "         'tvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'tvec__stop_words': [None, 'english'],\n",
    "         'cls__estimator': [MultinomialNB()]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bae78077-8824-4758-8dcb-f3921e89e517",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tvec', TfidfVectorizer()), ('cls', Multi_Classifier())],\n",
       " 'verbose': False,\n",
       " 'tvec': TfidfVectorizer(),\n",
       " 'cls': Multi_Classifier(),\n",
       " 'tvec__analyzer': 'word',\n",
       " 'tvec__binary': False,\n",
       " 'tvec__decode_error': 'strict',\n",
       " 'tvec__dtype': numpy.float64,\n",
       " 'tvec__encoding': 'utf-8',\n",
       " 'tvec__input': 'content',\n",
       " 'tvec__lowercase': True,\n",
       " 'tvec__max_df': 1.0,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__norm': 'l2',\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__smooth_idf': True,\n",
       " 'tvec__stop_words': None,\n",
       " 'tvec__strip_accents': None,\n",
       " 'tvec__sublinear_tf': False,\n",
       " 'tvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tvec__tokenizer': None,\n",
       " 'tvec__use_idf': True,\n",
       " 'tvec__vocabulary': None,\n",
       " 'cls__estimator__alpha': 1.0,\n",
       " 'cls__estimator__class_prior': None,\n",
       " 'cls__estimator__fit_prior': True,\n",
       " 'cls__estimator__force_alpha': 'warn',\n",
       " 'cls__estimator': MultinomialNB()}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe0.get_params() ## For LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "09d4c382-2b3c-4064-a849-da34ca43a47c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tvec', TfidfVectorizer()), ('cls', Multi_Classifier())],\n",
       " 'verbose': False,\n",
       " 'tvec': TfidfVectorizer(),\n",
       " 'cls': Multi_Classifier(),\n",
       " 'tvec__analyzer': 'word',\n",
       " 'tvec__binary': False,\n",
       " 'tvec__decode_error': 'strict',\n",
       " 'tvec__dtype': numpy.float64,\n",
       " 'tvec__encoding': 'utf-8',\n",
       " 'tvec__input': 'content',\n",
       " 'tvec__lowercase': True,\n",
       " 'tvec__max_df': 1.0,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__norm': 'l2',\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__smooth_idf': True,\n",
       " 'tvec__stop_words': None,\n",
       " 'tvec__strip_accents': None,\n",
       " 'tvec__sublinear_tf': False,\n",
       " 'tvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tvec__tokenizer': None,\n",
       " 'tvec__use_idf': True,\n",
       " 'tvec__vocabulary': None,\n",
       " 'cls__estimator__alpha': 1.0,\n",
       " 'cls__estimator__class_prior': None,\n",
       " 'cls__estimator__fit_prior': True,\n",
       " 'cls__estimator__force_alpha': 'warn',\n",
       " 'cls__estimator': MultinomialNB()}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe0.get_params() ## for Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3634f8bd-9429-481d-9f12-aad28a337c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs0 = RandomizedSearchCV(estimator=pipe0,\n",
    "                        param_distributions=params0,\n",
    "                        cv = 5\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aff007-a2ae-4a50-8a6b-7bc71fffe696",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs0.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f72bd5-d50c-4e2e-aea7-738746fa4438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
