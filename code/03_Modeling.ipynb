{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0371cdb1-ed20-4d08-8745-e19493d44d7c",
   "metadata": {},
   "source": [
    "# Model Fitting and Evalutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975f39b-5b89-4986-98b9-c8ad52908caf",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25aff06-ced7-4d60-8e25-323c105b7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "from sklearn.base import TransformerMixin # for class-driven multi-vectorizer assessment \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "# from functions.stem_post import *\n",
    "# from functions.lemmatize_post import *\n",
    "# from functions.model_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4055044c-3567-4d18-a584-f761ea52a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_extract = '2023-06-11 16:25'\n",
    "df = pd.read_csv(f'../data/reddit_posts_raw_{most_recent_extract}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb092a5-94f3-42c0-b8ad-7c84a81271b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1971, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ebf688-8ea0-4043-88eb-90c52d8becb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>top_comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dating</td>\n",
       "      <td>1471ube</td>\n",
       "      <td>2023-06-11 18:49:33</td>\n",
       "      <td>Am I Clueless?</td>\n",
       "      <td>So there is this girl I’ve known my whole life...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit       id          created_utc           title  \\\n",
       "0    dating  1471ube  2023-06-11 18:49:33  Am I Clueless?   \n",
       "\n",
       "                                            selftext top_comment_text  \n",
       "0  So there is this girl I’ve known my whole life...              NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53578f0-9e88-4023-92ad-e103fa24003a",
   "metadata": {},
   "source": [
    "### Data Leveraged\n",
    "For this project, reddit posts were pulled from two subreddits: r/dating, and r/datingoverthirty.  Due to limitations with community access (Summer 2023 Reddit Blackout) and APIs (the removal of some of Reddit's APIs), sourcing data was impeded.  Approximately 1000 posts were sourced the day before protests began, constituting the working dataset used in EDA and Modeling.  To augment the information available, the text of the top-voted comment was pulled for each post.  This enables the investigation to cover broader community interaction.\n",
    "\n",
    "Comment text was interesting in EDA to understand the post-community response in each subreddit.  **Title, Selftext and Comment Text are all leveraged** to maximize the use of available data and to attempt to differentiate the communities based on the conversation (the post, and community-supported response.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021088-c511-400b-ae95-5ea30d513cb5",
   "metadata": {},
   "source": [
    "#### Self-text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "52b87f74-a99a-4930-a060-41a47fe7e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.Series(df['selftext'])\n",
    "# y = df['subreddit'].map({'dating': 0,\n",
    "#                     'datingoverthirty':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792701d4-b20f-48ca-ace2-e34628960c6a",
   "metadata": {},
   "source": [
    "#### Self Text and Top Comment - Alternative Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81dca47-8a62-4dc1-835b-67b92a603b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['self_text_and_comment'] = df['title'].astype(str) +' '+ df['selftext'].astype(str) +' '+ df['top_comment_text'].astype(str)\n",
    "X = pd.Series(df['self_text_and_comment'])\n",
    "y = df['subreddit'].map({'dating': 0,\n",
    "                   'datingoverthirty':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a248248-9c44-4184-ab0d-efae7a69ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/reddit_post_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c0234-f3f8-4a5b-b6e5-bd6245d5e1de",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf8e268-e628-47f6-937f-8bde2e2ab325",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "X_train.to_pickle('../pickled_models/X_train.pkl')\n",
    "X_test.to_pickle('../pickled_models/X_test.pkl')\n",
    "y_train.to_pickle('../pickled_models/y_train.pkl')\n",
    "y_test.to_pickle('../pickled_models/y_test.pkl')\n",
    "\n",
    "# X_train = pd.read_pickle('./pickled_models/X_train.pkl')\n",
    "# X_test = pd.read_pickle('./pickled_models/X_test.pkl')\n",
    "# y_train = pd.read_pickle('./pickled_models/y_train.pkl')\n",
    "# y_test = pd.read_pickle('./pickled_models/y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23c5fac9-ef9d-43b7-9118-b5b1e83238e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1703    Dating Steps & Intimacy I (36m) met a great wo...\n",
       "175     Levels of Cleanliness As many of us have reali...\n",
       "744     Random Any girl how is single and can't have i...\n",
       "1945    How to prevent conversations getting sexual be...\n",
       "680     How to attract more guys I am attracted to? Hi...\n",
       "Name: self_text_and_comment, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674e456-0407-4f36-897a-52dfdde06822",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c35f5f-2dd3-4ad8-934e-ec3bb2783183",
   "metadata": {},
   "source": [
    "> The majority class holds 50.91% of responses.  This is the baseline score to beat.\n",
    "\n",
    "> Even class distribution makes a 75/25 train test split possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a934cc43-a7d1-499a-89c2-751b8e072a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5091277890466531"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy_preds = dummy.predict(y_test)\n",
    "dummy_accuracy = accuracy_score(y_test, dummy_preds)\n",
    "dummy_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439cc05-bbd5-459b-8b93-93766af8f2f0",
   "metadata": {},
   "source": [
    "## Baseline Investigation with Standard Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e359a09-fe26-4bac-8415-e06dbdd1cdb9",
   "metadata": {},
   "source": [
    "Vectorizers perform differently on varying corpora.  This simple look in Model Investigations helps shed the light on how these vectorizers perform out of the box with selftext from these subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c4714-6c21-4912-bf6d-c11448013d92",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8937a30d-1edb-4714-9028-2699ef3ce2c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cvec0 = CountVectorizer() #standard CountVectorizer\n",
    "cvec0.fit(X_train)\n",
    "pickle.dump(cvec0, open('../pickled_models/cvec0_baseline', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4df9b-d837-493c-95bb-03c7de07a16c",
   "metadata": {
    "tags": []
   },
   "source": [
    "> See Model Investigaion for Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004fc85f-1d24-4fc9-a9a4-be2d0c2eea52",
   "metadata": {},
   "source": [
    "#### Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baf17ad6-346c-4dca-88eb-079fc3281c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tvec0 = TfidfVectorizer()\n",
    "tvec0.fit(X_train)\n",
    "pickle.dump(tvec0, open('../pickled_models/tvec0_baseline', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc3109-57f4-461d-84df-9e014ced471b",
   "metadata": {},
   "source": [
    "> See Model Investigation for Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f253c1-1a47-4c5b-817d-a78af324a13e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93724164-f8db-4116-9831-cfb423a2bc18",
   "metadata": {},
   "source": [
    "#### Model Performance Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c596d17-2f13-486e-a0e7-64615712ed89",
   "metadata": {},
   "source": [
    "##### Model Performance Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c5f8db7-2fd4-42b1-a21e-503244720f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Data Frame to capture output from each model fit.\n",
    "model_performance_capture = pd.DataFrame(columns = ['model_name', 'model', 'best_score_CV', 'train_accuracy', 'test_accuracy', 'baseline_accuracy','model_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "524f8ac9-eb94-4262-9cde-40afb3db0bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>best_score_CV</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "      <th>model_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_name, model, best_score_CV, train_accuracy, test_accuracy, baseline_accuracy, model_params]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each model fit is recorded, including train, test, cross-validated accuracy scores, best scores, \n",
    "model_performance_capture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea6fa8-f6a7-4399-9347-7804c8430ebd",
   "metadata": {},
   "source": [
    "#### Stemming and Lematizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848c670-f0dc-40ab-b790-05baea0a1c08",
   "metadata": {},
   "source": [
    "> Functions for Stemming and Lemmatizing are stored in separate files: stem_post.py, lemmatize_post.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66c12595-e429-4d6f-85ef-4bdb583b4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "def stem_post(post):\n",
    "    split_post = post.split(' ')\n",
    "    return ' '.join([p_stemmer.stem(word) for word in split_post])\n",
    "#cite 6/9 Breakfast Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b46e54f-84c4-4c2d-a709-7a2767ad4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(stem_post, open('./pickled_models/function_stem_post.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57cac3e0-718b-46a7-af1f-4217a6013053",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# cite: Lesson 504 NLP 1 - Modified to handle complete posts.\n",
    "def lemmatize_post(post):\n",
    "    mapper = { \n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    post_split = post.split(' ')\n",
    "    post_tokens = [(token, tag) for token, tag in nltk.pos_tag(post_split)]\n",
    "    post_lem = []\n",
    "    for token in post_tokens:\n",
    "        pos = mapper.get(token[1][0])\n",
    "        # post_lem.append((token[0],pos) if pos != None else (token[0]))\n",
    "        post_lem.append(lemmatizer.lemmatize(token[0], pos) if pos != None else token[0])\n",
    "    return ' '.join(post_lem).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f085bfe4-8bca-4ee6-b77a-d8650845ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(lemmatize_post, open('./pickled_models/function_lemmatize_post', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dffd3a-0416-479d-9e91-d7dbd139907e",
   "metadata": {},
   "source": [
    "#### Multiple Estimator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05cd6f3a-0294-496e-931e-58c096f06156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating multiple classifiers in the same RandomSearchCV, trying different combinations of Tfidf / CountVectorizer and LogisticRegression() / MultinomialNB\n",
    "# Inspiration: Wrapper Class (https://stackoverflow.com/questions/50285973/pipeline-multiple-classifiers).  Content: DSI Lesson 507 on OOP (https://git.generalassemb.ly/bobadams1/507-lesson-object-oriented-programming)\n",
    "'''\n",
    "Notes from Inspiration above (no copy-paste):\n",
    "1. Need BaseEstimator() as the base class for all sklearn estimators - as a stand in for the estimator being selected\n",
    "2. The class only really needs to to have self and the estimator as objects in the class.\n",
    "3. The methods you would normally call for the estimator should be defined as functions within the model (don't forget to pass self every time!)\n",
    "'''\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Multi_Classifier(BaseEstimator):\n",
    "    def __init__(self, estimator = MultinomialNB()): #Multinomial NB as default\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def fit(self, X, y): # interested in LogisticRegression, NB... both take primarily X,y\n",
    "        return self.estimator.fit(X,y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "    \n",
    "    def score(self, X,y):\n",
    "        return self.estimator.score(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d9e3b-8203-4817-a9c6-5d09370523cf",
   "metadata": {},
   "source": [
    "#### Model Evaluation - Centralized Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c1a9df6-8671-43c9-a0f1-fa4cb7f8fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "def model_evaluation(model, model_name):\n",
    "    \n",
    "    # print(model_performance)\n",
    "    \n",
    "    try:\n",
    "        best_params = model.best_params_\n",
    "    except:\n",
    "        best_params = 'Not Applicable'\n",
    "        \n",
    "    try:\n",
    "        best_score = model.best_score_\n",
    "    except:\n",
    "        best_score = 'Not Applicable'\n",
    "    \n",
    "    #Print Model Evaluations to the screen\n",
    "    print(f\"Train-Test Accuracy Scores:\\n  Train: {round(model.score(X_train, y_train),5)} \\n  Test: {round(model.score(X_test, y_test),5)}\\n  Baseline: {round(dummy_accuracy,5)}\\n---\")\n",
    "    print(f\"\\n Classification Report:\\n{classification_report(y_test, model.predict(X_test), digits = 4)}\")\n",
    "    print(f\"\\n---\\nBest Parameters: \\n{best_params}\")\n",
    "    \n",
    "    # Plot and Save the Confusion Matrix\n",
    "    preds = model.predict(X_test)\n",
    "    plt.figure(figsize = (8,5))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, preds, cmap = 'YlOrBr', display_labels=['r/dating','r/datingoverthirty'])\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "    plt.savefig(fname= f'./images/{model_name}_Confusion_Matrix.png', bbox_inches = 'tight', dpi = 200)\n",
    "    plt.show()\n",
    "    \n",
    "    #Append results of key metrics to \n",
    "    # pd.concat(model_performance_capture,\n",
    "        \n",
    "    \n",
    "    model_performance = pd.DataFrame({\n",
    "        'model_name' : model_name,\n",
    "        'model' : model,\n",
    "        'best_score_CV' : best_score,\n",
    "        'train_accuracy' : model.score(X_train, y_train),\n",
    "        'test_accuracy' : model.score(X_test, y_test),\n",
    "        'baseline_accuracy' : dummy_accuracy,\n",
    "        'model_params' : [best_params]\n",
    "        })\n",
    "\n",
    "    return model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefba46-5a7f-4f3f-b223-8fb98e0f6ead",
   "metadata": {},
   "source": [
    "## Multiple-Evaluator Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9b446-9390-4b88-bb83-c473c9609104",
   "metadata": {},
   "source": [
    "### 01 - RandomSearch over Multiple Estimators with Tfidf Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddd8c8-5ab3-4535-b072-dab124957860",
   "metadata": {},
   "source": [
    "#### Pipeline & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b41831c-f116-4628-b840-075752cf6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('tvec' , TfidfVectorizer()),\n",
    "    # ('sc', StandardScaler()),\n",
    "    ('cls' , Multi_Classifier())\n",
    "])\n",
    "\n",
    "tvec_params1 = {'tvec__preprocessor': [lemmatize_post],     # Lemmatizing showed the best results in initial testing\n",
    "                'tvec__max_df': [1.0, 0.9],\n",
    "                'tvec__max_features': [None, 5000],\n",
    "                'tvec__min_df': [1],\n",
    "                'tvec__ngram_range': [(1,2)],               #words and bigrams showed best results early\n",
    "                'tvec__stop_words': ['english'] }\n",
    "\n",
    "logr_params1 = {'cls__estimator': [LogisticRegression()],\n",
    "                'cls__estimator__C': np.linspace(0.9, 2, 9)}\n",
    "\n",
    "mnb_params1 = {'cls__estimator': [MultinomialNB()]}\n",
    "\n",
    "ksvm_params1 = {'cls__estimator': [SVC()],\n",
    "                'cls__estimator__C': np.linspace(0.05, 2, 7),\n",
    "                'cls__estimator__degree': [2,3],\n",
    "                'cls__estimator__kernel': ['poly','rbf']}\n",
    "\n",
    "\n",
    "params1 = [# list of params... one for each estimator (order matters here). Cite: Tim Office Hours\n",
    "        # Logistic Regression\n",
    "        tvec_params1 | logr_params1\n",
    "        # Multinomial Naive Bayes\n",
    "        ,tvec_params1 | mnb_params1\n",
    "        #Kernelized SVM\n",
    "        ,tvec_params1 | ksvm_params1\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3634f8bd-9429-481d-9f12-aad28a337c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1 = RandomizedSearchCV(estimator=pipe1,\n",
    "                        param_distributions=params1,\n",
    "                        cv = 5,\n",
    "                        n_iter = 100\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795b264-fb94-45af-a4d3-17fddebf17ba",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aff007-a2ae-4a50-8a6b-7bc71fffe696",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 11 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "rs1.fit(X_train, y_train)\n",
    "# pickle.dump(rs1, open('./pickled_models/rs1_multi_tvec.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3fb95-12d5-4b3d-be2c-959df629c0e5",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55cf62-d44e-41ab-b098-e4127cbe1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs1, 'Model_1_RSCV_Multi_Tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351f11b-115a-467c-a4c6-4d1c8f882ba5",
   "metadata": {},
   "source": [
    "### 02 - RandomSearchCV over Multiple Estimators with CountVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79c4d0-18c5-49fc-b11c-327a173c29bd",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4b70e-317c-4068-bea6-7b27cf3af673",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('cvec' , CountVectorizer()),\n",
    "    ('cls' , Multi_Classifier())\n",
    "])\n",
    "\n",
    "cvec_params2 = {'cvec__max_df': [0.95, 0.9, 0.85],\n",
    "         'cvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'cvec__preprocessor': [None, lemmatize_post],\n",
    "         'cvec__stop_words': [None, 'english']}\n",
    "\n",
    "logr_params2 = {'cls__estimator': [LogisticRegression()],\n",
    "                'cls__estimator__C': np.linspace(0.00001, 1, 9)}\n",
    "\n",
    "mnb_params2 = {'cls__estimator': [MultinomialNB()],}\n",
    "\n",
    "ksvm_params2 = {'cls__estimator': [SVC()],\n",
    "                 'cls__estimator__C': np.linspace(0.05, 2, 7),\n",
    "                 'cls__estimator__degree': [2,3],\n",
    "                 'cls__estimator__kernel': ['poly','rbf']}\n",
    "\n",
    "params2 = [ # list of params... one for each estimator (order matters here). Cite: Tim Office Hours\n",
    "            ## Logistic Regression\n",
    "            cvec_params2 | logr_params2      \n",
    "\n",
    "            ## Multinomial Naive Bayes\n",
    "             ,cvec_params2| mnb_params2\n",
    "    \n",
    "            #Kernelized SVM\n",
    "            ,cvec_params2 | ksvm_params2\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab060a0-f6f2-4755-8ad3-ad3a7c63f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs2 = RandomizedSearchCV(estimator=pipe2,\n",
    "                        param_distributions=params2,\n",
    "                        cv = 5,\n",
    "                        n_iter = 100\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bfb00-c0ac-41f4-af5b-43425ecb9c82",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633b13e-6e5d-44d6-8164-d53ff90c3151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "rs2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0887909-7baa-450e-900a-36e76bae00fa",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8555740-ff8d-48a1-b823-27bde51202d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs2, 'Model_2_RsCV_Multi_CVEC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ee78a-6b5a-42f2-aeda-975e77901061",
   "metadata": {},
   "source": [
    "## Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c3a21-ef1f-470d-b8be-fb46e766c058",
   "metadata": {},
   "source": [
    "### 03 - Bagged Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46161a84-ea49-4b67-bd0f-47241641916d",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd9a4c-8836-48ff-a312-35e957226203",
   "metadata": {},
   "outputs": [],
   "source": [
    "params3 = {\n",
    "     'tvec__preprocessor': [None                            #Best Fit V1\n",
    "                            # ,stem_post\n",
    "                            ,lemmatize_post\n",
    "                           ],\n",
    "     'tvec__max_df': np.linspace(0.75, 0.95,6),               #0.9 on [1, 0.9] V1\n",
    "     'tvec__max_features': [4000, 5000, 6000], #5000 on [None, 5000] V1\n",
    "     'tvec__min_df': [1],\n",
    "     'tvec__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tvec__stop_words': ['english'],                #english best in V1 [None, 'english']\n",
    "     \n",
    "    'bag__estimator__max_depth': np.arange(8,11,1),  #10 on V1\n",
    "     'bag__estimator__min_samples_leaf': np.arange(1, 4, 1), # 1 in V1\n",
    "     'bag__n_estimators': [200]\n",
    "}\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "pipe3 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('bag', BaggingClassifier(tree))\n",
    "])\n",
    "\n",
    "rs3 = RandomizedSearchCV(estimator=pipe3, \n",
    "                         param_distributions=params3, \n",
    "                         cv = 5, \n",
    "                         n_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746825c3-55bd-40b1-8624-d177baa05d2f",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc73be2-492c-4b22-ae7a-93290f6137f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rs3.fit(X_train, y_train)\n",
    "# pickle.dump(rs3, open('./pickled_models/rs3_bagged_trees.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9844a7-fd53-4164-bb7a-7e69903ca786",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa572cb-6338-4f09-a200-093d3c7da77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs3, 'Model_3_Bagged_Trees'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ef965-d9b3-44cb-a21b-7cfad5bda13b",
   "metadata": {},
   "source": [
    "### 04 - RandomForest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f3dc0-a339-4495-9bc8-c5b078a99b77",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecea7f-6b25-4c38-9c5e-d63ee8386855",
   "metadata": {},
   "outputs": [],
   "source": [
    "params4 = {\n",
    "    'tvec__preprocessor': [None], #selected from lemmatize_post, None\n",
    "     'tvec__max_df': [0.83],        # selected from np.linspace(0.75, 0.95, 6)      \n",
    "     'tvec__max_features': [4000, 5000, 6000],\n",
    "     'tvec__min_df': [0.001, 0.005, 0.01, 0.05],\n",
    "     'tvec__ngram_range': [(1, 2)],  # selected from words and bigrams\n",
    "     'tvec__stop_words': ['english'],       \n",
    "    \n",
    "     'rfc__max_depth': [10,20, 30],\n",
    "     'rfc__min_samples_split': [6,8,10],\n",
    "     'rfc__n_estimators': [100],\n",
    "     'rfc__random_state': [2187]\n",
    "}\n",
    "\n",
    "pipe4 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rfc', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "rs4 = RandomizedSearchCV(estimator=pipe4, \n",
    "                         param_distributions=params4, \n",
    "                         n_iter = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e61502-0cba-4252-a88f-4a879de255ea",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60098ba6-6b02-48f1-a3f1-963270a076a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rs4.fit(X_train, y_train)\n",
    "# pickle.dump(rs4, open('./pickled_models/rs4_Random_Forest.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069793dd-f495-4fc8-8584-a8c5c740ae22",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc69f3-0adf-4299-855a-eab116adb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs4, 'Model_4_Random_Forrest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6e51d-71f1-4507-a263-6652ef6331d2",
   "metadata": {},
   "source": [
    "### 05 - Boosted Trees with AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f728d-3a61-4c38-82f9-0c57c7ea97c6",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf29a38-e8c8-454b-94a2-d653ad8eac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "params5 = {\n",
    "     'tvec__preprocessor': [None,lemmatize_post],\n",
    "     'tvec__max_df': np.linspace(0.75, 0.95,6),              \n",
    "     'tvec__max_features': [4000, 5000, 6000], \n",
    "     'tvec__min_df': [1],\n",
    "     'tvec__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tvec__stop_words': ['english'],      \n",
    "    \n",
    "     'ada__estimator': [None], #DecisionTreeClassifier default\n",
    "     'ada__learning_rate': [0.1, 1, 10],\n",
    "     'ada__n_estimators': [10,20,30],\n",
    "}\n",
    "\n",
    "pipe5 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "rs5 = RandomizedSearchCV(estimator=pipe5, param_distributions=params5, cv = 5, n_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3ba14-45c9-4f42-85f8-29542a6685be",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c689a-b452-401b-985c-dff2129d7965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rs5.fit(X_train, y_train)\n",
    "# pickle.dump(rs5, open('./pickled_models/rs5_AdaBoosted_Trees.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d789cce-c98d-4341-a160-c5d2b1b6e57e",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc442e8-53a5-483b-9076-2a9d506a4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs5, 'Model_5_AdaBoostClassifier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad56b0-a2fb-4787-bb61-0ed9560b85aa",
   "metadata": {},
   "source": [
    "## Single Estimator Models for Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9603bd-d086-4dfb-91e9-47c23a7fe4a4",
   "metadata": {},
   "source": [
    "### 06 Kernelized SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a7e3f-5335-4b3f-bd8e-227275bb72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiVectorizer(TransformerMixin):\n",
    "#     def __init__(self, vectorizer='CountVectorizer', **kwargs):  # CountVectorizer as Default\n",
    "#         self.label = vectorizer\n",
    "#         if vectorizer == 'TFIDF':\n",
    "#             self.vectorizer = TfidfVectorizer(**kwargs)\n",
    "#         elif vectorizer == 'CountVectorizer':\n",
    "#             self.vectorizer = CountVectorizer(**kwargs)\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self.vectorizer.fit(X, y)\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         return self.vectorizer.transform(X)\n",
    "    \n",
    "#     def get_params(self, **kwargs):\n",
    "#         return {**self.vectorizer.get_params(**kwargs), 'vectorizer':self.label}\n",
    "    \n",
    "#     def set_params(self, **params):\n",
    "#         return self.vectorizer.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2444a-e944-4c57-8549-72feb4195b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec6 = {\n",
    "     'tvec__preprocessor': [None],\n",
    "     'tvec__max_df': np.linspace(0.50, 0.80,6),              \n",
    "     'tvec__max_features': [5000, 6000, 7000], \n",
    "     'tvec__min_df': [0.001, 0.005, 0.01, 0.05],\n",
    "     'tvec__ngram_range': [(1, 1)],\n",
    "     'tvec__stop_words': ['english'],\n",
    "}\n",
    "\n",
    "ksvm_params6 = {\n",
    "    'ksvm__C': np.linspace(1.5, 2.5, 6),\n",
    "    'ksvm__degree': [2],\n",
    "    'ksvm__kernel': ['poly', 'rbf']\n",
    "}\n",
    "\n",
    "params6 = {\n",
    "    **tvec6, **ksvm_params6\n",
    "}\n",
    "\n",
    "pipe6 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('ksvm', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbfdd4-8eec-43ff-a89f-11b203cdc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs6 = RandomizedSearchCV(estimator=pipe6,\n",
    "                        param_distributions=params6,\n",
    "                        cv = 5,\n",
    "                        n_iter = 100\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da605297-d2c5-4fc2-9697-b1ae162954d7",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93745933-8272-4266-a258-75eec741f370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "rs6.fit(X_train, y_train)\n",
    "# pickle.dump(rs6, open('./pickled_models/rs6_SVM_Multi_Vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c14d04-3cd4-461d-b8ec-d8e27eea6db9",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177ce49-a34b-47a4-9378-c1c18680b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs6, 'Model_6_SVM_TFIDF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69448de-d2ca-495c-b618-e8ba462db899",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d826dd5-586f-4a3d-8975-3d9971c8fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cite: mors: https://stackoverflow.com/questions/42920148/using-sklearn-voting-ensemble-with-partial-fit\n",
    "# VotingClassifier Ensembling without model refit.\n",
    "classifier_list = [rs1, rs2, rs3, rs4, rs5, rs6]\n",
    "\n",
    "classifier_estimators = VotingClassifier(estimators = [('Model_1_RSCV_Multi_Tfidf' ,rs1),\n",
    "                                                       ('Model_2_RsCV_Multi_CVEC', rs2),\n",
    "                                                       ('Model_3_Bagged_Trees', rs3),\n",
    "                                                      ('Model_4_Random_Forrest', rs4),\n",
    "                                                      ('Model_5_AdaBoostClassifier', rs5),\n",
    "                                                      ('Model_6_SVM_TFIDF', rs6)], voting='hard')\n",
    "\n",
    "classifier_estimators.estimators_ = classifier_list\n",
    "classifier_estimators.le_ = LabelEncoder().fit(y)\n",
    "classifier_estimators.classes_ = classifier_estimators.le_.classes_\n",
    "\n",
    "# Now it will work without calling fit\n",
    "classifier_estimators.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627af59b-294a-4276-805f-d2d7e0a8f63a",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec99838-c900-4655-92b0-73c36803a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(classifier_estimators, 'Model_7_VotingClassifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787bc49-9d5a-4344-b2df-f4fd6ac67dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(model_performance_capture[['model_name', 'train_accuracy', 'test_accuracy','model_params']].sort_values(by = 'test_accuracy', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8451aca-bcc3-4db9-a807-e3f535c2d484",
   "metadata": {},
   "source": [
    "## Model Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df9cf7-3683-4716-a996-f22de63edeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([X_test, y_test], axis = 1)\n",
    "# test_data['post_index'] = test_data.index\n",
    "test_data.reset_index(inplace = True)\n",
    "\n",
    "# model_predictions = pd.DataFrame({\n",
    "#     'self_text_and_comment' : test_data['self_text_and_comment'],\n",
    "#     'subreddit_actual' : test_data['subreddit'],\n",
    "#     'Model_1_RSCV_Multi_Tfidf' : pd.Series(rs1.predict(X_test)),\n",
    "#     'Model_2_RsCV_Multi_CVEC' : pd.Series(rs2.predict(X_test)),\n",
    "#     'Model_3_Bagged_Trees' : pd.Series(rs3.predict(X_test)),\n",
    "#     'Model_4_Random_Forrest' : pd.Series(rs4.predict(X_test)),\n",
    "#     'Model_5_AdaBoostClassifier' : pd.Series(rs5.predict(X_test)),\n",
    "#     'Model_6_SVM_TFIDF' : pd.Series(rs6.predict(X_test)),\n",
    "#     'Model_7_VotingClassifier' : pd.Series(classifier_estimators.predict(X_test))\n",
    "# })\n",
    "model_predictions = pd.concat([test_data,\n",
    "                      pd.Series(rs1.predict(X_test)),\n",
    "                      pd.Series(rs2.predict(X_test)),\n",
    "                      pd.Series(rs3.predict(X_test)), \n",
    "                      pd.Series(rs4.predict(X_test)), \n",
    "                      pd.Series(rs5.predict(X_test)), \n",
    "                      pd.Series(rs6.predict(X_test)),\n",
    "                      pd.Series(classifier_estimators.predict(X_test))], axis = 1)\n",
    "model_predictions.rename(columns = {\n",
    "    0 : 'Model_1_RSCV_Multi_Tfidf',\n",
    "    1 : 'Model_2_RsCV_Multi_CVEC',\n",
    "    2 : 'Model_3_Bagged_Trees',\n",
    "    3 : 'Model_4_Random_Forrest',\n",
    "    4 : 'Model_5_AdaBoostClassifier',\n",
    "    5 : 'Model_6_SVM_TFIDF',\n",
    "    6 : 'Model_7_VotingClassifier'\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca90ab2-25f9-4a4c-a944-ad6cd3a8fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "mask = np.triu(np.ones_like(model_predictions[model_cols].corr()))\n",
    "sns.heatmap(model_predictions[model_cols].corr(), cmap = 'Oranges', vmin = 0.5, vmax = 1, annot = True, mask = mask)\n",
    "plt.title('Model Classification Correlation')\n",
    "plt.savefig(dpi = 200, bbox_inches = 'tight', fname = '../images/ModelClassification_Correlation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18859ada-d2bc-4455-9d5e-b8837856ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols = ['Model_1_RSCV_Multi_Tfidf','Model_2_RsCV_Multi_CVEC','Model_3_Bagged_Trees','Model_4_Random_Forrest','Model_5_AdaBoostClassifier','Model_6_SVM_TFIDF','Model_7_VotingClassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea78419-c510-4b4c-b722-791b8105d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions['models_missed'] = sum([np.abs(model_predictions['subreddit'] - model_predictions[col]) for col in model_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009fb2c-1652-49df-b150-5084db7b7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions.models_missed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d65dee-fe4c-4718-bc27-f03e08378f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,5))\n",
    "plt.hist(model_predictions['models_missed'],\n",
    "    bins = np.arange(0,9,1), rwidth=0.94, align='left',color = '#cc7707')\n",
    "plt.title('Model Misclassification Frequency by Post')\n",
    "plt.xlabel('Count of Models Misclassifying Subreddit')\n",
    "plt.ylabel('Count of Reddit Posts (Test Data)')\n",
    "plt.grid(visible=True,which = 'major',axis = 'y' )\n",
    "plt.savefig(dpi = 200, bbox_inches = 'tight', fname = '../images/model_misclassifications.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b3435-acb7-41b5-bc1d-7d167b781f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions['self_text_and_comment'].str.split(' ').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865cdb0-47a4-423f-a0e6-5cec16075d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (12,5))\n",
    "\n",
    "sns.boxplot(ax = ax1, x=model_predictions.loc[model_predictions['subreddit']==1, 'models_missed'], \n",
    "            y = model_predictions.loc[model_predictions['subreddit']==1, 'self_text_and_comment'].str.split(' ').str.len(),color = '#98c4ba')\n",
    "ax1.set_title('r/datingoverthirty Post Misclassifications vs. Word Count')\n",
    "ax1.set_xlabel('Models Misclassifying Post')\n",
    "ax1.set_ylabel('Post Conversation Word Count')\n",
    "\n",
    "sns.boxplot(ax = ax2, x=model_predictions.loc[model_predictions['subreddit']==0, 'models_missed'], \n",
    "            y = model_predictions.loc[model_predictions['subreddit']==0, 'self_text_and_comment'].str.split(' ').str.len(), color = '#f79411')\n",
    "ax2.set_title('r/dating Post Misclassifications vs. Word Count')\n",
    "ax2.set_xlabel('Models Misclassifying Post')\n",
    "ax2.set_ylabel('Post Conversation Word Count')\n",
    "plt.savefig(bbox_inches = 'tight', dpi = 200, fname = '../images/PostMisclassifications_vs_Word_Count.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388323b4-b2f2-4f02-a336-8218238fd498",
   "metadata": {},
   "source": [
    "### Mis-Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5660e2-379f-415d-83ef-54d055ec23d3",
   "metadata": {},
   "source": [
    "#### Actual: r/datingoverthirty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874e227-6e35-4af5-93ad-f239f5a5ad00",
   "metadata": {},
   "source": [
    "> The models unanimously missed 13 posts from r/datingoverthirty and classified them as r/dating.\n",
    "> These all seem to be shorter posts, generally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b4e3a-1b2d-4b32-991e-89055feeed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions.loc[(model_predictions['models_missed'] == 7) & (model_predictions['subreddit'] == 1),'self_text_and_comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f734782-800c-4087-8e51-4bd5e62448e8",
   "metadata": {},
   "source": [
    "#### Actual: r/dating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146d9c7-de29-4947-8a6f-cc30c46430a3",
   "metadata": {},
   "source": [
    "> The models unanimously misclassified 13 posts from r/dating into r/datingoverthirty\n",
    "* These are longer posts - age tags (ex. 31f) may be causing a shift towards the age-bucketed subreddit from the general forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa0ea2-3aa3-44ec-88b9-35d1d1c54488",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions.loc[(model_predictions['models_missed'] == 7) & (model_predictions['subreddit'] == 0),'self_text_and_comment']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
