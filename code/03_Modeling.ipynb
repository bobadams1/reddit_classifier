{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0371cdb1-ed20-4d08-8745-e19493d44d7c",
   "metadata": {},
   "source": [
    "# Model Fitting and Evalutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975f39b-5b89-4986-98b9-c8ad52908caf",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "b25aff06-ced7-4d60-8e25-323c105b7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "from sklearn.base import TransformerMixin # for class-driven multi-vectorizer assessment \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "from functions.stem_post import *\n",
    "from functions.lemmatize_post import *\n",
    "from functions.model_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4055044c-3567-4d18-a584-f761ea52a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_extract = '2023-06-11 16:25'\n",
    "df = pd.read_csv(f'data/reddit_posts_raw_{most_recent_extract}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb092a5-94f3-42c0-b8ad-7c84a81271b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1971, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ebf688-8ea0-4043-88eb-90c52d8becb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>top_comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dating</td>\n",
       "      <td>1471ube</td>\n",
       "      <td>2023-06-11 18:49:33</td>\n",
       "      <td>Am I Clueless?</td>\n",
       "      <td>So there is this girl I’ve known my whole life...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit       id          created_utc           title  \\\n",
       "0    dating  1471ube  2023-06-11 18:49:33  Am I Clueless?   \n",
       "\n",
       "                                            selftext top_comment_text  \n",
       "0  So there is this girl I’ve known my whole life...              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53578f0-9e88-4023-92ad-e103fa24003a",
   "metadata": {},
   "source": [
    "### Data Leveraged\n",
    "For this project, reddit posts were pulled from two subreddits: r/dating, and r/datingoverthirty.  Due to limitations with community access (Summer 2023 Reddit Blackout) and APIs (the removal of some of Reddit's APIs), sourcing data was impeded.  Approximately 1000 posts were sourced the day before protests began, constituting the working dataset used in EDA and Modeling.  To augment the information available, the text of the top-voted comment was pulled for each post.  This enables the investigation to cover broader community interaction.\n",
    "\n",
    "Comment text was interesting in EDA to understand the post-community response in each subreddit.  Title, Selftext and Comment Text are all leveraged to maximize the use of available data and to attempt to differentiate the communities based on the conversation (the post, and community-supported response.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021088-c511-400b-ae95-5ea30d513cb5",
   "metadata": {},
   "source": [
    "#### Self-text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "52b87f74-a99a-4930-a060-41a47fe7e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.Series(df['selftext'])\n",
    "# y = df['subreddit'].map({'dating': 0,\n",
    "#                     'datingoverthirty':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792701d4-b20f-48ca-ace2-e34628960c6a",
   "metadata": {},
   "source": [
    "#### Self Text and Top Comment - Alternative Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "e81dca47-8a62-4dc1-835b-67b92a603b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['self_text_and_comment'] = df['title'].astype(str) +' '+ df['selftext'].astype(str) +' '+ df['top_comment_text'].astype(str)\n",
    "X = pd.Series(df['self_text_and_comment'])\n",
    "y = df['subreddit'].map({'dating': 0,\n",
    "                   'datingoverthirty':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c0234-f3f8-4a5b-b6e5-bd6245d5e1de",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf8e268-e628-47f6-937f-8bde2e2ab325",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "X_train.to_pickle('./pickled_models/X_train.pkl')\n",
    "X_test.to_pickle('./pickled_models/X_test.pkl')\n",
    "y_train.to_pickle('./pickled_models/y_train.pkl')\n",
    "y_test.to_pickle('./pickled_models/y_test.pkl')\n",
    "\n",
    "# X_train = pd.read_pickle('./pickled_models/X_train.pkl')\n",
    "# X_test = pd.read_pickle('./pickled_models/X_test.pkl')\n",
    "# y_train = pd.read_pickle('./pickled_models/y_train.pkl')\n",
    "# y_test = pd.read_pickle('./pickled_models/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674e456-0407-4f36-897a-52dfdde06822",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c35f5f-2dd3-4ad8-934e-ec3bb2783183",
   "metadata": {},
   "source": [
    "> The majority class holds 50.63% of responses.  This is the baseline score to beat.\n",
    "\n",
    "> Even class distribution makes a 75/25 train test split possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a934cc43-a7d1-499a-89c2-751b8e072a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5091277890466531"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy_preds = dummy.predict(y_test)\n",
    "dummy_accuracy = accuracy_score(y_test, dummy_preds)\n",
    "dummy_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439cc05-bbd5-459b-8b93-93766af8f2f0",
   "metadata": {},
   "source": [
    "## Baseline Investigation with Standard Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e359a09-fe26-4bac-8415-e06dbdd1cdb9",
   "metadata": {},
   "source": [
    "Vectorizers perform differently on varying corpora.  This simple look in Model Investigations helps shed the light on how these vectorizers perform out of the box with selftext from these subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c4714-6c21-4912-bf6d-c11448013d92",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8937a30d-1edb-4714-9028-2699ef3ce2c3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec0 = CountVectorizer() #standard CountVectorizer\n",
    "cvec0.fit(X_train)\n",
    "# pickle.dump(cvec0, open('./pickled_models/cvec0_baseline', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4df9b-d837-493c-95bb-03c7de07a16c",
   "metadata": {
    "tags": []
   },
   "source": [
    "> See Model Investigaion for Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004fc85f-1d24-4fc9-a9a4-be2d0c2eea52",
   "metadata": {},
   "source": [
    "#### Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf17ad6-346c-4dca-88eb-079fc3281c32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec0 = TfidfVectorizer()\n",
    "tvec0.fit(X_train)\n",
    "# pickle.dump(tvec0, open('./pickled_models/tvec0_baseline', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc3109-57f4-461d-84df-9e014ced471b",
   "metadata": {},
   "source": [
    "> See Model Investigation for Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f253c1-1a47-4c5b-817d-a78af324a13e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93724164-f8db-4116-9831-cfb423a2bc18",
   "metadata": {},
   "source": [
    "#### Model Performance Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c596d17-2f13-486e-a0e7-64615712ed89",
   "metadata": {},
   "source": [
    "##### Model Performance Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "8c5f8db7-2fd4-42b1-a21e-503244720f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Data Frame to capture output from each model fit.\n",
    "model_performance_capture = pd.DataFrame(columns = ['model_name', 'model', 'best_score_CV', 'train_acuracy', 'test_accuracy', 'baseline_accuracy','model_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "524f8ac9-eb94-4262-9cde-40afb3db0bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>model_params</th>\n",
       "      <th>train_acuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "      <th>best_score_CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_1_RSCV_Multi_Tfidf</td>\n",
       "      <td>RandomizedSearchCV(cv=5,\\n                   e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.997970</td>\n",
       "      <td>0.805274</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.798388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_2_RsCV_Multi_CVEC</td>\n",
       "      <td>RandomizedSearchCV(cv=5,\\n                   e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.997970</td>\n",
       "      <td>0.805274</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.798388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_3_Bagged_Trees</td>\n",
       "      <td>RandomizedSearchCV(cv=5,\\n                   e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.939107</td>\n",
       "      <td>0.740365</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.766555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_4_Random_Forrest</td>\n",
       "      <td>RandomizedSearchCV(estimator=Pipeline(steps=[(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.895129</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.742242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_6_SVM_Multi-Vectorizer</td>\n",
       "      <td>RandomizedSearchCV(cv=5,\\n                   e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.572395</td>\n",
       "      <td>0.572008</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.572400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_name  \\\n",
       "0      Model_1_RSCV_Multi_Tfidf   \n",
       "0       Model_2_RsCV_Multi_CVEC   \n",
       "0          Model_3_Bagged_Trees   \n",
       "0        Model_4_Random_Forrest   \n",
       "0  Model_6_SVM_Multi-Vectorizer   \n",
       "\n",
       "                                               model best_score  \\\n",
       "0  RandomizedSearchCV(cv=5,\\n                   e...        NaN   \n",
       "0  RandomizedSearchCV(cv=5,\\n                   e...        NaN   \n",
       "0  RandomizedSearchCV(cv=5,\\n                   e...        NaN   \n",
       "0  RandomizedSearchCV(estimator=Pipeline(steps=[(...        NaN   \n",
       "0  RandomizedSearchCV(cv=5,\\n                   e...        NaN   \n",
       "\n",
       "                                        model_params  train_acuracy  \\\n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...       0.997970   \n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...       0.997970   \n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...       0.939107   \n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...       0.895129   \n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...       0.572395   \n",
       "\n",
       "   test_accuracy  baseline_accuracy  best_score_CV  \n",
       "0       0.805274           0.509128       0.798388  \n",
       "0       0.805274           0.509128       0.798388  \n",
       "0       0.740365           0.509128       0.766555  \n",
       "0       0.793103           0.509128       0.742242  \n",
       "0       0.572008           0.509128       0.572400  "
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each model fit is recorded, including train, test, cross-validated accuracy scores, best scores, \n",
    "model_performance_capture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea6fa8-f6a7-4399-9347-7804c8430ebd",
   "metadata": {},
   "source": [
    "#### Stemming and Lematizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848c670-f0dc-40ab-b790-05baea0a1c08",
   "metadata": {},
   "source": [
    "> Functions for Stemming and Lemmatizing are stored in separate files: stem_post.py, lemmatize_post.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c12595-e429-4d6f-85ef-4bdb583b4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_stemmer = PorterStemmer()\n",
    "# def stem_post(post):\n",
    "#     split_post = post.split(' ')\n",
    "#     return ' '.join([p_stemmer.stem(word) for word in split_post])\n",
    "# #cite 6/9 Breakfast Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b46e54f-84c4-4c2d-a709-7a2767ad4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(stem_post, open('./pickled_models/function_stem_post.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57cac3e0-718b-46a7-af1f-4217a6013053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# # cite: Lesson 504 NLP 1 - Modified to handle complete words.\n",
    "# def lemmatize_post(post):\n",
    "#     mapper = { \n",
    "#         'J': wordnet.ADJ,\n",
    "#         'V': wordnet.VERB,\n",
    "#         'N': wordnet.NOUN,\n",
    "#         'R': wordnet.ADV\n",
    "#     }\n",
    "#     post_split = post.split(' ')\n",
    "#     post_tokens = [(token, tag) for token, tag in nltk.pos_tag(post_split)]\n",
    "#     post_lem = []\n",
    "#     for token in post_tokens:\n",
    "#         pos = mapper.get(token[1][0])\n",
    "#         # post_lem.append((token[0],pos) if pos != None else (token[0]))\n",
    "#         post_lem.append(lemmatizer.lemmatize(token[0], pos) if pos != None else token[0])\n",
    "#     return ' '.join(post_lem).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f085bfe4-8bca-4ee6-b77a-d8650845ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(lemmatize_post, open('./pickled_models/function_lemmatize_post', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dffd3a-0416-479d-9e91-d7dbd139907e",
   "metadata": {},
   "source": [
    "#### Multiple Estimator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05cd6f3a-0294-496e-931e-58c096f06156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating multiple classifiers in the same RandomSearchCV, trying different combinations of Tfidf / CountVectorizer and LogisticRegression() / MultinomialNB\n",
    "# Inspiration: Wrapper Class (https://stackoverflow.com/questions/50285973/pipeline-multiple-classifiers).  Content: DSI Lesson 507 on OOP (https://git.generalassemb.ly/bobadams1/507-lesson-object-oriented-programming)\n",
    "'''\n",
    "Notes from Inspiration above (no copy-paste):\n",
    "1. Need BaseEstimator() as the base class for all sklearn estimators - as a stand in for the estimator being selected\n",
    "2. The class only really needs to to have self and the estimator as objects in the class.\n",
    "3. The methods you would normally call for the estimator should be defined as functions within the model (don't forget to pass self every time!)\n",
    "'''\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Multi_Classifier(BaseEstimator):\n",
    "    def __init__(self, estimator = MultinomialNB()): #Multinomial NB as default\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def fit(self, X, y): # interested in LogisticRegression, NB... both take primarily X,y\n",
    "        return self.estimator.fit(X,y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "    \n",
    "    def score(self, X,y):\n",
    "        return self.estimator.score(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d9e3b-8203-4817-a9c6-5d09370523cf",
   "metadata": {},
   "source": [
    "#### Model Evaluation - Centralized Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "5c1a9df6-8671-43c9-a0f1-fa4cb7f8fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model Performance\n",
    "# def model_evaluation(model, model_name):\n",
    "    \n",
    "#     # print(model_performance)\n",
    "    \n",
    "#     #Print Model Evaluations to the screen\n",
    "#     print(f\"Train-Test Accuracy Scores:\\n  Train: {round(model.score(X_train, y_train),5)} \\n  Test: {round(model.score(X_test, y_test),5)}\\n  Baseline: {round(dummy_accuracy,5)}\\n---\")\n",
    "#     print(f\"\\n Classification Report:\\n{classification_report(y_test, model.predict(X_test), digits = 4)}\")\n",
    "#     print(f\"\\n---\\nBest Parameters: \\n{model.best_params_}\")\n",
    "    \n",
    "#     # Plot and Save the Confusion Matrix\n",
    "#     preds = model.predict(X_test)\n",
    "#     plt.figure(figsize = (8,5))\n",
    "#     ConfusionMatrixDisplay.from_predictions(y_test, preds, cmap = 'YlOrBr', display_labels=['r/dating','r/datingoverthirty'])\n",
    "#     plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "#     # plt.suptitle('Stop Words: English | Unigrams and Bigrams | Max Documents:90% | No Stem/Lem | LogisticRegression', y=0, fontsize = 9)\n",
    "#     plt.savefig(fname= f'./images/{model_name}_Confusion Matrix.png', bbox_inches = 'tight', dpi = 200)\n",
    "#     plt.show()\n",
    "    \n",
    "#     #Append results of key metrics to \n",
    "#     # pd.concat(model_performance_capture,\n",
    "#     model_performance = pd.DataFrame({\n",
    "#         'model_name' : model_name,\n",
    "#         'model' : model,\n",
    "#         'best_score_CV' : model.best_score_,\n",
    "#         'train_acuracy' : model.score(X_train, y_train),\n",
    "#         'test_accuracy' : model.score(X_test, y_test),\n",
    "#         'baseline_accuracy' : dummy_accuracy,\n",
    "#         'model_params' : [model.best_params_]\n",
    "#         })\n",
    "\n",
    "#     return model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefba46-5a7f-4f3f-b223-8fb98e0f6ead",
   "metadata": {},
   "source": [
    "## Multiple-Evaluator Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9b446-9390-4b88-bb83-c473c9609104",
   "metadata": {},
   "source": [
    "### 01 - RandomSearch over Multiple Estimators with Tfidf Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddd8c8-5ab3-4535-b072-dab124957860",
   "metadata": {},
   "source": [
    "#### Pipeline & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "8b41831c-f116-4628-b840-075752cf6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('tvec' , TfidfVectorizer()),\n",
    "    # ('sc', StandardScaler()),\n",
    "    ('cls' , Multi_Classifier())\n",
    "])\n",
    "\n",
    "tvec_params1 = {'tvec__preprocessor': [lemmatize_post],     # Lemmatizing showed the best results in initial testing\n",
    "                'tvec__max_df': [1.0, 0.9],\n",
    "                'tvec__max_features': [None, 5000],\n",
    "                'tvec__min_df': [1],\n",
    "                'tvec__ngram_range': [(1,2)],               #words and bigrams showed best results early\n",
    "                'tvec__stop_words': ['english'] }\n",
    "\n",
    "logr_params1 = {'cls__estimator': [LogisticRegression()],\n",
    "                'cls__estimator__C': np.linspace(0.9, 2, 9)}\n",
    "\n",
    "mnb_params1 = {'cls__estimator': [MultinomialNB()]}\n",
    "\n",
    "ksvm_params1 = {'cls__estimator': [SVC()],\n",
    "                'cls__estimator__C': np.linspace(0.05, 2, 7),\n",
    "                'cls__estimator__degree': [2,3],\n",
    "                'cls__estimator__kernel': ['poly','rbf']}\n",
    "\n",
    "\n",
    "params1 = [# list of params... one for each estimator (order matters here). Cite: Tim Office Hours\n",
    "        # Logistic Regression\n",
    "        tvec_params1 | logr_params1\n",
    "        # Multinomial Naive Bayes\n",
    "        ,tvec_params1 | mnb_params1\n",
    "        #Kernelized SVM\n",
    "        ,tvec_params1 | ksvm_params1\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3634f8bd-9429-481d-9f12-aad28a337c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1 = RandomizedSearchCV(estimator=pipe0,\n",
    "                        param_distributions=params0,\n",
    "                        cv = 5,\n",
    "                        n_iter = 100\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795b264-fb94-45af-a4d3-17fddebf17ba",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d9aff007-a2ae-4a50-8a6b-7bc71fffe696",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 51 is smaller than n_iter=100. Running 51 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;cls&#x27;, Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{&#x27;cls__estimator&#x27;: [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)]...\n",
       "                                        {&#x27;cls__estimator&#x27;: [SVC(C=1.1333333333333335,\n",
       "                                                                degree=2)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         &#x27;cls__estimator__degree&#x27;: [2, 3],\n",
       "                                         &#x27;cls__estimator__kernel&#x27;: [&#x27;poly&#x27;,\n",
       "                                                                    &#x27;rbf&#x27;],\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)],\n",
       "                                         &#x27;tvec__preprocessor&#x27;: [&lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                         &#x27;tvec__stop_words&#x27;: [&#x27;english&#x27;]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-58\" type=\"checkbox\" ><label for=\"sk-estimator-id-58\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;cls&#x27;, Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{&#x27;cls__estimator&#x27;: [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)]...\n",
       "                                        {&#x27;cls__estimator&#x27;: [SVC(C=1.1333333333333335,\n",
       "                                                                degree=2)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         &#x27;cls__estimator__degree&#x27;: [2, 3],\n",
       "                                         &#x27;cls__estimator__kernel&#x27;: [&#x27;poly&#x27;,\n",
       "                                                                    &#x27;rbf&#x27;],\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)],\n",
       "                                         &#x27;tvec__preprocessor&#x27;: [&lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                         &#x27;tvec__stop_words&#x27;: [&#x27;english&#x27;]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-59\" type=\"checkbox\" ><label for=\"sk-estimator-id-59\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()), (&#x27;cls&#x27;, Multi_Classifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-60\" type=\"checkbox\" ><label for=\"sk-estimator-id-60\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-61\" type=\"checkbox\" ><label for=\"sk-estimator-id-61\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cls: Multi_Classifier</label><div class=\"sk-toggleable__content\"><pre>Multi_Classifier()</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-62\" type=\"checkbox\" ><label for=\"sk-estimator-id-62\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-63\" type=\"checkbox\" ><label for=\"sk-estimator-id-63\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                             ('cls', Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{'cls__estimator': [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         'cls__estimator__C': array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         'tvec__ngram_range': [(1, 2)]...\n",
       "                                        {'cls__estimator': [SVC(C=1.1333333333333335,\n",
       "                                                                degree=2)],\n",
       "                                         'cls__estimator__C': array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         'cls__estimator__degree': [2, 3],\n",
       "                                         'cls__estimator__kernel': ['poly',\n",
       "                                                                    'rbf'],\n",
       "                                         'tvec__ngram_range': [(1, 2)],\n",
       "                                         'tvec__preprocessor': [<function lemmatize_post at 0x7fbf48805510>],\n",
       "                                         'tvec__stop_words': ['english']}])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "rs1.fit(X_train, y_train)\n",
    "pickle.dump(rs1, open('./pickled_models/rs1_multi_tvec.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3fb95-12d5-4b3d-be2c-959df629c0e5",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ff55cf62-d44e-41ab-b098-e4127cbe1f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy Scores:\n",
      "  Train: 0.99797 \n",
      "  Test: 0.80527\n",
      "  Baseline: 0.50913\n",
      "---\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8318    0.7562    0.7922       242\n",
      "           1     0.7839    0.8526    0.8168       251\n",
      "\n",
      "    accuracy                         0.8053       493\n",
      "   macro avg     0.8079    0.8044    0.8045       493\n",
      "weighted avg     0.8074    0.8053    0.8047       493\n",
      "\n",
      "\n",
      "---\n",
      "Best Parameters: \n",
      "{'tvec__stop_words': 'english', 'tvec__preprocessor': <function lemmatize_post at 0x7fbf48805510>, 'tvec__ngram_range': (1, 2), 'cls__estimator__C': 1.7555555555555555, 'cls__estimator': LogisticRegression(C=1.7555555555555555)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/4sqt0lt90t72lgqr2zq3p6l80000gn/T/ipykernel_33031/1195430848.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_performance_capture = model_performance_capture.append(model_evaluation(rs1, 'Model_1_RSCV_Multi_Tfidf'))\n"
     ]
    }
   ],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs1, 'Model_1_RSCV_Multi_Tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351f11b-115a-467c-a4c6-4d1c8f882ba5",
   "metadata": {},
   "source": [
    "### 02 - RandomSearchCV over Multiple Estimators with CountVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79c4d0-18c5-49fc-b11c-327a173c29bd",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4b70e-317c-4068-bea6-7b27cf3af673",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([\n",
    "    ('cvec' , CountVectorizer()),\n",
    "    ('cls' , Multi_Classifier())\n",
    "])\n",
    "\n",
    "cvec_params2 = {'cvec__max_df': [0.95, 0.9, 0.85],\n",
    "         'cvec__ngram_range': [(1, 1), (1,2)],\n",
    "         'cvec__preprocessor': [None, lemmatize_post],\n",
    "         'cvec__stop_words': [None, 'english']}\n",
    "\n",
    "logr_params2 = {'cls__estimator': [LogisticRegression()],\n",
    "                'cls__estimator__C': np.linspace(0.00001, 1, 9)}\n",
    "\n",
    "mnb_params2 = {'cls__estimator': [MultinomialNB()],}\n",
    "\n",
    "ksvm_params2 = {'cls__estimator': [SVC()],\n",
    "                 'cls__estimator__C': np.linspace(0.05, 2, 7),\n",
    "                 'cls__estimator__degree': [2,3],\n",
    "                 'cls__estimator__kernel': ['poly','rbf']}\n",
    "\n",
    "params2 = [ # list of params... one for each estimator (order matters here). Cite: Tim Office Hours\n",
    "            ## Logistic Regression\n",
    "            cvec_params2 | logr_params2      \n",
    "\n",
    "            ## Multinomial Naive Bayes\n",
    "             ,cvec_params2| mnb_params2\n",
    "    \n",
    "            #Kernelized SVM\n",
    "            ,cvec_params2 | ksvm_params2\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2ab060a0-f6f2-4755-8ad3-ad3a7c63f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs2 = RandomizedSearchCV(estimator=pipe1,\n",
    "                        param_distributions=params1,\n",
    "                        cv = 5,\n",
    "                        n_iter = 100\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bfb00-c0ac-41f4-af5b-43425ecb9c82",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "30c6a5b5-53ff-4568-acb4-a51680b81b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickled_models/rs2_multi_cvec.pkl', 'wb') as f:\n",
    "    pickle.dump((rs2.best_estimator_, lemmatize_post), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "de81ba7a-e4cd-46ec-b735-e4811075153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rs2.best_estimator_, open('./pickled_models/rs2_multi_cvec.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4633b13e-6e5d-44d6-8164-d53ff90c3151",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;cls&#x27;, Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{&#x27;cls__estimator&#x27;: [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                         &#x27;t...\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         &#x27;cls__estimator__degree&#x27;: [2, 3],\n",
       "                                         &#x27;cls__estimator__kernel&#x27;: [&#x27;poly&#x27;,\n",
       "                                                                    &#x27;rbf&#x27;],\n",
       "                                         &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                         &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                         &#x27;tvec__min_df&#x27;: [1],\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)],\n",
       "                                         &#x27;tvec__preprocessor&#x27;: [&lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                         &#x27;tvec__stop_words&#x27;: [&#x27;english&#x27;]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-64\" type=\"checkbox\" ><label for=\"sk-estimator-id-64\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;cls&#x27;, Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{&#x27;cls__estimator&#x27;: [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                         &#x27;t...\n",
       "                                         &#x27;cls__estimator__C&#x27;: array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         &#x27;cls__estimator__degree&#x27;: [2, 3],\n",
       "                                         &#x27;cls__estimator__kernel&#x27;: [&#x27;poly&#x27;,\n",
       "                                                                    &#x27;rbf&#x27;],\n",
       "                                         &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                         &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                         &#x27;tvec__min_df&#x27;: [1],\n",
       "                                         &#x27;tvec__ngram_range&#x27;: [(1, 2)],\n",
       "                                         &#x27;tvec__preprocessor&#x27;: [&lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                         &#x27;tvec__stop_words&#x27;: [&#x27;english&#x27;]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-65\" type=\"checkbox\" ><label for=\"sk-estimator-id-65\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()), (&#x27;cls&#x27;, Multi_Classifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-66\" type=\"checkbox\" ><label for=\"sk-estimator-id-66\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-67\" type=\"checkbox\" ><label for=\"sk-estimator-id-67\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cls: Multi_Classifier</label><div class=\"sk-toggleable__content\"><pre>Multi_Classifier()</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-68\" type=\"checkbox\" ><label for=\"sk-estimator-id-68\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-69\" type=\"checkbox\" ><label for=\"sk-estimator-id-69\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                             ('cls', Multi_Classifier())]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions=[{'cls__estimator': [LogisticRegression(C=1.7555555555555555)],\n",
       "                                         'cls__estimator__C': array([0.9       , 1.02222222, 1.14444444, 1.26666667, 1.38888889,\n",
       "       1.51111111, 1.63333333, 1.75555556, 1.87777778, 2.        ]),\n",
       "                                         'tvec__max_df': [1.0, 0.9],\n",
       "                                         't...\n",
       "                                         'cls__estimator__C': array([0.05      , 0.26666667, 0.48333333, 0.7       , 0.91666667,\n",
       "       1.13333333, 1.35      , 1.56666667, 1.78333333, 2.        ]),\n",
       "                                         'cls__estimator__degree': [2, 3],\n",
       "                                         'cls__estimator__kernel': ['poly',\n",
       "                                                                    'rbf'],\n",
       "                                         'tvec__max_df': [1.0, 0.9],\n",
       "                                         'tvec__max_features': [None, 5000],\n",
       "                                         'tvec__min_df': [1],\n",
       "                                         'tvec__ngram_range': [(1, 2)],\n",
       "                                         'tvec__preprocessor': [<function lemmatize_post at 0x7fbf48805510>],\n",
       "                                         'tvec__stop_words': ['english']}])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "rs2.fit(X_train, y_train)\n",
    "pickle.dump(rs2, open('./pickled_models/rs2_multi_cvec.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0887909-7baa-450e-900a-36e76bae00fa",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f8555740-ff8d-48a1-b823-27bde51202d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy Scores:\n",
      "  Train: 0.99797 \n",
      "  Test: 0.80527\n",
      "  Baseline: 0.50913\n",
      "---\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8318    0.7562    0.7922       242\n",
      "           1     0.7839    0.8526    0.8168       251\n",
      "\n",
      "    accuracy                         0.8053       493\n",
      "   macro avg     0.8079    0.8044    0.8045       493\n",
      "weighted avg     0.8074    0.8053    0.8047       493\n",
      "\n",
      "\n",
      "---\n",
      "Best Parameters: \n",
      "{'tvec__stop_words': 'english', 'tvec__preprocessor': <function lemmatize_post at 0x7fbf48805510>, 'tvec__ngram_range': (1, 2), 'tvec__min_df': 1, 'tvec__max_features': None, 'tvec__max_df': 0.9, 'cls__estimator__C': 1.7555555555555555, 'cls__estimator': LogisticRegression(C=1.7555555555555555)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/4sqt0lt90t72lgqr2zq3p6l80000gn/T/ipykernel_33031/2230439169.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_performance_capture = model_performance_capture.append(model_evaluation(rs2, 'Model_2_RsCV_Multi_CVEC'))\n"
     ]
    }
   ],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs2, 'Model_2_RsCV_Multi_CVEC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ee78a-6b5a-42f2-aeda-975e77901061",
   "metadata": {},
   "source": [
    "## Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c3a21-ef1f-470d-b8be-fb46e766c058",
   "metadata": {},
   "source": [
    "### 03 - Bagged Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46161a84-ea49-4b67-bd0f-47241641916d",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "becd9a4c-8836-48ff-a312-35e957226203",
   "metadata": {},
   "outputs": [],
   "source": [
    "params3 = {\n",
    "     'tvec__preprocessor': [None                            #Best Fit V1\n",
    "                            # ,stem_post\n",
    "                            ,lemmatize_post\n",
    "                           ],\n",
    "     'tvec__max_df': np.linspace(0.75, 0.95,6),               #0.9 on [1, 0.9] V1\n",
    "     'tvec__max_features': [4000, 5000, 6000], #5000 on [None, 5000] V1\n",
    "     'tvec__min_df': [1],\n",
    "     'tvec__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tvec__stop_words': ['english'],                #english best in V1 [None, 'english']\n",
    "     \n",
    "    'bag__estimator__max_depth': np.arange(8,11,1),  #10 on V1\n",
    "     'bag__estimator__min_samples_leaf': np.arange(1, 4, 1), # 1 in V1\n",
    "     'bag__n_estimators': [200]\n",
    "}\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "pipe3 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('bag', BaggingClassifier(tree))\n",
    "])\n",
    "\n",
    "rs3 = RandomizedSearchCV(estimator=pipe3, \n",
    "                         param_distributions=params3, \n",
    "                         cv = 5, \n",
    "                         n_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746825c3-55bd-40b1-8624-d177baa05d2f",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4dc73be2-492c-4b22-ae7a-93290f6137f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {color: black;background-color: white;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;bag&#x27;,\n",
       "                                              BaggingClassifier(estimator=DecisionTreeClassifier()))]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={&#x27;bag__estimator__max_depth&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       "                                        &#x27;bag__estimator__min_samples_leaf&#x27;: array([ 1,  3,  5,  7,  9, 11]),\n",
       "                                        &#x27;bag__n_estimators&#x27;: [100],\n",
       "                                        &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                        &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                        &#x27;tvec__min_df&#x27;: [1],\n",
       "                                        &#x27;tvec__ngram_range&#x27;: [(1, 2), (1, 2)],\n",
       "                                        &#x27;tvec__preprocessor&#x27;: [None,\n",
       "                                                               &lt;function stem_post at 0x7fbf4dfebb50&gt;,\n",
       "                                                               &lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                        &#x27;tvec__stop_words&#x27;: [None, &#x27;english&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-70\" type=\"checkbox\" ><label for=\"sk-estimator-id-70\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;bag&#x27;,\n",
       "                                              BaggingClassifier(estimator=DecisionTreeClassifier()))]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={&#x27;bag__estimator__max_depth&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       "                                        &#x27;bag__estimator__min_samples_leaf&#x27;: array([ 1,  3,  5,  7,  9, 11]),\n",
       "                                        &#x27;bag__n_estimators&#x27;: [100],\n",
       "                                        &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                        &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                        &#x27;tvec__min_df&#x27;: [1],\n",
       "                                        &#x27;tvec__ngram_range&#x27;: [(1, 2), (1, 2)],\n",
       "                                        &#x27;tvec__preprocessor&#x27;: [None,\n",
       "                                                               &lt;function stem_post at 0x7fbf4dfebb50&gt;,\n",
       "                                                               &lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                        &#x27;tvec__stop_words&#x27;: [None, &#x27;english&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-71\" type=\"checkbox\" ><label for=\"sk-estimator-id-71\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                (&#x27;bag&#x27;, BaggingClassifier(estimator=DecisionTreeClassifier()))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-72\" type=\"checkbox\" ><label for=\"sk-estimator-id-72\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">bag: BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(estimator=DecisionTreeClassifier())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                             ('bag',\n",
       "                                              BaggingClassifier(estimator=DecisionTreeClassifier()))]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'bag__estimator__max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       "                                        'bag__estimator__min_samples_leaf': array([ 1,  3,  5,  7,  9, 11]),\n",
       "                                        'bag__n_estimators': [100],\n",
       "                                        'tvec__max_df': [1.0, 0.9],\n",
       "                                        'tvec__max_features': [None, 5000],\n",
       "                                        'tvec__min_df': [1],\n",
       "                                        'tvec__ngram_range': [(1, 2), (1, 2)],\n",
       "                                        'tvec__preprocessor': [None,\n",
       "                                                               <function stem_post at 0x7fbf4dfebb50>,\n",
       "                                                               <function lemmatize_post at 0x7fbf48805510>],\n",
       "                                        'tvec__stop_words': [None, 'english']})"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs3.fit(X_train, y_train)\n",
    "pickle.dump(rs3, open('./pickled_models/rs3_bagged_trees.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9844a7-fd53-4164-bb7a-7e69903ca786",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1aa572cb-6338-4f09-a200-093d3c7da77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy Scores:\n",
      "  Train: 0.93911 \n",
      "  Test: 0.74037\n",
      "  Baseline: 0.50913\n",
      "---\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7127    0.7893    0.7490       242\n",
      "           1     0.7733    0.6932    0.7311       251\n",
      "\n",
      "    accuracy                         0.7404       493\n",
      "   macro avg     0.7430    0.7412    0.7401       493\n",
      "weighted avg     0.7436    0.7404    0.7399       493\n",
      "\n",
      "\n",
      "---\n",
      "Best Parameters: \n",
      "{'tvec__stop_words': 'english', 'tvec__preprocessor': None, 'tvec__ngram_range': (1, 2), 'tvec__min_df': 1, 'tvec__max_features': 5000, 'tvec__max_df': 0.9, 'bag__n_estimators': 100, 'bag__estimator__min_samples_leaf': 1, 'bag__estimator__max_depth': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/4sqt0lt90t72lgqr2zq3p6l80000gn/T/ipykernel_33031/1974463261.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_performance_capture = model_performance_capture.append(model_evaluation(rs3, 'Model_3_Bagged_Trees'))\n"
     ]
    }
   ],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs3, 'Model_3_Bagged_Trees'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ef965-d9b3-44cb-a21b-7cfad5bda13b",
   "metadata": {},
   "source": [
    "### 04 - RandomForest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f3dc0-a339-4495-9bc8-c5b078a99b77",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5aecea7f-6b25-4c38-9c5e-d63ee8386855",
   "metadata": {},
   "outputs": [],
   "source": [
    "params4 = {\n",
    "    'tvec__preprocessor': [None, lemmatize_post],\n",
    "     'tvec__max_df': np.linspace(0.75, 0.95,6),              \n",
    "     'tvec__max_features': [4000, 5000, 6000],\n",
    "     'tvec__min_df': [1],\n",
    "     'tvec__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tvec__stop_words': ['english'],       \n",
    "    \n",
    "     'rfc__max_depth': [10,20,30],\n",
    "     'rfc__min_samples_split': [2,4,8],\n",
    "     'rfc__n_estimators': [100],\n",
    "     'rfc__random_state': [2187]\n",
    "}\n",
    "\n",
    "pipe4 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rfc', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "rs4 = RandomizedSearchCV(estimator=pipe4, \n",
    "                         param_distributions=params4, \n",
    "                         n_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e61502-0cba-4252-a88f-4a879de255ea",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "60098ba6-6b02-48f1-a3f1-963270a076a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;background-color: white;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;rfc&#x27;,\n",
       "                                              RandomForestClassifier())]),\n",
       "                   n_iter=1,\n",
       "                   param_distributions={&#x27;rfc__max_depth&#x27;: [10, 20, 30],\n",
       "                                        &#x27;rfc__min_samples_split&#x27;: [2, 4, 8],\n",
       "                                        &#x27;rfc__n_estimators&#x27;: [100, 200],\n",
       "                                        &#x27;rfc__random_state&#x27;: [2187],\n",
       "                                        &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                        &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                        &#x27;tvec__min_df&#x27;: [1],\n",
       "                                        &#x27;tvec__ngram_range&#x27;: [(1, 2), (1, 2)],\n",
       "                                        &#x27;tvec__preprocessor&#x27;: [None,\n",
       "                                                               &lt;function stem_post at 0x7fbf4dfebb50&gt;,\n",
       "                                                               &lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                        &#x27;tvec__stop_words&#x27;: [None, &#x27;english&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(estimator=Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()),\n",
       "                                             (&#x27;rfc&#x27;,\n",
       "                                              RandomForestClassifier())]),\n",
       "                   n_iter=1,\n",
       "                   param_distributions={&#x27;rfc__max_depth&#x27;: [10, 20, 30],\n",
       "                                        &#x27;rfc__min_samples_split&#x27;: [2, 4, 8],\n",
       "                                        &#x27;rfc__n_estimators&#x27;: [100, 200],\n",
       "                                        &#x27;rfc__random_state&#x27;: [2187],\n",
       "                                        &#x27;tvec__max_df&#x27;: [1.0, 0.9],\n",
       "                                        &#x27;tvec__max_features&#x27;: [None, 5000],\n",
       "                                        &#x27;tvec__min_df&#x27;: [1],\n",
       "                                        &#x27;tvec__ngram_range&#x27;: [(1, 2), (1, 2)],\n",
       "                                        &#x27;tvec__preprocessor&#x27;: [None,\n",
       "                                                               &lt;function stem_post at 0x7fbf4dfebb50&gt;,\n",
       "                                                               &lt;function lemmatize_post at 0x7fbf48805510&gt;],\n",
       "                                        &#x27;tvec__stop_words&#x27;: [None, &#x27;english&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-77\" type=\"checkbox\" ><label for=\"sk-estimator-id-77\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer()), (&#x27;rfc&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-78\" type=\"checkbox\" ><label for=\"sk-estimator-id-78\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\" ><label for=\"sk-estimator-id-79\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                             ('rfc',\n",
       "                                              RandomForestClassifier())]),\n",
       "                   n_iter=1,\n",
       "                   param_distributions={'rfc__max_depth': [10, 20, 30],\n",
       "                                        'rfc__min_samples_split': [2, 4, 8],\n",
       "                                        'rfc__n_estimators': [100, 200],\n",
       "                                        'rfc__random_state': [2187],\n",
       "                                        'tvec__max_df': [1.0, 0.9],\n",
       "                                        'tvec__max_features': [None, 5000],\n",
       "                                        'tvec__min_df': [1],\n",
       "                                        'tvec__ngram_range': [(1, 2), (1, 2)],\n",
       "                                        'tvec__preprocessor': [None,\n",
       "                                                               <function stem_post at 0x7fbf4dfebb50>,\n",
       "                                                               <function lemmatize_post at 0x7fbf48805510>],\n",
       "                                        'tvec__stop_words': [None, 'english']})"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs4.fit(X_train, y_train)\n",
    "pickle.dump(rs4, open('./pickled_models/rs4_Random_Forest.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069793dd-f495-4fc8-8584-a8c5c740ae22",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "00bc69f3-0adf-4299-855a-eab116adb882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy Scores:\n",
      "  Train: 0.89513 \n",
      "  Test: 0.7931\n",
      "  Baseline: 0.50913\n",
      "---\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7381    0.8967    0.8097       242\n",
      "           1     0.8744    0.6932    0.7733       251\n",
      "\n",
      "    accuracy                         0.7931       493\n",
      "   macro avg     0.8062    0.7950    0.7915       493\n",
      "weighted avg     0.8075    0.7931    0.7912       493\n",
      "\n",
      "\n",
      "---\n",
      "Best Parameters: \n",
      "{'tvec__stop_words': 'english', 'tvec__preprocessor': <function lemmatize_post at 0x7fbf48805510>, 'tvec__ngram_range': (1, 2), 'tvec__min_df': 1, 'tvec__max_features': 5000, 'tvec__max_df': 1.0, 'rfc__random_state': 2187, 'rfc__n_estimators': 100, 'rfc__min_samples_split': 8, 'rfc__max_depth': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/4sqt0lt90t72lgqr2zq3p6l80000gn/T/ipykernel_33031/3342005247.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_performance_capture = model_performance_capture.append(model_evaluation(rs4, 'Model_4_Random_Forrest'))\n"
     ]
    }
   ],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs4, 'Model_4_Random_Forrest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6e51d-71f1-4507-a263-6652ef6331d2",
   "metadata": {},
   "source": [
    "### 05 - Boosted Trees with AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f728d-3a61-4c38-82f9-0c57c7ea97c6",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "aaf29a38-e8c8-454b-94a2-d653ad8eac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "params5 = {\n",
    "     'tvec__preprocessor': [None,lemmatize_post],\n",
    "     'tvec__max_df': np.linspace(0.75, 0.95,6),              \n",
    "     'tvec__max_features': [4000, 5000, 6000], \n",
    "     'tvec__min_df': [1],\n",
    "     'tvec__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tvec__stop_words': ['english'],      \n",
    "    \n",
    "     'ada__estimator': [None], #DecisionTreeClassifier default\n",
    "     'ada__learning_rate': [0.1, 1, 10],\n",
    "     'ada__n_estimators': [10,20,30],\n",
    "}\n",
    "\n",
    "pipe5 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "rs5 = RandomizedSearchCV(estimator=pipe5, param_distributions=params5, cv = 5, n_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3ba14-45c9-4f42-85f8-29542a6685be",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "027c689a-b452-401b-985c-dff2129d7965",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[312], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrs5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2126\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2127\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2128\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2129\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2130\u001b[0m )\n\u001b[0;32m-> 2131\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m             )\n\u001b[1;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1273\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m, in \u001b[0;36mlemmatize_post\u001b[0;34m(post)\u001b[0m\n\u001b[1;32m      4\u001b[0m mapper \u001b[38;5;241m=\u001b[39m { \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m'\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADJ,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mVERB,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mNOUN,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m'\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADV\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m post_split \u001b[38;5;241m=\u001b[39m post\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m post_tokens \u001b[38;5;241m=\u001b[39m [(token, tag) \u001b[38;5;28;01mfor\u001b[39;00m token, tag \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost_split\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     12\u001b[0m post_lem \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m post_tokens:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tag/__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tag/__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tag/perceptron.py:187\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[1;32m    186\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[0;32m--> 187\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n\u001b[1;32m    190\u001b[0m prev2 \u001b[38;5;241m=\u001b[39m prev\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tag/perceptron.py:62\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     60\u001b[0m scores \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feat \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mor\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rs5.fit(X_train, y_train)\n",
    "pickle.dump(rs5, open('./pickled_models/rs5_AdaBoosted_Trees.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d789cce-c98d-4341-a160-c5d2b1b6e57e",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc442e8-53a5-483b-9076-2a9d506a4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs5, 'Model_5_AdaBoostClassifier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad56b0-a2fb-4787-bb61-0ed9560b85aa",
   "metadata": {},
   "source": [
    "## Single Estimator Models for Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9603bd-d086-4dfb-91e9-47c23a7fe4a4",
   "metadata": {},
   "source": [
    "### 06 Kernelized SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "e62a7e3f-5335-4b3f-bd8e-227275bb72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVectorizer(TransformerMixin):\n",
    "    def __init__(self, vectorizer='CountVectorizer', **kwargs):  # CountVectorizer as Default\n",
    "        self.label = vectorizer\n",
    "        if vectorizer == 'TFIDF':\n",
    "            self.vectorizer = TfidfVectorizer(**kwargs)\n",
    "        elif vectorizer == 'CountVectorizer':\n",
    "            self.vectorizer = CountVectorizer(**kwargs)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self.vectorizer.fit(X, y)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "    \n",
    "    def get_params(self, **kwargs):\n",
    "        return {**self.vectorizer.get_params(**kwargs), 'vectorizer':self.label}\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        return self.vectorizer.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "71cbf6e1-cd9d-4c74-af18-0824147ec565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.float64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'norm': 'l2',\n",
       " 'preprocessor': None,\n",
       " 'smooth_idf': True,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'sublinear_tf': False,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'use_idf': True,\n",
       " 'vocabulary': None,\n",
       " 'vectorizer': 'TFIDF'}"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiVectorizer(vectorizer='TFIDF').get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "98a2444a-e944-4c57-8549-72feb4195b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tvec', TfidfVectorizer()), ('ksvm', SVC())],\n",
       " 'verbose': False,\n",
       " 'tvec': TfidfVectorizer(),\n",
       " 'ksvm': SVC(),\n",
       " 'tvec__analyzer': 'word',\n",
       " 'tvec__binary': False,\n",
       " 'tvec__decode_error': 'strict',\n",
       " 'tvec__dtype': numpy.float64,\n",
       " 'tvec__encoding': 'utf-8',\n",
       " 'tvec__input': 'content',\n",
       " 'tvec__lowercase': True,\n",
       " 'tvec__max_df': 1.0,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__norm': 'l2',\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__smooth_idf': True,\n",
       " 'tvec__stop_words': None,\n",
       " 'tvec__strip_accents': None,\n",
       " 'tvec__sublinear_tf': False,\n",
       " 'tvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tvec__tokenizer': None,\n",
       " 'tvec__use_idf': True,\n",
       " 'tvec__vocabulary': None,\n",
       " 'ksvm__C': 1.0,\n",
       " 'ksvm__break_ties': False,\n",
       " 'ksvm__cache_size': 200,\n",
       " 'ksvm__class_weight': None,\n",
       " 'ksvm__coef0': 0.0,\n",
       " 'ksvm__decision_function_shape': 'ovr',\n",
       " 'ksvm__degree': 3,\n",
       " 'ksvm__gamma': 'scale',\n",
       " 'ksvm__kernel': 'rbf',\n",
       " 'ksvm__max_iter': -1,\n",
       " 'ksvm__probability': False,\n",
       " 'ksvm__random_state': None,\n",
       " 'ksvm__shrinking': True,\n",
       " 'ksvm__tol': 0.001,\n",
       " 'ksvm__verbose': False}"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec6 = {\n",
    "     'tvec__preprocessor': [None,lemmatize_post],\n",
    "     'tvec__max_df': np.linspace(0.75, 0.95,6),              \n",
    "     'tvec__max_features': [4000, 5000, 6000], \n",
    "     'tvec__min_df': [1],\n",
    "     'tvec__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tvec__stop_words': ['english'],\n",
    "}\n",
    "\n",
    "ksvm_params6 = {\n",
    "    'ksvm__C': np.linspace(0.05, 2, 7),\n",
    "    'ksvm__degree': [2, 3],\n",
    "    'ksvm__kernel': ['poly', 'rbf']\n",
    "}\n",
    "\n",
    "params6 = {\n",
    "    **tvec6, **ksvm_params6\n",
    "}\n",
    "\n",
    "pipe6 = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('ksvm', SVC())\n",
    "])\n",
    "pipe6.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "f6dbfdd4-8eec-43ff-a89f-11b203cdc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs6 = RandomizedSearchCV(estimator=pipe6,\n",
    "                        param_distributions=params6,\n",
    "                        cv = 5,\n",
    "                        n_iter = 1\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da605297-d2c5-4fc2-9697-b1ae162954d7",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "93745933-8272-4266-a258-75eec741f370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/robertadams/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "rs6.fit(X_train, y_train)\n",
    "pickle.dump(rs6, open('./pickled_models/rs6_SVM_Multi_Vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c14d04-3cd4-461d-b8ec-d8e27eea6db9",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "4177ce49-a34b-47a4-9378-c1c18680b90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy Scores:\n",
      "  Train: 0.5724 \n",
      "  Test: 0.57201\n",
      "  Baseline: 0.50913\n",
      "---\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5342    1.0000    0.6964       242\n",
      "           1     1.0000    0.1594    0.2749       251\n",
      "\n",
      "    accuracy                         0.5720       493\n",
      "   macro avg     0.7671    0.5797    0.4857       493\n",
      "weighted avg     0.7714    0.5720    0.4818       493\n",
      "\n",
      "\n",
      "---\n",
      "Best Parameters: \n",
      "{'tvec__stop_words': 'english', 'tvec__preprocessor': <function lemmatize_post at 0x7fbf48805510>, 'tvec__ngram_range': (1, 1), 'tvec__min_df': 1, 'tvec__max_features': 6000, 'tvec__max_df': 0.87, 'ksvm__kernel': 'rbf', 'ksvm__degree': 2, 'ksvm__C': 0.05}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/4sqt0lt90t72lgqr2zq3p6l80000gn/T/ipykernel_33031/244385066.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_performance_capture = model_performance_capture.append(model_evaluation(rs6, 'Model_6_SVM_Multi-Vectorizer'))\n"
     ]
    }
   ],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(rs6, 'Model_6_SVM_TFIDF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69448de-d2ca-495c-b618-e8ba462db899",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "a787bc49-9d5a-4344-b2df-f4fd6ac67dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>model_params</th>\n",
       "      <th>train_acuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "      <th>best_score_CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_1_RSCV_Multi_Tfidf</td>\n",
       "      <td>RandomizedSearchCV(cv=5,\\n                   e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.997970</td>\n",
       "      <td>0.805274</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.798388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_2_RsCV_Multi_CVEC</td>\n",
       "      <td>RandomizedSearchCV(cv=5,\\n                   e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.997970</td>\n",
       "      <td>0.805274</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.798388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_3_Bagged_Trees</td>\n",
       "      <td>RandomizedSearchCV(cv=5,\\n                   e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.939107</td>\n",
       "      <td>0.740365</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.766555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_4_Random_Forrest</td>\n",
       "      <td>RandomizedSearchCV(estimator=Pipeline(steps=[(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.895129</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.742242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_6_SVM_Multi-Vectorizer</td>\n",
       "      <td>RandomizedSearchCV(cv=5,\\n                   e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'tvec__stop_words': 'english', 'tvec__preproc...</td>\n",
       "      <td>0.572395</td>\n",
       "      <td>0.572008</td>\n",
       "      <td>0.509128</td>\n",
       "      <td>0.572400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_name  \\\n",
       "0      Model_1_RSCV_Multi_Tfidf   \n",
       "0       Model_2_RsCV_Multi_CVEC   \n",
       "0          Model_3_Bagged_Trees   \n",
       "0        Model_4_Random_Forrest   \n",
       "0  Model_6_SVM_Multi-Vectorizer   \n",
       "\n",
       "                                               model best_score  \\\n",
       "0  RandomizedSearchCV(cv=5,\\n                   e...        NaN   \n",
       "0  RandomizedSearchCV(cv=5,\\n                   e...        NaN   \n",
       "0  RandomizedSearchCV(cv=5,\\n                   e...        NaN   \n",
       "0  RandomizedSearchCV(estimator=Pipeline(steps=[(...        NaN   \n",
       "0  RandomizedSearchCV(cv=5,\\n                   e...        NaN   \n",
       "\n",
       "                                        model_params  train_acuracy  \\\n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...       0.997970   \n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...       0.997970   \n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...       0.939107   \n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...       0.895129   \n",
       "0  {'tvec__stop_words': 'english', 'tvec__preproc...       0.572395   \n",
       "\n",
       "   test_accuracy  baseline_accuracy  best_score_CV  \n",
       "0       0.805274           0.509128       0.798388  \n",
       "0       0.805274           0.509128       0.798388  \n",
       "0       0.740365           0.509128       0.766555  \n",
       "0       0.793103           0.509128       0.742242  \n",
       "0       0.572008           0.509128       0.572400  "
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance_capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d826dd5-586f-4a3d-8975-3d9971c8fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cite: mors: https://stackoverflow.com/questions/42920148/using-sklearn-voting-ensemble-with-partial-fit\n",
    "# VotingClassifier Ensembling without model refit.\n",
    "classifier_list = [rs1, rs2, rs3, rs4, rs5, rs6]\n",
    "\n",
    "classifier_estimators = VotingClassifier(estimators = [('Model_1_RSCV_Multi_Tfidf' ,rs1),\n",
    "                                                       ('Model_2_RsCV_Multi_CVEC', rs2),\n",
    "                                                       ('Model_3_Bagged_Trees', rs3),\n",
    "                                                      ('Model_4_Random_Forrest', rs4),\n",
    "                                                      ('Model_5_AdaBoostClassifier', rs5),\n",
    "                                                      ('Model_6_SVM_TFIDF', rs6)], voting='soft')\n",
    "\n",
    "classifier_estimators.estimators_ = classifier_list\n",
    "classifier_estimators.le_ = LabelEncoder().fit(y)\n",
    "classifier_estimators.classes_ = classifier_estimators.le_.classes_\n",
    "\n",
    "# Now it will work without calling fit\n",
    "classifier_estimators.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627af59b-294a-4276-805f-d2d7e0a8f63a",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "2ec99838-c900-4655-92b0-73c36803a7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Accuracy Scores:\n",
      "  Train: 0.5724 \n",
      "  Test: 0.57201\n",
      "  Baseline: 0.50913\n",
      "---\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5342    1.0000    0.6964       242\n",
      "           1     1.0000    0.1594    0.2749       251\n",
      "\n",
      "    accuracy                         0.5720       493\n",
      "   macro avg     0.7671    0.5797    0.4857       493\n",
      "weighted avg     0.7714    0.5720    0.4818       493\n",
      "\n",
      "\n",
      "---\n",
      "Best Parameters: \n",
      "{'tvec__stop_words': 'english', 'tvec__preprocessor': <function lemmatize_post at 0x7fbf48805510>, 'tvec__ngram_range': (1, 1), 'tvec__min_df': 1, 'tvec__max_features': 6000, 'tvec__max_df': 0.87, 'ksvm__kernel': 'rbf', 'ksvm__degree': 2, 'ksvm__C': 0.05}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/4sqt0lt90t72lgqr2zq3p6l80000gn/T/ipykernel_33031/244385066.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_performance_capture = model_performance_capture.append(model_evaluation(rs6, 'Model_6_SVM_Multi-Vectorizer'))\n"
     ]
    }
   ],
   "source": [
    "model_performance_capture = model_performance_capture.append(model_evaluation(, 'Model_6_SVM_TFIDF'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "e9f830b0-4e18-4261-816e-437e258e7c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[446], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_voting.py:361\u001b[0m, in \u001b[0;36mVotingClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict class labels for X.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        Predicted class labels.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoting \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    363\u001b[0m         maj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:1390\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1386\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1387\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "vc.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
